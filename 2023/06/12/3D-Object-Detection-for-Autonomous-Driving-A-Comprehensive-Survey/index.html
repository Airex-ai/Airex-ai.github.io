<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Airex Yu">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/06/12/3d-object-detection-for-autonomous-driving-a-comprehensive-survey/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="2.3 Evaluation metricsEvaluation metrics分为两类：Average Precision（AP）、Other methods that resolve the evaluation problem from a more practical perspective Evaluation metricsAverage Precision（AP）是从2D目标检测扩展">
<meta property="og:type" content="article">
<meta property="og:title" content="3D Object Detection for Autonomous Driving A Comprehensive Survey">
<meta property="og:url" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="2.3 Evaluation metricsEvaluation metrics分为两类：Average Precision（AP）、Other methods that resolve the evaluation problem from a more practical perspective Evaluation metricsAverage Precision（AP）是从2D目标检测扩展">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612141434426.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612141631222.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612150946847.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612152709159.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230613094258152.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230613094307400.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230613104918566.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092515272.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230615114616506.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230615115824862.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230615120128609.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092306490.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092320866.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092425913.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092743421.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092953143.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093101538.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093148450.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093731084.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093754616.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093810488.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619094053140.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619094622900.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619094634557.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619102053446.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619104652316.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619110114707.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619110147069.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619123536113.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619135143749.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625085352159.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625090628295.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625101559399.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141305192.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141320922.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141333313.png">
<meta property="og:image" content="http://example.com/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141343069.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625150717439.png">
<meta property="og:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625191741450.png">
<meta property="article:published_time" content="2023-06-12T01:34:54.000Z">
<meta property="article:modified_time" content="2023-07-04T02:26:47.055Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="3D Object Detection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612141434426.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/%E9%B1%BC.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/%E9%B1%BC.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/%E9%B1%BC.svg">
    <!--- Page Info-->
    
    <title>
        
            3D Object Detection for Autonomous Driving A Comprehensive Survey -
        
        Airex-Daily
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    <!--- Font Part-->
    
    
    
        <link href="" rel="stylesheet">
    
    
        <link href="" rel="stylesheet">
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":true,"family":null,"url":null},"english":{"enable":true,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fix","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-rry6gw.png"},"title":"伸手也握不住彩虹🌈","subtitle":{"text":["——我期待"],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/Airex-ai","instagram":null,"zhihu":null,"twitter":null,"email":"airex.yu@foxmail.com"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":"https://music.163.com/song?id=1501530173&userid=253099352","cover":null}]},"mermaid":{"enable":true,"version":"9.3.0"}},"version":"2.1.5","navbar":{"auto_hide":true,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"About":{"icon":"fa-regular fa-user","submenus":{"Github":"https://github.com/Airex-ai?tab=repositories"}},"随记":{"icon":"fa-solid fa-tree-palm","path":"/masonry/"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Airex-Daily
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        归档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/Airex-ai?tab=repositories">GITHUB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/masonry/"  >
                                    
                                        
                                            <i class="fa-solid fa-tree-palm"></i>
                                        
                                        随记
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                归档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://github.com/Airex-ai?tab=repositories">GITHUB</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/masonry/"  >
                             
                                
                                    <i class="fa-solid fa-tree-palm"></i>
                                
                                随记
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">3D Object Detection for Autonomous Driving A Comprehensive Survey</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/%E5%A4%B4%E5%83%8F.JPG">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Airex Yu</span>
                            
                                <span class="author-label">Lv3</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-06-12 09:34:54</span>
        <span class="mobile">2023-06-12 09:34</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-07-04 10:26:47</span>
            <span class="mobile">2023-07-04 10:26</span>
            <span class="hover-info">更新</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/3D-Object-Detection/">3D Object Detection</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h2 id="2-3-Evaluation-metrics"><a href="#2-3-Evaluation-metrics" class="headerlink" title="2.3 Evaluation metrics"></a>2.3 Evaluation metrics</h2><p>Evaluation metrics分为两类：Average Precision（AP）、Other methods that resolve the evaluation problem from a more practical perspective</p>
<h3 id="Evaluation-metrics"><a href="#Evaluation-metrics" class="headerlink" title="Evaluation metrics"></a>Evaluation metrics</h3><p>Average Precision（AP）是从2D目标检测扩展来的，公式为：是精确度-召回率曲线。而与2DAP不同的是，3D的AP在计算精确度和召回率时匹配真实值与预测值的匹配标准的不同。</p>
<p><strong>KITTI</strong>：KITTI广泛使用两个AP指标，<code>AP(3D)</code>、<code>AP(BEV)</code>，都是在BEV视角下，当预测值与真实值的两个3D物体的IOU超过临界值就匹配</p>
<p><strong>NuScenes：</strong><code>AP(center)</code>，当预测值与真实值中心的距离低于临界值就匹配。同时<code>Nuscenes</code>还引入了其他参数，如大小，方向角，速度。</p>
<p><strong>Waymo</strong>：<code>Waymo</code>使用<code>AP(hungarian)</code>算法匹配预测值与真实值，同时引入<code>AP weighted by HEading(APH)</code>考虑方向角的误差。</p>
<h3 id="Other-methods"><a href="#Other-methods" class="headerlink" title="Other methods"></a>Other methods</h3><p>此类方法会根据具体的任务来选择更合适的评估指标，如PKL、SDE</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>AP忽略了检测过程中的安全问题，例如车辆周围的误检与车辆很远处的检测可能会得到相似的AP，但车辆周围的误检会造成安全隐患。而PKL与SDE解决了此类的安全隐患，但也出现了额外的其他挑战。</p>
<h1 id="LiDAR-based-3D-Object-Detection"><a href="#LiDAR-based-3D-Object-Detection" class="headerlink" title="LiDAR-based 3D Object Detection"></a>LiDAR-based 3D Object Detection</h1><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612141434426.png"
                      class=""
                >

<h2 id="3D-对象检测的数据表示"><a href="#3D-对象检测的数据表示" class="headerlink" title="3D 对象检测的数据表示"></a>3D 对象检测的数据表示</h2><h3 id="Point-based-3D-object-detection"><a href="#Point-based-3D-object-detection" class="headerlink" title="Point-based 3D object detection"></a>Point-based 3D object detection</h3><p>基于点的 3D 对象检测器有两个基本组成部分：<strong>点云采样和特征学习。</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612141631222.png"
                      class=""
                >



<h4 id="Point-Cloud-Sampling"><a href="#Point-Cloud-Sampling" class="headerlink" title="Point Cloud Sampling"></a>Point Cloud Sampling</h4><p><strong>Farther Point Sampling</strong></p>
<p>Farthest Point Sampling（FPS）是一种常用的点云采样方法，<strong>用于从给定的点云数据中选择具有最大空间距离的一组采样点。</strong>FPS的目标是选择一组具有最大空间分散性的点，以保留原始点云数据的重要特征，并减少冗余点的数量。<br>以下是Farthest Point Sampling的一般操作步骤：</p>
<ol>
<li>随机选择一个起始点作为第一个采样点，将其加入采样点集合中。</li>
<li>计算其他点与已选择的采样点之间的距离。</li>
<li>对于剩余的点，选择与已选择的采样点之间距离最大的点作为下一个采样点，并将其加入采样点集合中。</li>
<li>重复步骤 3，直到达到所需的采样点数目或没有剩余的点可选择。</li>
</ol>
<p><strong>Segmentation based Sampling</strong></p>
<p>Segmentation-based sampling是点云处理中的一种采样方法，<strong>其目的是从点云数据中选择具有代表性的点子集，以减少数据的复杂性并保留关键信息。</strong>在Segmentation-based sampling中，首先进行点云的语义分割，将点云数据按照不同的语义类别进行标记，例如地面、建筑物、车辆、行人等。然后，根据语义分割结果，对每个语义类别进行采样操作。<br>具体的操作步骤如下：</p>
<ol>
<li><p>进行点云的语义分割，将点云数据划分为不同的语义类别。</p>
</li>
<li><p>对每个语义类别进行采样。采样方法可以根据具体需求选择，常见的方法包括：</p>
<ul>
<li><p>基于体素的采样：将每个语义类别的点云数据转换为体素表示，然后在体素网格中进行采样，选择代表性的体素作为采样点。</p>
</li>
<li><p>基于距离的采样：根据点云数据中每个点到语义类别的距离，选择距离较远的点作为采样点，以确保较好的覆盖范围。</p>
</li>
<li><p>基于特征的采样：使用点云数据的特征信息，例如法向量、曲率等，选择具有代表性的特征点作为采样点。</p>
</li>
</ul>
</li>
<li><p>根据采样点的选择结果，生成新的采样点云数据集。</p>
</li>
</ol>
<p><strong>Voxel based Sampling</strong></p>
<p>根据点云是否落在网格中进行采样</p>
<h4 id="Feature-Learning"><a href="#Feature-Learning" class="headerlink" title="Feature Learning"></a>Feature Learning</h4><p><strong>Set Abstraction</strong></p>
<p><u>首先，通过球形查询（ball query）的方式，在给定的半径范围内收集上下文点。</u>球形查询是一种在点云中确定半径内的邻域点的方法，通常是以某个中心点为圆心，指定一个半径，在这个范围内找到周围的点。</p>
<p><u>接下来，将收集到的上下文点和对应的特征输入到多层感知机（multi-layer perceptrons）和最大池化（maxpooling）中进行特征的聚合。</u>在多层感知机中，可以通过一系列的全连接层和激活函数来对上下文点的特征进行变换和组合，以获得新的特征表示。多层感知机通常具有多个隐藏层，每个隐藏层包含多个神经元，用于学习更高级的特征表示。</p>
<p><u>在特征聚合过程中，通常使用最大池化操作来提取最显著的特征。</u></p>
<p><strong>graph NN</strong></p>
<p>Graph Neural Network (GNN) 是一种神经网络模型，专门用于处理图结构数据的特征提取和分析。</p>
<p>传统的神经网络模型适用于处理向量形式的数据，但对于图结构数据（如社交网络、分子结构、道路网络等），传统的神经网络无法直接应用。Graph Neural Network 基于图结构数据的特点，通过在节点之间建立连接和传播信息的方式，能够有效地对图中的节点和边进行建模和学习。</p>
<p>Graph Neural Network 的基本思想是每个节点都具有一个特征向量，<strong>通过聚合邻居节点的特征信息，更新自身的特征表示</strong>。具体而言，GNN 使用一系列的图卷积操作，通过将节点的特征与其邻居节点的特征进行聚合和组合，逐步传播和更新信息。这样，节点在多次迭代中不断更新自身的特征表示，融合了其邻居节点的信息和图结构的拓扑关系。</p>
<p><strong>transformer</strong></p>
<p>在特征提取中，”transformer”是指一种基于自注意力机制（self-attention）的神经网络模型，最初被引入于自然语言处理领域，用于解决序列数据的建模任务，如机器翻译和语言理解。</p>
<p><strong>Transformer模型的核心思想是通过自注意力机制来建立输入序列中各个元素之间的关联性，从而实现对序列的全局建模</strong>。自注意力机制能够根据输入序列的内容自动计算每个元素与其他元素的关联程度，并根据关联程度对元素进行加权聚合。通过多层自注意力模块的堆叠，Transformer模型能够捕捉不同抽象级别的语义信息，从而实现强大的特征表示能力。</p>
<p>在计算机视觉领域，特别是图像处理任务中，Transformer模型被广泛应用于特征提取和图像生成等任务。通过将图像划分为不同的空间位置，可以将图像视为一系列的局部特征。然后，使用Transformer模型对局部特征进行处理，利用自注意力机制捕捉图像内部的空间关系，从而获得全局一致的特征表示。</p>
<h4 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h4><p>利用点云和特征进行预测</p>
<h4 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h4><p>基于点云的检测器的效率取决于两个因素：点云的数量、特征学习SA中sampling中的半径的选取。</p>
<ul>
<li>增加上下文点的数量将获得更多的表示能力，但代价是增加大量内存消耗。</li>
<li>如果半径太小，上下文信息可能不足；如果半径太大，细粒度的3D信息可能丢失，即带你云中每个点的局部细节</li>
</ul>
<p>点云取样中，体素取样属于随即均匀采样，具有很高的效率。但点云不是均匀分布的，随机均匀采样更倾向于点云密度高的区域，而对稀疏区域来说，最远距离采样可能会有更好的表现。<strong>然而，最远点采样本质上是一种序列算法，不能变得高度并行。因此，最远点采样通常是耗时的，实时检测效果较差。</strong></p>
<p><strong>基于点云采样和特征学习的基于点的检测方法分类</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612150946847.png"
                      class=""
                >



<h3 id="Grid-based-3D-object-detection"><a href="#Grid-based-3D-object-detection" class="headerlink" title="Grid-based 3D object detection"></a>Grid-based 3D object detection</h3><p>基于网格的3D对象检测器首先将点云光栅化为离散网格表示，<strong>即体素、柱和鸟瞰图（BEV）特征图</strong>。然后，他们应用传统的2D卷积神经网络或3D稀疏神经网络从网格中提取特征。基于网格的探测器有两个基本组件：：<strong>基于网格的表示和基于网格的神经网络。</strong></p>
<blockquote>
<p>对于体素格方法，点云被划分为规则的三维体素格子，每个格子表示一个小的体积单元。点云中的点被分配到相应的体素格子中，以表示其在空间中的位置。</p>
<p>柱状体方法类似于体素格方法，但是在每个格子中，点云被表示为一个柱状体，其中包含格子内的所有点云信息。这种表示方式可以更有效地处理密集的点云数据。</p>
<p>鸟瞰图特征图是一种将点云投影到二维平面上的表示方法，通常是在地面平面上生成一个栅格图。每个栅格单元格代表一个固定大小的区域，并且记录该区域内点云的特征信息，例如点的密度、高度等。</p>
</blockquote>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230612152709159.png"
                      class=""
                >

<blockquote>
<p>在卷积网络中，”squeeze”操作通常指的是将张量的维度进行压缩，即减少维度的数量或者将维度的大小减小到1。这个操作通常用于减少参数数量或者将高维特征映射转换为低维表示。</p>
</blockquote>
<blockquote>
<p>稀疏卷积是一种在卷积操作中仅考虑输入中非零元素的卷积方式。在传统的卷积操作中，卷积核会与输入特征图的每个位置进行卷积计算，包括那些值为零的位置。而稀疏卷积则将计算限制在非零元素上，忽略输入中的零值位置。</p>
<p>稀疏卷积常用于处理稀疏数据，例如在图像或点云等领域中。这些数据通常具有大量的零值，因此使用稀疏卷积可以减少计算量和内存消耗，并加快卷积操作的速度。</p>
</blockquote>
<h4 id="基于网格-的表示"><a href="#基于网格-的表示" class="headerlink" title="基于网格 的表示"></a>基于网格 的表示</h4><p><strong>Voxels</strong></p>
<p>如果我们将检测空间光栅化为规则的3D网格，体素就是网格单元。由于点云是离散分布的，所以大多数体素单元是空的，不包含点云。如果点云落入该网格单元中，则体素便包含了元素。</p>
<p><u><code>VoxelNet</code>是最先利用离散体素网格的，并使用<code>voxel feature encoding(VFE)</code>从体素单元提取特征。</u></p>
<p>此外，有两类方法试图改进用于3D对象检测的体素表示：</p>
<ul>
<li>多视角体素：一些方法从不同的视图提出了<strong>动态体素化和融合方案</strong>，例如鸟瞰图和透视图，圆柱形和球形视图，范围视图</li>
<li>多尺度体素：一些方法生成不同尺度的体素或使用可重构的体素。</li>
</ul>
<p><strong>pillars</strong></p>
<p>柱可以被视为特殊的体素，其中体素大小在垂直方向上是无限的。支柱特征可以通过<code>PointNet</code>从点聚合，然后散射回来以构建用于特征提取的2D BEV图像。</p>
<p><strong>BEV特征图</strong></p>
<p>“Bird’s-eye view feature map”（俯视图特征图）是在俯视图上生成的密集二维表示，其中每个像素对应于特定的区域，并编码了该区域内的点云信息。在生成俯视图特征图时，可以根据像素所对应的区域中的点云密度、点云高度、点云分类等信息进行编码。每个像素的数值可以表示该区域内的点云数量、点云的平均高度或其他相关特征。</p>
<p><strong>BEV特征图可以通过将3D特征投影到鸟瞰图中从体素和柱体中获得，也可以通过汇总像素区域内的点统计数据从原始点云中直接获得。</strong>常用的统计数据包括二进制占用率、局部点云的高度和密度</p>
<h4 id="基于网格的神经网络"><a href="#基于网格的神经网络" class="headerlink" title="基于网格的神经网络"></a>基于网格的神经网络</h4><p>有两种主要类型的基于网格的网络：<u>用于BEV特征图和支柱的2D卷积神经网络，以及用于体素的3D稀疏神经网络</u>。</p>
<p><strong>2D神经网络</strong></p>
<p>2D卷积神经网络。 可以将常规的2D卷积神经网络应用于BEV特征图从鸟瞰图中检测 3D 对象。 在大多数作品中，2D 网络架构通常改编自 2D 对象检测中的那些成功设计</p>
<p><strong>3D稀疏神经网络</strong></p>
<p>3D 稀疏卷积神经网络网络基于两个专门的 3D 卷积运算符：<strong>稀疏卷积和子流形卷积</strong> </p>
<blockquote>
<p>传统的卷积操作在规则的二维图像或三维体数据上进行，但对于点云等非规则结构数据，传统的卷积操作并不适用。Submanifold convolutions利用点云数据的局部结构，将点云中的每个点及其相邻点构成的局部区域看作是一个子流形（submanifold）。然后，对于每个子流形，使用局部坐标系进行卷积操作，以提取特征。</p>
<p><strong>Submanifold convolutions的主要思想是通过局部坐标系来解决非规则结构数据上的卷积问题。</strong>它可以更好地捕捉点云数据中的局部结构和特征，并在不同位置进行共享和汇聚。这种方法在点云处理任务中具有重要的应用，如点云分类、分割和目标检测等。</p>
</blockquote>
<h4 id="分析-2"><a href="#分析-2" class="headerlink" title="分析"></a>分析</h4><p>与 BEV 特征图与支柱等 2D 表示形成对比，<strong>体素包含更多结构化的 3D 信息</strong>。 此外，可以通过 3D 稀疏网络学习深度体素特征。然而，3D 神经网络带来了额外的时间和内存成本。</p>
<p> BEV 特征图是最有效的网格表示，它直接将点云投影到 2D 伪图像中，而无需专门的 3D 运算符，如稀疏卷积或支柱编码。二维检测技术也可以无缝无需太多修改即可应用于 BEV 特征图。 基于 BEV 的检测方法通常可以获得很高的效率和实时推理速度。 <strong>不过简单总结一下像素区域内的点统计丢失了太多的 3D 信息，这导致与基于体素的检测。</strong></p>
<p>基于柱子的检测方法利用 <code>PointNet</code> 对柱子单元内的 3D 点信息进行编码，然后将特征分散回二维伪图像（为了得到更高的检测效率），平衡了效率和3D目标检测的效率。</p>
<blockquote>
<p><code>PointNet</code>是一种用于处理点云数据的神经网络，可以提取点云的局部特征。通过将<code>PointNet</code>应用于每个柱状单元，可以将柱状单元内的点云信息编码为特征向量。</p>
</blockquote>
<p><strong>所有基于网格的方法都必须面对的一个关键问题是选择合适的网格单元大小。</strong> <u>较小的网格产生高分辨率网格，因此保持更细粒度的细节</u>，这对于准确的 3D 对象检测至关重要。然而，减小网格单元的大小会导致二次方二维网格表示的内存消耗增加像 BEV 特征图或支柱。<strong>这是因为较小的单元尺寸需要更多的网格单元来覆盖整个感兴趣区域，从而占用更多的存储空间</strong></p>
<h3 id="Point-voxel-based-3D-object-detection"><a href="#Point-voxel-based-3D-object-detection" class="headerlink" title="Point-voxel based 3D object detection"></a>Point-voxel based 3D object detection</h3><p>基于点体素的方法采用混合架构，利用点和体素进行3D对象检测。这些方法可分为两类:<strong>单阶段检测框架和两阶段检测框架。</strong></p>
<p><strong>单阶段点-体素检测框架</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230613094258152.png"
                      class="" title="image-20230613094258152"
                >

<p>点包含细粒度的几何信息，体素的计算效率很高，在特征提取阶段将它们结合在一起自然会受益于这两种表示。</p>
<p>基于单级点体素的三维目标检测器试图通过骨干网络中的<code>point-to-voxel</code>和<code>voxel-to-point</code>转换来桥接点和体素的特征。</p>
<p><strong>两阶段点-体素检测框架</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230613094307400.png"
                      class="" title="image-20230613094307400"
                >

<p>基于点体素的两阶段三维目标检测器在不同的检测阶段采用不同的数据表示。</p>
<blockquote>
<p>第一阶段使用体素表示对整个点云进行初步的物体检测，生成一组候选框。而第二阶段则在关键点的基础上，通过特定的点操作方法对候选框进行进一步的优化和细化，以得到更精确的检测结果。这样的两阶段设计可以结合体素的全局信息和关键点的局部特征，从而提高目标检测的性能和效果。</p>
</blockquote>
<h4 id="分析-3"><a href="#分析-3" class="headerlink" title="分析"></a>分析</h4><p>对于混合点-体素骨架，单阶段的框架中，点与体素特征的融合通常依赖于体素到点和点到体素的转换机制，这将带来不可忽略的时间成本。对于两阶段的点体素检测框架，如何有效地聚合3D提案的点特征是一个关键的挑战，因为现有的模块和算子通常非常耗时。</p>
<h3 id="Range-based-3D-object-detection"><a href="#Range-based-3D-object-detection" class="headerlink" title="Range-based 3D object detection"></a>Range-based 3D object detection</h3><p>距离图像是一种密集而紧凑的2D表示，其中每个像素包含3D距离信息而不是RGB值。基于距离的方法从两个方面解决检测问题：<strong>针对距离图像设计新的模型和算子，选择合适的视图进行检测</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230613104918566.png"
                      class="" title="image-20230613104918566"
                >

<p><strong>处理方法：</strong>由于距离图像是像RGB图像一样的2D表示，基于距离的3D物体检测器自然可以借用2D物体检测中的模型来处理距离图像。</p>
<p><strong>算子：</strong>距离图像的像素包含3D距离信息，而不是颜色值，因此传统2D网络架构中的标准卷积算子对于基于距离的检测不是最优的，<strong>因为滑动窗口中的像素可能在3D空间中彼此相距很远</strong></p>
<blockquote>
<p>传统的二维卷积操作是在二维平面上进行的，它在像素之间共享权重，假设相邻像素之间有一定的空间局部性。然而，在距离图像中，像素之间的距离并不一定与它们在二维平面上的位置相对应。因此，在使用传统的二维卷积操作时，可能无法充分利用像素之间的实际距离信息，导致目标检测的性能下降。</p>
<p>为了更好地处理距离图像，需要使用适合距离信息的卷积操作。一种常见的方法是使用三维卷积操作，即在距离图像的三维空间中进行卷积。这样可以确保卷积操作在距离上具有一致的感受野大小，更好地捕捉物体在三维空间中的形状和结构。</p>
</blockquote>
<p><strong>视图：</strong>距离图像是从距离视图(RV)中捕获的，理想情况下，<strong>距离视图是点云的球形投影</strong>。对于许多基于距离的方法来说，直接从距离视图中检测3D物体是一种自然的解决方案。然而，从距离视图进行检测不可避免地会受到球面投影带来的遮挡和尺度变化问题的困扰</p>
<h4 id="分析-4"><a href="#分析-4" class="headerlink" title="分析"></a>分析</h4><p>距离图像是一种密集紧凑的二维表示，因此传统的或专门的二维卷积可以无缝地应用于距离图像，这使得特征提取过程非常高效。然而，与鸟瞰图检测相比，距离视图检测容易受到遮挡和尺度变化的影响。<u>因此，从距离视图中提取特征并从鸟瞰视图中检测目标成为基于距离的三维目标检测最实用的解决方案</u>。</p>
<h2 id="3D目标检测的学习目标"><a href="#3D目标检测的学习目标" class="headerlink" title="3D目标检测的学习目标"></a>3D目标检测的学习目标</h2><p>学习目标在目标检测中至关重要。<strong>由于三维物体相对于整个检测范围很小，因此在三维检测中强烈需要特殊的机制来增强对小物体的定位。</strong></p>
<p>另一方面，考虑到点云是稀疏的，<strong>物体通常具有不完整的形状，准确估计三维物体的中心和大小是一个长期的挑战。</strong></p>
<h3 id="Anchor-based-3D-object-detection"><a href="#Anchor-based-3D-object-detection" class="headerlink" title="Anchor-based 3D object detection"></a>Anchor-based 3D object detection</h3><p>Anchor是预定义的具有固定形状的可以放在3D空间的矩形框。而通过计算正Anchor与真实值之间的IOU可以预测3D目标的位置。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092515272.png"
                      class="" title="image-20230619092515272"
                >



<h4 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a><strong>先决条件</strong></h4><p>3D目标g真实值：[x<del>g</del>, y<del>g</del>, z<del>g</del>,l<del>g</del>,w<del>g</del>,h<del>g</del>,θ<del>g</del>]，anchor：[x<del>a</del>, y<del>a</del>, z<del>a</del>,l<del>a</del>,w<del>a</del>,h<del>a</del>,θ<del>a</del>]，预测值：[x,y,z,l,w,h,θ]。通过anchor来计算预测值。</p>
<h4 id="Anchor-configurations"><a href="#Anchor-configurations" class="headerlink" title="Anchor configurations"></a><strong>Anchor configurations</strong></h4><p>基于Anchor的3D目标检测方法是在BEV的视角下，将3Danchors放在BEV特征图的网格中。而对每一类物体来说，都会有相似的大小。</p>
<h4 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a><strong>Loss functions</strong></h4><p>基于anchor的3D目标检测方法使用分类损失 L<del>cls</del>来学习区分正负anchor；回归损失L<del>reg</del>学习基于正anchor预测的物体的尺寸和位置；此外，使用L<del>θ</del>来学习目标的方位角。损失函数：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230615114616506.png"
                      class="" title="image-20230615114616506"
                ></p>
<blockquote>
<p>区分正负anchors</p>
</blockquote>
<p><strong><code>VoxelNet</code>将具有高IOU的anchor视为正anchor，其他视为负anchor。</strong>为了区分正负anchors，对于每一个在BEV特征图的anchor来说，使用二进制交叉熵损失来区分，公式如下：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230615115824862.png"
                      class="" title="image-20230615115824862"
                ></p>
<p>p是每个anchor的预测概率，q取值为0或1，表示正负anchor。为了提高定位能力，<code>VoxelNet</code>又引入了focal loss：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230615120128609.png"
                      class="" title="image-20230615120128609"
                >

<p><strong>在大多数工作中， <code> α = 0.25</code>，<code>γ = 2 </code></strong></p>
<blockquote>
<p>regression targets</p>
</blockquote>
<p>回归目标可以用来预测正anchors，以此来学习3D目标的大小和位置</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092306490.png"
                      class="" title="image-20230619092306490"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092320866.png"
                      class="" title="image-20230619092320866"
                > 是anchor在BEV视角的对角线，`SmoothL1 loss`用来回归目标：

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092425913.png"
                      class="" title="image-20230619092425913"
                >

<blockquote>
<p>heading angle</p>
</blockquote>
<p>为了获得航向角θ，弧度方向偏移可以直接用SmoothL1损失进行回归:</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092743421.png"
                      class="" title="image-20230619092743421"
                >

<p><strong>然而，由于回归范围大，直接回归弧度偏移通常是困难的。</strong>另外，<strong>基于bin的航向估计</strong>是一种更好的航向角学习方法，该方法首先将角度空间划分为bin，采用基于bin的分类<code>Ldir</code>和残差回归:</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619092953143.png"
                      class="" title="image-20230619092953143"
                >

<p>其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093101538.png"
                      class="" title="image-20230619093101538"
                > 是一个<code>bin</code>中的残差。除此之外，可以使用正弦函数编码残差：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093148450.png"
                      class="" title="image-20230619093148450"
                >

<blockquote>
<p><strong>使用正弦函数编码残差的作用是为了有效地表示和学习周期性模式或振荡性的数据。</strong>正弦函数是一种周期性函数，具有平滑的连续性和周期性的特点。</p>
</blockquote>
<blockquote>
<p>L<del>IOU</del> </p>
</blockquote>
<p>除了分别学习物体的大小、位置和方向的损失函数外，<strong>考虑所有物体参数的<code>IoU</code>损失也可以应用于3D物体检测：</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093731084.png"
                      class="" title="image-20230619093731084"
                >

<p>其中，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093754616.png"
                      alt="image-20230619093754616"
                >与<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619093810488.png"
                      alt="image-20230619093810488"
                >分别是真实值与预测的3D边框。而IOU是以差分的方式计算3D IOU的</p>
<blockquote>
<p>corner loss</p>
</blockquote>
<p>除了<code>IoU</code>损耗外，还引入了角损来最小化地面真值与预测框的8个角之间的距离，即</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619094053140.png"
                      class="" title="image-20230619094053140"
                >

<p>其中，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619094622900.png"
                      alt="image-20230619094622900"
                >与<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619094634557.png"
                      alt="image-20230619094634557"
                > 是真实值与预测框的第<code>i</code>个角。</p>
<blockquote>
<p>在目标检测任务中，边界框通常由两种表示方式之一来描述：中心点和尺寸、或者顶点坐标。在这种情况下，”corner”指的是边界框的顶点。</p>
</blockquote>
<h4 id="分析-5"><a href="#分析-5" class="headerlink" title="分析"></a>分析</h4><p><strong>基于锚点的方法可以受益于相同类别的3D物体应该具有相似形状的先验知识，因此可以借助3D锚点生成准确的物体预测</strong>。<u>然而，由于3D物体相对于检测范围而言相对较小，因此需要大量的锚点来确保整个检测范围的完全覆盖</u>，例如在KITTI[82]数据集上[333]使用了大约70k个锚点。</p>
<p><strong>此外，对于那些非常小的对象，如行人和骑自行车的人，应用基于锚点的分配可能相当具有挑战性。</strong>考虑到锚点一般放置在每个网格单元的中心，如果网格单元较大，单元中的物体较小，则该单元的锚点与小物体的<code>IoU</code>较低，可能会阻碍训练过程</p>
<h3 id="Anchor-free-3D-object-detection"><a href="#Anchor-free-3D-object-detection" class="headerlink" title="Anchor-free 3D object detection"></a>Anchor-free 3D object detection</h3><p><strong>基于锚点的方法和无锚点的方法的主要区别在于正样本和负样本的选择。</strong></p>
<h4 id="Grid-based-assignment"><a href="#Grid-based-assignment" class="headerlink" title="Grid-based assignment"></a>Grid-based assignment</h4><blockquote>
<p>Grid-based assignment是一种在基于激光雷达的3D目标检测中常用的分配策略。它的目的是将点云中的每个点分配给离它最近的网格单元格。</p>
</blockquote>
<p><strong>位于真实3D目标内部的网格单元被视为正样本，因为它们与目标物体具有重叠或相交的关系，而其他未被真实3D目标覆盖的网格单元被视为负样本。</strong></p>
<p>Grid-based assignment可以使用<code>二值交叉熵损失</code>与<code>焦点损失</code>作为分类损失。回归目标为：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619102053446.png"
                      class="" title="image-20230619102053446"
                >

<p>其中<code>dx</code>和<code>dy</code>是正网格单元和物体中心之间的偏移量。使用<code>SmmothL1</code>回归∆</p>
<h4 id="Point-based-assignment"><a href="#Point-based-assignment" class="headerlink" title="Point-based assignment"></a>Point-based assignment</h4><p>大多数基于点的检测方法采用无锚点和基于点的分配策略<strong>，首先对点进行分割，选择3D物体内部或附近的前景点作为正样本，最后从这些前景点学习三维边界框。</strong></p>
<h4 id="Range-based-assignment"><a href="#Range-based-assignment" class="headerlink" title="Range-based assignment"></a>Range-based assignment</h4><p>一种常见的解决方案是选择3D对象内部的范围像素作为正样本，与其他回归目标基于全局三维坐标系的方法不同，基于距离的方法采用以对象为中心的方法。回归目标同样使用：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619104652316.png"
                      class="" title="image-20230619104652316"
                >

<h4 id="Set-to-Set-assignment"><a href="#Set-to-Set-assignment" class="headerlink" title="Set-to-Set assignment"></a>Set-to-Set assignment</h4><blockquote>
<p>Set-to-set assignment是一种用于多目标追踪的匹配算法，用于将一个集合中的元素与另一个集合中的元素进行匹配和关联。</p>
</blockquote>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619110114707.png"
                      class="" title="image-20230619110114707"
                >

<p>其中，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619110147069.png"
                      class="" title="image-20230619110147069"
                > 是正样本与3D目标一对一的映射。</p>
<h4 id="分析-6"><a href="#分析-6" class="headerlink" title="分析"></a>分析</h4><p>无锚检测方法摒弃了复杂的锚设计，在分配策略上表现出更强的灵活性。通过无锚分配，<u>3D对象可以直接在各种表示上进行预测，包括点、距离像素、体素、柱和BEV网格单元。</u></p>
<p>无需引入额外的形状先验，学习过程也大大简化了。<strong>在这些无锚点方法中，基于中心的方法在检测小物体方面显示出巨大的潜力，并且在广泛使用的基准测试中优于基于锚点的检测方法。</strong></p>
<p>尽管有这些优点，无锚方法的一个普遍挑战是正确选择正样本来生成3D对象预测。与基于锚点的方法只选择那些高<code>IoU</code>样本相比，无锚点的方法可能会选择一些不良的阳性样本，从而产生不准确的对象预测。因此，在大多数无锚定方法中，仔细设计以过滤掉那些不利的积极因素是很重要的。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619123536113.png"
                      class="" title="image-20230619123536113"
                >

<h3 id="3D-object-detection-with-auxiliary-tasks"><a href="#3D-object-detection-with-auxiliary-tasks" class="headerlink" title="3D object detection with auxiliary tasks"></a>3D object detection with auxiliary tasks</h3><p>许多方法采用辅助任务来增强空间特征，并为精确的3D目标检测提供隐式指导。</p>
<h4 id="Semantic-segmentation"><a href="#Semantic-segmentation" class="headerlink" title="Semantic segmentation"></a>Semantic segmentation</h4><p>语义分割可以在3个方面帮助3D物体检测：</p>
<ol>
<li>前景分割可以提供关于物体位置的隐式信息。</li>
<li>空间特征可以通过分割来增强。</li>
<li>语义分割可以作为预处理步骤来过滤背景样本，使3D物体检测更高效。</li>
</ol>
<h4 id="IoU-predication"><a href="#IoU-predication" class="headerlink" title="IoU predication"></a><strong>IoU predication</strong></h4><p>交集比联合<code>IoU</code>可以作为一种有用的监督信号来校正目标置信度分数。</p>
<p>可以使用一个辅助分支来预测每个检测到的3D物体的<code>IoU</code>分数S<del>I0u</del>。在推理过程中，常规分类分支的原始置信分数S<del>conf</del>&#x3D; S<del>cls</del>被S<del>IoU</del>进一步修正:</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230619135143749.png"
                      class="" title="image-20230619135143749"
                >

<p>其中超参数β控制抑制低<code>IoU</code>预测和增强高<code>IoU</code>预测的程度。通过<code>IoU</code>校正，更容易选择高质量的3D对象作为最终预测对象。</p>
<h4 id="Objectshape-completion"><a href="#Objectshape-completion" class="headerlink" title="Objectshape completion"></a><strong>Objectshape completion</strong></h4><p><u>由于激光雷达传感器的性质，远距离物体通常只接收到其表面上的几个点，因此3D物体通常是稀疏和不完整的</u>。提高检测性能的一种直接方法是从稀疏的点云中完成物体形状。<strong>完整的形状可以为准确和鲁棒的检测提供更多有用的信息</strong>。在3D检测中已经提出了许多形状补全技术，包括形状解码器、形状签名[388]和概率占用网格。</p>
<h4 id="Object-part-estimation"><a href="#Object-part-estimation" class="headerlink" title="Object part estimation"></a><strong>Object part estimation</strong></h4><p>识别物体内部的部分信息有助于三维物体检测，因为它揭示了物体更细粒度的三维结构信息</p>
<h4 id="分析-7"><a href="#分析-7" class="headerlink" title="分析"></a>分析</h4><p>3D物体检测与许多其他3D感知和生成任务天生相关。3D检测和分割的多任务学习比独立训练3D目标检测器更有益，形状补全也可以帮助3D目标检测。还有其他任务可以帮助提高3D物体探测器的性能。例如，<strong>场景流估计可以识别静态和移动物体，并且在点云序列中跟踪相同的3D物体可以更准确地估计该物体。</strong></p>
<h1 id="Multi-Modal-3D-Object-detection"><a href="#Multi-Modal-3D-Object-detection" class="headerlink" title="Multi-Modal 3D Object detection"></a>Multi-Modal 3D Object detection</h1><p>根据不同的传感器类型，多模态的3D目标检测方法可以分为以下三个类型：<code>LiDAR-camera</code>|、<code>radar</code>、<code>map fusion-based</code></p>
<h2 id="Multi-modal-detection-with-LiDAR-camera-fusion"><a href="#Multi-modal-detection-with-LiDAR-camera-fusion" class="headerlink" title="Multi-modal detection with LiDAR-camera fusion"></a>Multi-modal detection with LiDAR-camera fusion</h2><p>相机和激光雷达是两种互补的传感器类型，用于3D物体检测。<strong>摄像头提供颜色信息，从中可以提取丰富的语义特征，而激光雷达传感器专门用于3D定位，并提供有关3D结构的丰富信息</strong>。<u>由于基于激光雷达的检测方法比基于相机的方法性能要好得多，目前最先进的方法主要是基于激光雷达的3D物体探测器，并试图将图像信息整合到激光雷达检测管道的不同阶段</u>。鉴于基于激光雷达和基于摄像机的检测系统的复杂性，将两种方式结合在一起不可避免地会带来额外的计算开销和推理延迟。</p>
<p>多模态3D物体检测方法分类如下表所示。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625085352159.png"
                      class="" title="image-20230625085352159"
                >

<h3 id="Early-fusion-based-3D-object-detection"><a href="#Early-fusion-based-3D-object-detection" class="headerlink" title="Early-fusion based 3D object detection"></a>Early-fusion based 3D object detection</h3><p><u>基于早期融合的方法旨在将图像中的信息整合到点云中，然后将其输入基于激光雷达的检测框架中</u>。因此，早期融合框架通常采用顺序构建的方式：<strong>首先利用二维检测或分割网络从图像中提取信息，然后将图像信息传递给点云，最后将增强的点云馈送给基于LiDAR的三维目标检测器。</strong>根据融合类型的不同，早期融合方法可分为<strong>区域级知识融合</strong>和<strong>点级知识融合</strong>两大类。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625090628295.png"
                      class="" title="image-20230625090628295"
                >

<h4 id="Region-level-knowledge-fusion"><a href="#Region-level-knowledge-fusion" class="headerlink" title="Region-level knowledge fusion"></a>Region-level knowledge fusion</h4><p>区域级融合方法旨在利用图像中的区域信息来缩小三维点云中的目标候选区域。具体来说，图像首先通过二维物体检测器生成二维边界框，然后将二维边界框提取成三维视锥体。将三维视锥体应用于激光雷达点云，减少了搜索空间。最后，只有选定的点云区域被送入激光雷达探测器进行三维目标检测。</p>
<blockquote>
<p>视锥体是由2D边界框在三维空间中的延伸而形成的几何体，它表示物体在相机视角下的可见范围。</p>
<p><strong>在3D视锥体中，边界框的底部是在图像上定义的，它代表物体在相机视野中的位置和大小。然后，通过延伸视景锥体的顶部和侧面，将其在三维空间中定义出来，以形成一个锥形体。</strong>这个锥形体代表了相机视角下的物体可见区域，限定了激光雷达点云中需要进行搜索和分析的空间范围。</p>
</blockquote>
<h4 id="Point-level-knowledge-fusion"><a href="#Point-level-knowledge-fusion" class="headerlink" title="Point-level knowledge fusion"></a>Point-level knowledge fusion</h4><p>点级融合的目的是利用图像特征增强输入点云。然后将增强的点云送入激光雷达探测器，以获得更好的检测结果。</p>
<h4 id="分析-8"><a href="#分析-8" class="headerlink" title="分析"></a>分析</h4><p>基于早期融合的方法侧重于在点云通过激光雷达三维目标检测框架之前用图像信息增强点云。<u>大多数基于早期融合的方法都与各种基于激光雷达的3D物体探测器兼容，并且可以作为一个相当有效的预处理步骤来提高检测性能</u>。然而，早期融合方法通常以顺序的方式进行多模态融合和3D目标检测，这带来了额外的推理延迟。由于融合步骤通常需要复杂的二维目标检测或语义分割网络，因此多模态融合带来的时间成本通常是不可忽略的。因此，如何在早期有效地进行多模态融合已成为一个关键的挑战。</p>
<h3 id="Intermediate-fusion-based-3D-object-detection"><a href="#Intermediate-fusion-based-3D-object-detection" class="headerlink" title="Intermediate-fusion based 3D object detection"></a>Intermediate-fusion based 3D object detection</h3><p>基于中间融合的方法试图在基于LiDAR的3D目标检测器的中间阶段融合图像和LiDAR特征，例如<strong>在骨干网络中</strong>，<strong>在提案生成阶段</strong>，或<strong>在<code>RoI</code>细化阶段。</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625101559399.png"
                      class="" title="image-20230625101559399"
                >

<h4 id="Fusion-in-backbone-networks"><a href="#Fusion-in-backbone-networks" class="headerlink" title="Fusion in backbone networks"></a>Fusion in backbone networks</h4><p>为了在骨干网中逐步融合图像和激光雷达特征，人们做了许多努力。在这些方法中，首先通过激光雷达到相机的变换建立<strong>点像对应关系</strong>，然后利用点像对应关系，通过不同的融合算子将激光雷达主干网的特征与图像主干网的特征进行融合。</p>
<blockquote>
<p>具体来说，首先通过LiDAR到相机的变换，将点云中的点映射到图像上的像素位置，建立点云中的每个点与对应的图像像素之间的对应关系。这样就可以将点云中的点与图像上的像素进行匹配，形成点到像素的对应关系。</p>
<p>在建立了点到像素的对应关系之后，可以使用不同的融合操作将来自LiDAR骨干网络的特征和图像骨干网络的特征进行融合。<strong>这些融合操作可以是简单的特征拼接、加权求和或者其他复杂的操作。</strong>通过融合操作，可以将来自点云和图像的特征相结合，以获得更全面和丰富的信息，从而提高算法在目标检测、场景理解等任务中的性能和表现。</p>
</blockquote>
<p><u>基于网格的检测骨干的中间层的多模态融合</u>：<strong>连续卷积</strong>、混合体素特征编码、Transformer</p>
<p><u>在主干网的输出特征图上进行多模态融合</u>：<strong>gated attention</strong>、<strong>BEV pooling</strong>、<strong>Transformer</strong></p>
<blockquote>
<p>Gated attention是一种注意力机制的变体，用于增强模型对输入数据的关注程度。在传统的注意力机制中，通过计算输入的注意力权重，可以选择性地聚焦于输入的不同部分。<strong>而在gated attention中，除了计算注意力权重外，还引入了一个门控机制，用于调节注意力的强度和选择性。</strong></p>
<p><strong>具体来说，Gated attention利用一个门控机制来控制两个传感器或特征之间的权重。</strong>该门控机制可以是一个sigmoid激活函数，它将输入值映射到0到1之间的范围。这个值可以被解释为对应传感器或特征的重要性或权重。</p>
<p>在多传感器融合中，Gated attention通过计算两个传感器或特征的相似度或相关性来确定它们之间的权重。然后，将这些权重与原始的传感器或特征进行加权融合，以得到最终的融合结果。</p>
</blockquote>
<blockquote>
<p>连续卷积（Continuous Convolution）是一种用于处理连续数据的卷积操作。与传统的离散卷积不同，<strong>连续卷积是在连续的定义域上进行的，适用于处理连续信号或连续函数。</strong></p>
<p>在连续卷积中，卷积核（或滤波器）是一个函数，而输入信号（或输入函数）也是一个函数。通过将卷积核与输入函数进行卷积运算，可以得到输出函数，表示了输入函数与卷积核之间的某种关系。</p>
<p>连续卷积可以用数学公式表示为：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141305192.png"
                      class="" title="image-20230625141305192"
                >

<p>其中，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141320922.png"
                      alt="image-20230625141320922"
                >是输入函数，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141333313.png"
                      alt="image-20230625141333313"
                > 是卷积核函数，<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625141343069.png"
                      alt="image-20230625141343069"
                >是输出函数。公式中的积分表示了对输入函数和卷积核在整个定义域上的乘积进行积分求和的过程。</p>
</blockquote>
<blockquote>
<p>多传感器融合中的BEV（鸟瞰图）池化是一种操作，<u>用于将多个传感器（如图像和点云）中提取的特征在BEV空间中进行汇聚和池化。</u></p>
<p>BEV池化操作的目的是将不同传感器中的特征映射到相同的BEV空间，以便进行后续的信息融合和处理。它通过将BEV空间划分为固定大小的网格单元（或称为格子），然后在每个网格单元中对对应的特征进行聚合和池化。</p>
<p>具体来说，<strong>对于每个BEV网格单元，BEV池化操作会将该网格单元内的特征进行汇聚，常见的汇聚方式包括最大池化（选取最大值）、平均池化（计算平均值）等</strong>。这样，每个BEV网格单元就可以用一个汇聚后的特征表示，从而将多个传感器中的特征信息汇聚到相同的BEV空间中。</p>
<p>BEV池化操作的优点是可以减少特征的维度，并提取出每个网格单元内的主要特征信息。这样做的好处是可以降低计算复杂度，并且在后续的任务中提供更简洁、更高效的特征表示，如目标检测、目标跟踪等。</p>
</blockquote>
<blockquote>
<p>在多传感器融合中，Transformer是一种用于处理多模态数据的神经网络模型。<strong>它基于自注意力机制，能够有效地捕捉不同模态之间的关联和依赖关系。</strong></p>
<p>在多传感器融合中，<strong>Transformer通过将不同传感器的特征作为输入，利用自注意力机制学习传感器之间的相关性和权重。</strong>通过在编码器和解码器中使用多头注意力机制，Transformer能够在保留传感器特征的同时，将不同传感器的信息进行交互和整合，从而提高多传感器数据融合的效果。</p>
<p>具体而言，Transformer通过自注意力机制在编码器中对输入特征进行编码，然后在解码器中生成最终的融合表示。在编码器中，自注意力机制可以自动学习输入特征之间的关系，捕捉全局上下文信息。在解码器中，自注意力机制可以根据编码器的输出和目标任务的信息，动态地调整注意力权重，生成最终的融合表示。</p>
</blockquote>
<h4 id="Fusion-in-proposal-generation-and-RoI-head"><a href="#Fusion-in-proposal-generation-and-RoI-head" class="headerlink" title="Fusion in proposal generation and RoI head"></a>Fusion in proposal generation and RoI head</h4><p>存在一类在提案生成和<code>RoI</code>细化阶段进行多模态特征融合的工作。</p>
<p>具体来说，首先从LiDAR数据中使用一个LiDAR检测器生成3D目标候选框（proposals）。然后，这些3D候选框被投影到多个视角，包括图像视图和鸟瞰图，<strong>以从图像和LiDAR的主干网络中裁剪特征</strong>。最后，裁剪得到的图像和LiDAR特征会在一个<code>RoI</code>头部（Region of Interest head）中进行融合，以预测每个3D目标的参数。</p>
<h4 id="分析-9"><a href="#分析-9" class="headerlink" title="分析"></a>分析</h4><p>中间方法鼓励更深入地集成多模态表示，并产生更高质量的3D盒子。</p>
<h3 id="Late-fusion-based-3D-object-detection"><a href="#Late-fusion-based-3D-object-detection" class="headerlink" title="Late-fusion based 3D object detection"></a>Late-fusion based 3D object detection</h3><p>基于后期融合的方法分别对基于lidar的3D物体检测器和基于图像的2D物体检测器的输出(即3D和2D边界框)进行操作。在这些方法中，相机和LiDAR传感器可以并行进行目标检测，并将输出的2D和3D盒子融合在一起，从而获得更准确的3D检测结果。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625150717439.png"
                      class="" title="image-20230625150717439"
                >

<p>基于后期融合的方法侧重于实例级聚合，只在实例级上的输出进行多模态融合，<strong>避免了中间特征和输入点云之间复杂的交互</strong>。</p>
<p>因此，与其他方法相比，这些方法要有效得多。<u>然而，这些方法没有利用相机和激光雷达传感器的深度特征，无法整合不同模态的丰富语义信息，这限制了这类方法的潜力。</u></p>
<blockquote>
<ol>
<li>早期融合（Early Fusion）：<strong>早期融合是指在输入数据的早期阶段将不同模态的特征进行融合</strong>。在早期融合中，不同模态的数据在输入网络之前就被组合在一起。这意味着多模态特征被直接融合成一个单一的表示，然后一起输入到模型中进行处理。早期融合的优点是可以同时利用多个模态的信息，提供更丰富的输入特征。<u>然而，早期融合可能会导致特征维度过高或信息冗余，同时也可能忽略了模态间的差异性。</u></li>
<li>中期融合（Mid-level Fusion）：<strong>中期融合是指在网络的中间层将不同模态的特征进行融合</strong>。在中期融合中，每个模态的特征在各自模态的网络中提取，并在中间层进行融合。这样可以在各自模态网络中保留模态特定的信息，然后在中间层进行模态间的融合。中期融合的优点是可以更好地利用每个模态的特点，并充分考虑模态间的差异性。<u>然而，中期融合也可能导致融合层的计算量增加，并且需要设计合适的融合策略。</u></li>
<li>后期融合（Late Fusion）：<strong>后期融合是指在每个模态的独立网络中分别提取特征，并在最后阶段将它们的结果进行融合</strong>。在后期融合中，每个模态的特征在各自的网络中提取，然后在特征提取完成后进行融合。通常，融合的方式可以是简单的拼接、加权求和或者通过其他融合操作。后期融合的优点是每个模态的网络可以独立地学习特定模态的信息，而融合操作可以根据任务需求进行自定义。<u>然而，后期融合可能会忽略模态间的相互作用，并且需要更多的计算资源来进行融合操作。</u></li>
</ol>
</blockquote>
<h2 id="Multi-modal-detection-with-radar-signals"><a href="#Multi-modal-detection-with-radar-signals" class="headerlink" title="Multi-modal detection with radar signals"></a>Multi-modal detection with radar signals</h2><p>雷达是驾驶系统中一种重要的传感类型。与激光雷达传感器相比，雷达在实际应用中具有四个不可替代的优势：<strong>雷达比激光雷达传感器便宜得多；雷达不易受到极端天气条件的影响；雷达有更大的探测范围；雷达提供额外的速度测量</strong>。然而，与产生密集点云的激光雷达传感器相比，雷达只能提供稀疏和有噪声的测量。因此，如何有效地处理雷达信号仍然是一个严峻的挑战。</p>
<h3 id="Radar-LiDAR-fusion"><a href="#Radar-LiDAR-fusion" class="headerlink" title="Radar-LiDAR fusion"></a>Radar-LiDAR fusion</h3><p>许多论文试图通过引入新的融合机制来融合两种模式，以实现雷达和激光雷达信号之间的信息传递，包括<strong>基于体素的融合</strong> 、<strong>基于注意力的融合</strong>、引入距离-方位-多普勒张量、利用<strong>图神经网络</strong>、利用动态占用图和<strong>引入4D雷达数据</strong>。</p>
<h3 id="Radar-camera-fusion"><a href="#Radar-camera-fusion" class="headerlink" title="Radar-camera fusion"></a>Radar-camera fusion</h3><p>雷达-相机融合与激光雷达-相机融合非常相似，因为雷达和激光雷达数据都是3D点表示。大多数雷达-相机方法采用现有的基于lidar的检测架构来处理稀疏的雷达点，并采用与基于lidar -相机的方法相似的融合策略。</p>
<h2 id="Multi-modal-detection-with-high-definition-maps"><a href="#Multi-modal-detection-with-high-definition-maps" class="headerlink" title="Multi-modal detection with high-definition maps"></a>Multi-modal detection with high-definition maps</h2><p>高清地图(HD maps)包含道路形状、道路标线、交通标志、障碍物等详细的道路信息。高清地图提供了关于周围环境的丰富语义信息，可以作为辅助3D物体检测的强大前提。如何有效地将地图信息整合到三维目标检测框架中已成为研究领域面临的一个开放挑战。</p>
<h2 id="Multi-modal-detection-with-map-information"><a href="#Multi-modal-detection-with-map-information" class="headerlink" title="Multi-modal detection with map information"></a>Multi-modal detection with map information</h2><p>高清地图可以很容易地转换为鸟瞰图表示，并与栅格化的BEV点云或特征地图融合。融合可以通过简单地连接栅格化点云和高清地图的通道来进行，将LiDAR点云和高清地图馈送到单独的主干，并融合两种模式的输出特征地图，或者简单地过滤掉那些不属于相关地图区域的预测。其他地图类型也被探索过，例如可见度地图、矢量化地图。</p>
<h1 id="3D-Object-Detection-in-Driving-Systems"><a href="#3D-Object-Detection-in-Driving-Systems" class="headerlink" title="3D Object Detection in Driving Systems"></a>3D Object Detection in Driving Systems</h1><h2 id="End-to-end-learning-for-autonomous-driving"><a href="#End-to-end-learning-for-autonomous-driving" class="headerlink" title="End-to-end learning for autonomous driving"></a>End-to-end learning for autonomous driving</h2><p>3D物体检测是感知系统的关键组成部分，3D物体探测器的性能将对跟踪、预测和规划等下游任务产生深远的影响。因此，<strong>从系统的角度来看，与其他感知任务以及下游任务共同训练3D物体检测模型将是更好的自动驾驶解决方案</strong>。如何将所有驾驶任务整合到一个统一的框架中，并以端到端的方式对这些任务进行联合训练，这是一个开放的挑战。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/image-20230625191741450.png"
                      class="" title="image-20230625191741450"
                >

<h3 id="Joint-perception-and-prediction"><a href="#Joint-perception-and-prediction" class="headerlink" title="Joint perception and prediction"></a>Joint perception and prediction</h3><p>有许多工作学习感知和跟踪3D对象，然后以端到端的方式预测它们的未来轨迹。<code>FaF</code>是一项开创性的工作，它提出了使用单个3D卷积网络对3D目标检测、跟踪和轨迹预测进行联合推理。</p>
<h3 id="Joint-perception-prediction-and-planning"><a href="#Joint-perception-prediction-and-planning" class="headerlink" title="Joint perception, prediction, and planning"></a>Joint perception, prediction, and planning</h3><p>人们做出了许多努力，将感知、预测和计划纳入一个统一的框架。与联合感知和预测方法相比，<u>通过在端到端管道中添加运动规划，整个系统可以受益于规划器的反馈。</u></p>
<h3 id="End-to-end-learning-for-autonomous-driving-1"><a href="#End-to-end-learning-for-autonomous-driving-1" class="headerlink" title="End-to-end learning for autonomous driving"></a>End-to-end learning for autonomous driving</h3><p>一些方法试图构建一个完全端到端的自动驾驶系统，其中自动驾驶车辆接受感官输入，依次进行感知、预测、规划和运动控制，最后产生用于驾驶的转向和速度信号。<code>End to end learning for self-driving cars</code>首先介绍了基于图像的端到端驾驶系统的基本思想，并利用卷积神经网络实现了该系统。</p>
<p><code>Multimodal end-to-end autonomous driving</code>提出了一个具有多模态输入的端到端体系结构。</p>
<h2 id="Simulation-for-3D-object-detection"><a href="#Simulation-for-3D-object-detection" class="headerlink" title="Simulation for 3D object detection"></a>Simulation for 3D object detection</h2><p>三维目标检测模型通常需要大量的数据进行训练。<strong>虽然可以在实际场景中收集数据，但实际数据通常受到长尾分布的影响</strong>。例如，交通事故或极端天气的场景很少被记录，但对于训练一个强大的3D物体探测器非常重要。模拟是解决长尾数据分布问题的一个很有前途的解决方案，因为我们可以为那些罕见但关键的场景创建合成数据。<u>模拟的一个公开挑战是如何创建更真实的合成数据。</u></p>
<blockquote>
<p>在数据集中，”long-tail”是指数据分布呈现长尾分布的情况。长尾数据表示存在一小部分常见类别或样本数量较多，而大部分类别或样本数量较少，形成一个长尾形状的分布曲线。这种情况通常在现实世界的许多数据集中都存在。</p>
<p>长尾数据的特点是：</p>
<ol>
<li>少数类别或样本占据了大部分数据集的比例，而多数类别或样本数量相对较少。</li>
<li>频率较低的类别或样本数量远远大于频率较高的类别或样本数量。</li>
<li>长尾数据集通常呈现出一个右偏的分布曲线，尾部延伸。</li>
</ol>
</blockquote>
<h3 id="Visual-simulation"><a href="#Visual-simulation" class="headerlink" title="Visual simulation"></a>Visual simulation</h3><p>为了在驾驶场景中生成逼真的合成图像，人们做了很多努力。这些方法的思想包括利用<strong>图形引擎，利用纹理映射<code>surfels</code>(surface elements)，利用真实世界的数据，以及学习一个可控的神经模拟器。</strong></p>
<blockquote>
<p><strong>图形引擎（Graphics Engine）是一种软件或硬件系统，用于生成和渲染图形和图像。</strong>它是计算机图形学的关键组成部分，用于创建和显示图形对象、场景和特效。</p>
<p>图形引擎通常具备以下功能：</p>
<ol>
<li>图形渲染：图形引擎能够接收输入的图形数据，如几何信息、纹理映射、光照等，并通过算法和技术将这些数据转化为最终的可视图像。它可以处理对象的位置、形状、颜色、纹理、光照和阴影等视觉属性，以产生逼真的图像。</li>
<li>场景管理：图形引擎可以管理和组织图形对象的场景，包括对象的布局、层次结构、相机视角和动画等。它能够处理复杂的场景，并控制对象的交互和动态变化。</li>
<li>渲染管线：<strong>在计算机图形学中，图形渲染管线（Graphics Rendering Pipeline）是一系列的阶段和操作，用于将输入的图形数据转换为最终的可视图像。</strong>图形引擎通过渲染管线来处理图形数据的转换和处理过程。渲染管线通常包括几何处理、光栅化、着色、投影、深度测试和图像合成等阶段，以将图形数据转化为最终的像素图像。</li>
<li>物理模拟：一些高级的图形引擎还可以支持物理模拟，模拟物体的运动、碰撞、重力等物理特性，以增加场景的真实感和动态效果。</li>
</ol>
</blockquote>
<blockquote>
<p><code>Texture-mapped surfels</code>（贴图的面元）是一种表示三维场景的表面元素，每个表面元素包含几何信息和纹理信息。每个<code>surfel</code>存储了一个位置、一个法线和一个纹理坐标。通过对每个<code>surfel</code>进行纹理映射，将二维图像的纹理信息贴合到三维表面上。这样，当观察场景时，可以根据观察角度和距离从正确的位置获取纹理信息，并将其渲染到对应的表面元素上。</p>
<p><code>Texture-mapped surfels</code>的过程可以概括为以下几个步骤：</p>
<ol>
<li>数据获取：首先，需要获取三维场景的数据，通常使用激光扫描（LiDAR）或结构光等技术获取点云数据，以及使用相机拍摄图像数据。</li>
<li>表面重建：基于点云数据，进行表面重建操作，将离散的点云转换为连续的表面。这可以通过点云配准、点云分割和三角化等方法实现。</li>
<li>纹理映射：将图像的纹理映射到表面元素上。首先，根据点云和图像之间的对应关系，确定每个表面元素的位置和法线。然后，通过将纹理图像按照对应的纹理坐标映射到每个表面元素上，实现纹理的贴合。</li>
<li>渲染：在渲染过程中，根据观察者的位置和角度，从正确的位置获取表面元素的纹理信息，并将其渲染到屏幕上。这涉及到光照、阴影和投影等计算，以呈现真实的视觉效果。</li>
</ol>
</blockquote>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> 3D Object Detection for Autonomous Driving A Comprehensive Survey</li>
        <li><strong>作者:</strong> Airex Yu</li>
        <li><strong>创建于:</strong> 2023-06-12 09:34:54</li>
        
            <li>
                <strong>更新于:</strong> 2023-07-04 10:26:47
            </li>
        
        <li>
            <strong>链接:</strong> http://example.com/2023/06/12/3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey/
        </li>
        <li>
            <strong>版权声明:</strong> 本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/3D-Object-Detection/">#3D Object Detection</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2023/06/17/VsCode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">VsCode远程连接</span>
                                    <span class="post-nav-item">上一篇</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/06/02/shell/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">shell</span>
                                    <span class="post-nav-item">下一篇</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">3D Object Detection for Autonomous Driving A Comprehensive Survey</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Evaluation-metrics"><span class="nav-text">2.3 Evaluation metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-metrics"><span class="nav-text">Evaluation metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-methods"><span class="nav-text">Other methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-text">分析</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LiDAR-based-3D-Object-Detection"><span class="nav-text">LiDAR-based 3D Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA"><span class="nav-text">3D 对象检测的数据表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Point-based-3D-object-detection"><span class="nav-text">Point-based 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Grid-based-3D-object-detection"><span class="nav-text">Grid-based 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Point-voxel-based-3D-object-detection"><span class="nav-text">Point-voxel based 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Range-based-3D-object-detection"><span class="nav-text">Range-based 3D object detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="nav-text">3D目标检测的学习目标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anchor-based-3D-object-detection"><span class="nav-text">Anchor-based 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Anchor-free-3D-object-detection"><span class="nav-text">Anchor-free 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3D-object-detection-with-auxiliary-tasks"><span class="nav-text">3D object detection with auxiliary tasks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-Modal-3D-Object-detection"><span class="nav-text">Multi-Modal 3D Object detection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-modal-detection-with-LiDAR-camera-fusion"><span class="nav-text">Multi-modal detection with LiDAR-camera fusion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Early-fusion-based-3D-object-detection"><span class="nav-text">Early-fusion based 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Intermediate-fusion-based-3D-object-detection"><span class="nav-text">Intermediate-fusion based 3D object detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Late-fusion-based-3D-object-detection"><span class="nav-text">Late-fusion based 3D object detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-modal-detection-with-radar-signals"><span class="nav-text">Multi-modal detection with radar signals</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Radar-LiDAR-fusion"><span class="nav-text">Radar-LiDAR fusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Radar-camera-fusion"><span class="nav-text">Radar-camera fusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-modal-detection-with-high-definition-maps"><span class="nav-text">Multi-modal detection with high-definition maps</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-modal-detection-with-map-information"><span class="nav-text">Multi-modal detection with map information</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3D-Object-Detection-in-Driving-Systems"><span class="nav-text">3D Object Detection in Driving Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#End-to-end-learning-for-autonomous-driving"><span class="nav-text">End-to-end learning for autonomous driving</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Joint-perception-and-prediction"><span class="nav-text">Joint perception and prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Joint-perception-prediction-and-planning"><span class="nav-text">Joint perception, prediction, and planning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-to-end-learning-for-autonomous-driving-1"><span class="nav-text">End-to-end learning for autonomous driving</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simulation-for-3D-object-detection"><span class="nav-text">Simulation for 3D object detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Visual-simulation"><span class="nav-text">Visual simulation</span></a></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Airex Yu</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a> 驱动</span>
                <br>
            <span class="theme-version-container">主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.5</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/01/05 11:45:14
            </div>
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>




    
<script src="/js/libs/mermaid.min.js"></script>

    
<script src="/js/plugins/mermaid.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
