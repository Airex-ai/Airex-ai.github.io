<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Airex Yu">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/04/06/mpi/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="介绍MPI 的历史简介分布式计算现在对于我们来说，就跟日常生活里的手机和电脑一样普及。你很明显应该认同这个观点，因为你发现了这个了不起的 MPI 教程网站！不管你是出于什么原因想学习并行编程（parallel programming），或者说分布式编程、并行编程，也许是因为课程需要，或者是工作，或者单纯地觉得好玩，我觉得你都应该选择一项在未来几年依然十分有价值的技术去学习。我觉得「消息传递接口」（">
<meta property="og:type" content="article">
<meta property="og:title" content="MPI">
<meta property="og:url" content="http://example.com/2023/04/06/MPI/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="介绍MPI 的历史简介分布式计算现在对于我们来说，就跟日常生活里的手机和电脑一样普及。你很明显应该认同这个观点，因为你发现了这个了不起的 MPI 教程网站！不管你是出于什么原因想学习并行编程（parallel programming），或者说分布式编程、并行编程，也许是因为课程需要，或者是工作，或者单纯地觉得好玩，我觉得你都应该选择一项在未来几年依然十分有价值的技术去学习。我觉得「消息传递接口」（">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406190258065.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406190643521.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406192458314.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406201220958.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406201727561.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406202312516.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406203808578.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230406204451384.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407102050741.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407103300194.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407105421091.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407145700539.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407150118235.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407151502131.png">
<meta property="og:image" content="http://example.com/2023/04/06/MPI/image-20230407152146975.png">
<meta property="article:published_time" content="2023-04-06T12:41:56.000Z">
<meta property="article:modified_time" content="2023-05-25T12:56:41.691Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="进程">
<meta property="article:tag" content="通讯">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/04/06/MPI/image-20230406190258065.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/%E9%B1%BC.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/%E9%B1%BC.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/%E9%B1%BC.svg">
    <!--- Page Info-->
    
    <title>
        
            MPI -
        
        Airex-Daily
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    <!--- Font Part-->
    
    
    
        <link href="" rel="stylesheet">
    
    
        <link href="" rel="stylesheet">
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":true,"family":null,"url":null},"english":{"enable":true,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fix","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-rry6gw.png"},"title":"伸手也握不住彩虹🌈","subtitle":{"text":["——我期待"],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/Airex-ai","instagram":null,"zhihu":null,"twitter":null,"email":"airex.yu@foxmail.com"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":"https://music.163.com/song?id=1501530173&userid=253099352","cover":null}]},"mermaid":{"enable":true,"version":"9.3.0"}},"version":"2.1.5","navbar":{"auto_hide":true,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"About":{"icon":"fa-regular fa-user","submenus":{"Github":"https://github.com/Airex-ai?tab=repositories"}},"随记":{"icon":"fa-solid fa-tree-palm","path":"/masonry/"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Airex-Daily
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        归档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/Airex-ai?tab=repositories">GITHUB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/masonry/"  >
                                    
                                        
                                            <i class="fa-solid fa-tree-palm"></i>
                                        
                                        随记
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                归档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://github.com/Airex-ai?tab=repositories">GITHUB</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/masonry/"  >
                             
                                
                                    <i class="fa-solid fa-tree-palm"></i>
                                
                                随记
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
             
                <div class="article-title">         
                    <img src="/images/mpi.png" alt="MPI" />
                    <h1 class="article-title-cover">MPI</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/%E5%A4%B4%E5%83%8F.JPG">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Airex Yu</span>
                            
                                <span class="author-label">Lv3</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-04-06 20:41:56</span>
        <span class="mobile">2023-04-06 20:41</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-05-25 20:56:41</span>
            <span class="mobile">2023-05-25 20:56</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/MPI/">MPI</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E8%BF%9B%E7%A8%8B/">进程</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E9%80%9A%E8%AE%AF/">通讯</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="MPI-的历史简介"><a href="#MPI-的历史简介" class="headerlink" title="MPI 的历史简介"></a>MPI 的历史简介</h2><p>分布式计算现在对于我们来说，就跟日常生活里的手机和电脑一样普及。你很明显应该认同这个观点，因为你发现了这个了不起的 MPI 教程网站！不管你是出于什么原因想学习并行编程（parallel programming），或者说分布式编程、并行编程，也许是因为课程需要，或者是工作，或者单纯地觉得好玩，我觉得你都应该选择一项在未来几年依然十分有价值的技术去学习。我觉得「<strong>消息传递接口</strong>」（Message Passing Interface, MPI）就是这样一项技术，而且学习它确实可以让你的并行编程知识变得更深厚。尽管 <strong>MPI 比大多数并行框架要更底层</strong>（比如 Hadoop），但是学习 MPI 会为你的并行编程打下良好的基础。</p>
<p>在 90 年代之前，程序员可没我们这么幸运。对于不同的计算架构写并发程序是一件困难而且冗长的事情。当时，很多软件库可以帮助写并发程序，但是没有一个大家都接受的标准来做这个事情。</p>
<p>在当时，大多数的并发程序只出现在科学和研究的领域。最广为接受的模型就是<strong>消息传递模型</strong>。什么是消息传递模型？它其实只是指程序通过在进程间传递消息（消息可以理解成带有一些信息和数据的一个数据结构）来完成某些任务。在实践中，并发程序用这个模型去实现特别容易。举例来说，主进程（manager process）可以通过对从进程（worker process）发送一个描述工作的消息来把这个工作分配给它。另一个例子就是一个并发的排序程序可以在当前进程中对当前进程可见的（我们称作本地的，locally）数据进行排序，然后把排好序的数据发送的邻居进程上面来进行合并的操作。几乎所有的并行程序可以使用消息传递模型来描述。</p>
<p>由于当时<strong>很多软件库都用到了这个消息传递模型，但是在定义上有些微小的差异</strong>，这些库的作者以及一些其他人为了解决这个问题就在 Supercomputing 1992 大会上定义了一个消息传递接口的标准- 也就是 MPI。这个标准接口使得程序员写的并发程序可以在所有主流的并发框架中运行。并且允许他们可以使用当时已经在使用的一些流行库的特性和模型。</p>
<h2 id="MPI-对于消息传递模型的设计"><a href="#MPI-对于消息传递模型的设计" class="headerlink" title="MPI 对于消息传递模型的设计"></a>MPI 对于消息传递模型的设计</h2><p>先解释一下 MPI 在消息传递模型设计上的一些经典概念。第一个概念是<em><strong>通讯器</strong></em>（communicator）。通讯器定义了一组能够互相发消息的进程。在这组进程中，每个进程会被分配一个序号，称作<em>秩</em>（rank），进程间显性地通过指定秩来进行通信。</p>
<p>通信的基础建立在不同进程间发送和接收操作。一个进程可以通过指定另一个进程的秩以及一个独一无二的消息<em>标签</em>（<em>tag</em>）来发送消息给另一个进程。接受者可以发送一个接收特定标签标记的消息的请求（或者也可以完全不管标签，接收任何消息），然后依次处理接收到的数据。类似这样的涉及一个发送者以及一个接受者的通信被称作<strong><em>点对点</em>（point-to-point）通信</strong>。</p>
<p>当然在很多情况下，某个进程可能需要跟所有其他进程通信。比如主进程想发一个广播给所有的从进程。在这种情况下，手动去写一个个进程点对点的信息传递就显得很笨拙。而且事实上这样会导致网络利用率低下。MPI 有专门的接口来帮我们处理这类所有进程间的<strong><em>集体性</em>（collective）通信。</strong></p>
<p>把<strong>点对点通信</strong>和<strong>集体性通信</strong>这两个机制合在一起已经可以创造十分复杂的并发程序了。事实上，这两个功能已经强大到我现在不需要再介绍任何 MPI 高级的特性了</p>
<h1 id="MPI-Hello-World"><a href="#MPI-Hello-World" class="headerlink" title="MPI Hello World"></a>MPI Hello World</h1><h2 id="Hello-world-代码案例"><a href="#Hello-world-代码案例" class="headerlink" title="Hello world 代码案例"></a>Hello world 代码案例</h2><div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">    <span class="comment">// 初始化 MPI 环境</span></span><br><span class="line">    MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过调用以下方法来得到所有可以工作的进程数量</span></span><br><span class="line">    <span class="type">int</span> world_size;</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);		<span class="comment">//MPI_Comm_size函数返回进程数量到world_size。MPI_WORLD是MPI自动生成的通讯器</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到当前进程的秩</span></span><br><span class="line">    <span class="type">int</span> world_rank;</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到当前进程的名字</span></span><br><span class="line">    <span class="type">char</span> processor_name[MPI_MAX_PROCESSOR_NAME];</span><br><span class="line">    <span class="type">int</span> name_len;</span><br><span class="line">    MPI_Get_processor_name(processor_name, &amp;name_len);	<span class="comment">//MPI_GET_processor_name返回当前进程的名字</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印一条带有当前进程名字，秩以及</span></span><br><span class="line">    <span class="comment">// 整个 communicator 的大小的 hello world 消息。</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello world from processor %s, rank %d out of %d processors\n&quot;</span>,</span><br><span class="line">           processor_name, world_rank, world_size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放 MPI 的一些资源</span></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="运行-MPI-hello-world-程序"><a href="#运行-MPI-hello-world-程序" class="headerlink" title="运行 MPI hello world 程序"></a>运行 MPI hello world 程序</h2><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; git clone https://github.com/mpitutorial/mpitutorial</span><br><span class="line">&gt;&gt;&gt; cd mpitutorial/tutorials/mpi-hello-world/code</span><br><span class="line">&gt;&gt;&gt; cat makefile</span><br><span class="line">EXECS=mpi_hello_world</span><br><span class="line">MPICC?=mpicc</span><br><span class="line"></span><br><span class="line">all: $&#123;EXECS&#125;</span><br><span class="line"></span><br><span class="line">mpi_hello_world: mpi_hello_world.c</span><br><span class="line">    $&#123;MPICC&#125; -o mpi_hello_world mpi_hello_world.c</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">    rm $&#123;EXECS&#125;</span><br></pre></td></tr></table></figure></div>

<p>makefile 会去找 MPICC 这个环境变量。如果你把 MPICH2 装在了本地文件夹里面而不是全局 PATH 下面, 手动设置一下 MPICC 这个环境变量，把它指向你的 mpicc 二进制程序。mpicc 二进制程序其实只是对 gcc 做了一层封装，使得编译和链接所有的 MPI 程序更方便。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; export MPICC=/home/kendall/bin/mpicc</span><br><span class="line">&gt;&gt;&gt; make</span><br><span class="line">/home/kendall/bin/mpicc -o mpi_hello_world mpi_hello_world.c</span><br></pre></td></tr></table></figure></div>

<p>当程序编译好之后，它就可以被执行了。</p>
<h1 id="MPI-Send-and-Receive"><a href="#MPI-Send-and-Receive" class="headerlink" title="MPI Send and Receive"></a>MPI Send and Receive</h1><p>发送和接收是 MPI 里面两个基础的概念。MPI 里面几乎所有单个的方法都可以使用基础的发送和接收 API 来实现。</p>
<h2 id="MPI-的发送和接收简介"><a href="#MPI-的发送和接收简介" class="headerlink" title="MPI 的发送和接收简介"></a>MPI 的发送和接收简介</h2><p>MPI 的发送和接收方法是按以下方式进行的：开始的时候，<em>A</em> 进程决定要发送一些消息给 <em>B</em> 进程。A进程就会把需要发送给B进程的所有数据打包好，放到一个缓存里面。因为所有数据会被打包到一个大的信息里面，因此缓存常常会被比作<em>信封</em>（就像我们把好多信纸打包到一个信封里面然后再寄去邮局）。数据打包进缓存之后，通信设备（通常是网络）就需要负责把信息传递到正确的地方。<strong>这个正确的地方也就是根据特定秩确定的那个进程。</strong><code>秩确定传送对象</code></p>
<p>尽管数据已经被送达到 B 了，但是进程 B 依然需要确认它想要接收 A 的数据。一旦它确定了这点，数据就被传输成功了。进程 A 会接收到数据传递成功的信息，然后去干其他事情。</p>
<p>有时候 A 需要传递很多不同的消息给 B。为了让 B 能比较方便地区分不同的消息，MPI 运行发送者和接受者额外地指定一些信息 ID (正式名称是<em>标签</em>, <em>tags</em>)。当 B 只要求接收某种特定标签的信息的时候，其他的不是这个标签的信息会先被缓存起来，等到 B 需要的时候才会给 B。<code>tag确定传送文件</code></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MPI_Send(</span><br><span class="line">    void* data,								<span class="comment">#数据缓存</span></span><br><span class="line">    <span class="built_in">int</span> count,								<span class="comment">#数据量</span></span><br><span class="line">    MPI_Datatype datatype,					<span class="comment">#类型</span></span><br><span class="line">    <span class="built_in">int</span> destination,						<span class="comment">#秩</span></span><br><span class="line">    <span class="built_in">int</span> tag,								<span class="comment">#标签</span></span><br><span class="line">    MPI_Comm communicator)					<span class="comment">#通讯器</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">MPI_Recv(</span><br><span class="line">    void* data,</span><br><span class="line">    <span class="built_in">int</span> count,</span><br><span class="line">    MPI_Datatype datatype,</span><br><span class="line">    <span class="built_in">int</span> source,								<span class="comment">#秩</span></span><br><span class="line">    <span class="built_in">int</span> tag,</span><br><span class="line">    MPI_Comm communicator,</span><br><span class="line">    MPI_Status* status)</span><br></pre></td></tr></table></figure></div>

<h2 id="基础-MPI-数据结构"><a href="#基础-MPI-数据结构" class="headerlink" title="基础 MPI 数据结构"></a>基础 MPI 数据结构</h2><p><code>MPI_send</code> 和 <code>MPI_Recv</code> 方法使用了 MPI 的数据结构作为一种在更高层次指定消息结构的方法（<strong>重新定义了发送消息的数据结构</strong>）。举例来说，如果一个进程想要发送一个整数给另一个进程，它会指定 count 为 1，数据结构为 <code>MPI_INT</code>。其他的 MPI 数据结构以及它们在 C 语言里对应的结构如下：</p>
<table>
<thead>
<tr>
<th>MPI datatype</th>
<th>C equivalent</th>
</tr>
</thead>
<tbody><tr>
<td>MPI_SHORT</td>
<td>short int</td>
</tr>
<tr>
<td>MPI_INT</td>
<td>int</td>
</tr>
<tr>
<td>MPI_LONG</td>
<td>long int</td>
</tr>
<tr>
<td>MPI_LONG_LONG</td>
<td>long long int</td>
</tr>
<tr>
<td>MPI_UNSIGNED_CHAR</td>
<td>unsigned char</td>
</tr>
<tr>
<td>MPI_UNSIGNED_SHORT</td>
<td>unsigned short int</td>
</tr>
<tr>
<td>MPI_UNSIGNED</td>
<td>unsigned int</td>
</tr>
<tr>
<td>MPI_UNSIGNED_LONG</td>
<td>unsigned long int</td>
</tr>
<tr>
<td>MPI_UNSIGNED_LONG_LONG</td>
<td>unsigned long long int</td>
</tr>
<tr>
<td>MPI_FLOAT</td>
<td>float</td>
</tr>
<tr>
<td>MPI_DOUBLE</td>
<td>double</td>
</tr>
<tr>
<td>MPI_LONG_DOUBLE</td>
<td>long double</td>
</tr>
<tr>
<td>MPI_BYTE</td>
<td>char</td>
</tr>
</tbody></table>
<h2 id="MPI-发送-x2F-接收-程序"><a href="#MPI-发送-x2F-接收-程序" class="headerlink" title="MPI 发送 &#x2F; 接收 程序"></a>MPI 发送 &#x2F; 接收 程序</h2><div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 得到当前进程的 rank 以及整个 communicator 的大小</span></span><br><span class="line"><span class="type">int</span> world_rank;</span><br><span class="line">MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"><span class="type">int</span> world_size;</span><br><span class="line">MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> number;</span><br><span class="line"><span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    number = <span class="number">-1</span>;</span><br><span class="line">    MPI_Send(&amp;number, <span class="number">1</span>, MPI_INT, <span class="number">1</span>, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (world_rank == <span class="number">1</span>) &#123;</span><br><span class="line">    MPI_Recv(&amp;number, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD,</span><br><span class="line">             MPI_STATUS_IGNORE);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Process 1 received number %d from process 0\n&quot;</span>,</span><br><span class="line">           number);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>如果当前进程是 0 进程，那么我们就初始化一个数字 -1 然后把它发送给 1 进程。然后你可以看到 <code>else if</code> 条件语句里的话题，进程 1 会调用 <code>MPI_Recv</code> 去接受这个数字。然后会将接收到的数字打印出来。由于我们明确地发送接收了一个整数，因此 <code>MPI_INT</code> 数据类型被使用了。每个进程还使用了 0 作为消息标签来指定消息。由于我们这里只有一种类型的信息被传递了，因此进程也可以使用预先定义好的常量 <code>MPI_ANY_TAG</code> 来作为标签数字。</p>
<h2 id="MPI-乒乓程序"><a href="#MPI-乒乓程序" class="headerlink" title="MPI 乒乓程序"></a>MPI 乒乓程序</h2><p>接下来的程序比较有趣，是一个乒乓游戏。两个进程会一直使用 <code>MPI_Send</code> 和 <code>MPI_Recv</code> 方法来“推挡”消息，直到他们决定不玩了。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> PING_PONG_LIMIT = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Initialize the MPI environment</span></span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">  <span class="comment">// Find out rank, size</span></span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// We are assuming 2 processes for this task</span></span><br><span class="line">  <span class="keyword">if</span> (world_size != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;World size must be two for %s\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">    MPI_Abort(MPI_COMM_WORLD, <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> ping_pong_count = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> partner_rank = (world_rank + <span class="number">1</span>) % <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">while</span> (ping_pong_count &lt; PING_PONG_LIMIT) &#123;				<span class="comment">//10次以后退出程序</span></span><br><span class="line">    <span class="keyword">if</span> (world_rank == ping_pong_count % <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="comment">// Increment the ping pong count before you send it</span></span><br><span class="line">      ping_pong_count++;</span><br><span class="line">      MPI_Send(&amp;ping_pong_count, <span class="number">1</span>, MPI_INT, partner_rank, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%d sent and incremented ping_pong_count %d to %d\n&quot;</span>,</span><br><span class="line">             world_rank, ping_pong_count, partner_rank);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      MPI_Recv(&amp;ping_pong_count, <span class="number">1</span>, MPI_INT, partner_rank, <span class="number">0</span>, MPI_COMM_WORLD,</span><br><span class="line">               MPI_STATUS_IGNORE);</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%d received ping_pong_count %d from %d\n&quot;</span>,</span><br><span class="line">             world_rank, ping_pong_count, partner_rank);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>随着 <code>ping_pong_count</code> 的递增，两个进程会轮流成为发送者和接受者。最后，当我们设定的 limit 被触发的时候，进程就停止了发送和接收。这个程序在其他机器上运行的输出可能会由于进程调度的不同跟上面的不一样。不管怎么样，你可以看到，进程0和进程1在轮流发送和接收 ping_pong_count。</p>
<h2 id="环程序"><a href="#环程序" class="headerlink" title="环程序"></a>环程序</h2><p>一个值会在各个进程之间以一个环的形式传递。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  <span class="comment">// Initialize the MPI environment</span></span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">  <span class="comment">// Find out rank, size</span></span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> token;</span><br><span class="line">  <span class="comment">// Receive from the lower process and send to the higher process. Take care</span></span><br><span class="line">  <span class="comment">// of the special case when you are the first process to prevent deadlock.</span></span><br><span class="line">  <span class="keyword">if</span> (world_rank != <span class="number">0</span>) &#123;</span><br><span class="line">    MPI_Recv(&amp;token, <span class="number">1</span>, MPI_INT, world_rank - <span class="number">1</span>, <span class="number">0</span>, MPI_COMM_WORLD,</span><br><span class="line">             MPI_STATUS_IGNORE);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Process %d received token %d from process %d\n&quot;</span>, world_rank, token,</span><br><span class="line">           world_rank - <span class="number">1</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Set the token&#x27;s value if you are process 0</span></span><br><span class="line">    token = <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  MPI_Send(&amp;token, <span class="number">1</span>, MPI_INT, (world_rank + <span class="number">1</span>) % world_size, <span class="number">0</span>,</span><br><span class="line">           MPI_COMM_WORLD);</span><br><span class="line">  <span class="comment">// Now process 0 can receive from the last process. This makes sure that at</span></span><br><span class="line">  <span class="comment">// least one MPI_Send is initialized before all MPI_Recvs (again, to prevent</span></span><br><span class="line">  <span class="comment">// deadlock)</span></span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    MPI_Recv(&amp;token, <span class="number">1</span>, MPI_INT, world_size - <span class="number">1</span>, <span class="number">0</span>, MPI_COMM_WORLD,</span><br><span class="line">             MPI_STATUS_IGNORE);				<span class="comment">//默认不接受MPI_Status结构体的参数</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Process %d received token %d from process %d\n&quot;</span>, world_rank, token,</span><br><span class="line">           world_size - <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>这个环程序在进程0上面初始化了一个值-1，赋值给 token。然后这个值会依次传递给每个进程。程序会在进程0从最后一个进程接收到值之后结束。如你所见，我们的逻辑避免了死锁的发生。具体来说，进程0保证了在想要接受数据之前发送了 token。所有其他的进程只是简单的调用 <code>MPI_Recv</code> (从他们的邻居进程接收数据)，然后调用 <code>MPI_Send</code> (发送数据到他们的邻居进程)把数据从环上传递下去。 <code>MPI_Send</code> 和 <code>MPI_Recv</code> 会阻塞直到数据传递完成。因为这个特性，打印出来的数据是跟数据传递的次序一样的</p>
<h1 id="Dynamic-Receiving-with-MPI-Probe-and-MPI-Status"><a href="#Dynamic-Receiving-with-MPI-Probe-and-MPI-Status" class="headerlink" title="Dynamic Receiving with MPI Probe (and MPI Status)"></a>Dynamic Receiving with MPI Probe (and MPI Status)</h1><h2 id="MPI-Status-结构体"><a href="#MPI-Status-结构体" class="headerlink" title="MPI_Status 结构体"></a>MPI_Status 结构体</h2><p><code>MPI_Recv</code> 将 <code>MPI_Status</code> 结构体的地址作为参数（可以使用 <code>MPI_STATUS_IGNORE</code> 忽略）。 如果我们将 <code>MPI_Status</code> 结构体传递给 <code>MPI_Recv</code> 函数，则操作完成后将在该结构体中填充有关接收操作的其他信息。 三个主要的信息包括：</p>
<ol>
<li><strong>发送端秩</strong>. 发送端的秩存储在结构体的 <code>MPI_SOURCE</code> 元素中。也就是说，如果我们声明一个 <code>MPI_Status stat</code> 变量，则可以通过 <code>stat.MPI_SOURCE</code> 访问秩。</li>
<li><strong>消息的标签</strong>. 消息的标签可以通过结构体的 <code>MPI_TAG</code> 元素访问（类似于 <code>MPI_SOURCE</code>）。</li>
<li><strong>消息的长度</strong>. 消息的长度在结构体中没有预定义的元素。相反，我们必须使用 <code>MPI_Get_count</code> 找出消息的长度。</li>
</ol>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MPI_Get_count(</span><br><span class="line">    MPI_Status* status,			<span class="comment">//MPI_Status 结构体</span></span><br><span class="line">    MPI_Datatype datatype,		<span class="comment">//数据类型</span></span><br><span class="line">    <span class="type">int</span>* count)					<span class="comment">//已接受datatype的元素的数目</span></span><br></pre></td></tr></table></figure></div>

<p> 事实证明，<code>MPI_Recv</code> 可以将 <code>MPI_ANY_SOURCE</code> 用作发送端的秩，将 <code>MPI_ANY_TAG</code> 用作消息的标签。 在这种情况下，<code>MPI_Status</code> 结构体是找出消息的<strong>实际发送端和标签的唯一方法</strong>。 此外，并不能保证 <code>MPI_Recv</code> 能够接收函数调用参数的全部元素。 相反，它只接收已发送给它的元素数量（如果发送的元素多于所需的接收数量，则返回错误。） <code>MPI_Get_count</code> 函数用于确定实际的接收量。</p>
<h2 id="MPI-Status-结构体查询的示例"><a href="#MPI-Status-结构体查询的示例" class="headerlink" title="MPI_Status 结构体查询的示例"></a><code>MPI_Status</code> 结构体查询的示例</h2><p>程序将随机数量的数字发送给接收端，然后接收端找出发送了多少个数字</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line">  <span class="keyword">if</span> (world_size != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;Must use two processes for this example\n&quot;</span>);</span><br><span class="line">    MPI_Abort(MPI_COMM_WORLD, <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> MAX_NUMBERS = <span class="number">100</span>;</span><br><span class="line">  <span class="type">int</span> numbers[MAX_NUMBERS];</span><br><span class="line">  <span class="type">int</span> number_amount;</span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// Pick a random amount of integers to send to process one</span></span><br><span class="line">    srand(time(<span class="literal">NULL</span>));	<span class="comment">//srand()用于设置随机数生成器的种子，种子是随机数生成器的一个重要参数，它用于初始化生成器并确定生成的随机数序列。如果使用相同的种子，随机数生成器将生成相同的随机数序列。一般以当前时间作为种子</span></span><br><span class="line">      </span><br><span class="line">    number_amount = (rand() / (<span class="type">float</span>)RAND_MAX) * MAX_NUMBERS;</span><br><span class="line">    <span class="comment">// Send the amount of integers to process one</span></span><br><span class="line">    MPI_Send(numbers, number_amount, MPI_INT, <span class="number">1</span>, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;0 sent %d numbers to 1\n&quot;</span>, number_amount);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (world_rank == <span class="number">1</span>) &#123;</span><br><span class="line">    MPI_Status status;</span><br><span class="line">    <span class="comment">// Receive at most MAX_NUMBERS from process zero</span></span><br><span class="line">    MPI_Recv(numbers, MAX_NUMBERS, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD, &amp;status);</span><br><span class="line">    <span class="comment">// After receiving the message, check the status to determine how many</span></span><br><span class="line">    <span class="comment">// numbers were actually received</span></span><br><span class="line">    MPI_Get_count(&amp;status, MPI_INT, &amp;number_amount);</span><br><span class="line">    <span class="comment">// Print off the amount of numbers, and also print additional information</span></span><br><span class="line">    <span class="comment">// in the status object</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;1 received %d numbers from 0. Message source = %d, tag = %d\n&quot;</span>,</span><br><span class="line">           number_amount, status.MPI_SOURCE, status.MPI_TAG);</span><br><span class="line">  &#125;</span><br><span class="line">  MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>如我们所见，进程 0 将最多 <code>MAX_NUMBERS</code> 个整数以随机数量发送到进程 1。 进程 1 然后调用 <code>MPI_Recv</code> 以获取总计 <code>MAX_NUMBERS</code> 个整数。 <strong>尽管进程 1 以 <code>MAX_NUMBERS</code> 作为 <code>MPI_Recv</code> 函数参数</strong>，但进程 1 将最多接收到此数量的数字。 在代码中，<strong>进程 1 使用 <code>MPI_INT</code> 作为数据类型的参数，调用 <code>MPI_Get_count</code>，以找出实际接收了多少个整数。</strong> 除了打印出接收到的消息的大小外，进程 1 还通过访问 status 结构体的 <code>MPI_SOURCE</code> 和 <code>MPI_TAG</code> 元素来打印消息的来源和标签。</p>
<p><code>MPI_Get_count</code> 的返回值是相对于传递的数据类型而言的</p>
<h2 id="使用-MPI-Probe-找出消息大小"><a href="#使用-MPI-Probe-找出消息大小" class="headerlink" title="使用 MPI_Probe 找出消息大小"></a>使用 <code>MPI_Probe</code> 找出消息大小</h2><p>除了传递接收消息并简易地配备一个很大的缓冲区来为所有可能的大小的消息提供处理（就像我们在上一个示例中所做的那样），您可以<strong>使用 <code>MPI_Probe</code> 在实际接收消息之前查询消息大小。</strong> 函数原型看起来像这样：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MPI_Probe(</span><br><span class="line">    <span class="type">int</span> source,</span><br><span class="line">    <span class="type">int</span> tag,</span><br><span class="line">    MPI_Comm comm,</span><br><span class="line">    MPI_Status* status)</span><br></pre></td></tr></table></figure></div>

<p><code>MPI_Probe</code> 看起来与 <code>MPI_Recv</code> 非常相似。 实际上，您可以将 <code>MPI_Probe</code> 视为 <code>MPI_Recv</code>，除了不接收消息外，它们执行相同的功能。 与 <code>MPI_Recv</code> 类似，**<code>MPI_Probe</code> 将阻塞具有匹配标签和发送端的消息。 当消息可用时，它将填充 status 结构体**。 然后，用户可以使用 <code>MPI_Recv</code> 接收实际的消息。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> number_amount;</span><br><span class="line"><span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> MAX_NUMBERS = <span class="number">100</span>;</span><br><span class="line">    <span class="type">int</span> numbers[MAX_NUMBERS];</span><br><span class="line">    <span class="comment">// Pick a random amount of integers to send to process one</span></span><br><span class="line">    srand(time(<span class="literal">NULL</span>));</span><br><span class="line">    number_amount = (rand() / (<span class="type">float</span>)RAND_MAX) * MAX_NUMBERS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Send the random amount of integers to process one</span></span><br><span class="line">    MPI_Send(numbers, number_amount, MPI_INT, <span class="number">1</span>, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;0 sent %d numbers to 1\n&quot;</span>, number_amount);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (world_rank == <span class="number">1</span>) &#123;</span><br><span class="line">    MPI_Status status;</span><br><span class="line">    <span class="comment">// Probe for an incoming message from process zero</span></span><br><span class="line">    MPI_Probe(<span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD, &amp;status);		<span class="comment">//在receive之前</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// When probe returns, the status object has the size and other</span></span><br><span class="line">    <span class="comment">// attributes of the incoming message. Get the message size</span></span><br><span class="line">    MPI_Get_count(&amp;status, MPI_INT, &amp;number_amount);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate a buffer to hold the incoming numbers</span></span><br><span class="line">    <span class="type">int</span>* number_buf = (<span class="type">int</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>) * number_amount);		<span class="comment">//缓冲区</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Now receive the message with the allocated buffer</span></span><br><span class="line">    MPI_Recv(number_buf, number_amount, MPI_INT, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">             MPI_COMM_WORLD, MPI_STATUS_IGNORE);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;1 dynamically received %d numbers from 0.\n&quot;</span>,</span><br><span class="line">           number_amount);</span><br><span class="line">    <span class="built_in">free</span>(number_buf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h1 id="MPI-广播以及集体-collective-通信"><a href="#MPI-广播以及集体-collective-通信" class="headerlink" title="MPI 广播以及集体(collective)通信"></a>MPI 广播以及集体(collective)通信</h1><p>集体通信指的是一个涉及 communicator 里面所有进程的一个方法。解释集体通信以及一个标准的方法 - broadcasting (广播)</p>
<h2 id="集体通信以及同步点"><a href="#集体通信以及同步点" class="headerlink" title="集体通信以及同步点"></a>集体通信以及同步点</h2><p>关于集体通信需要记住的一点是它在进程间引入了<strong>同步点</strong>的概念。这意味着所有的进程在执行代码的时候必须首先<em>都</em>到达一个同步点才能继续执行后面的代码。</p>
<p>在看具体的集体通信方法之前，让我们更仔细地看一下同步这个概念。事实上，MPI 有一个特殊的函数来做同步进程的这个操作</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MPI_Barrier(MPI_Comm communicator)</span><br></pre></td></tr></table></figure></div>

<p>这个函数的名字十分贴切（Barrier，屏障）- 这个方法会构建一个屏障，任何进程都没法跨越屏障，直到所有的进程都到达屏障</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406190258065.png"
                      class="" title="image-20230406190258065"
                >

<p><code>MPI_Barrier</code> 在很多时候很有用。其中一个用途是用来同步一个程序，使得分布式代码中的某一部分可以被精确的计时。</p>
<p>关于同步最后一个要注意的地方是：<strong>始终记得每一个你调用的集体通信方法都是同步的</strong>。也就是说，如果你没法让所有进程都完成 <code>MPI_Barrier</code>，那么你也没法完成任何集体调用。如果你在没有确保所有进程都调用 <code>MPI_Barrier</code> 的情况下调用了它，那么程序会空闲下来。</p>
<h2 id="使用-MPI-Bcast-来进行广播"><a href="#使用-MPI-Bcast-来进行广播" class="headerlink" title="使用 MPI_Bcast 来进行广播"></a>使用 MPI_Bcast 来进行广播</h2><p><em><strong>广播</strong></em> (broadcast) 是标准的集体通信技术之一。一个广播发生的时候，一个进程会把同样一份数据传递给一个 communicator 里的所有其他进程。广播的主要用途之一是把用户输入传递给一个分布式程序，或者把一些配置参数传递给所有的进程。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406190643521.png"
                      class="" title="image-20230406190643521"
                >

<p>在 MPI 里面，广播可以使用 <code>MPI_Bcast</code> 来做到：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MPI_Bcast(</span><br><span class="line">    void* data,</span><br><span class="line">    int count,</span><br><span class="line">    MPI_Datatype datatype,</span><br><span class="line">    int root,</span><br><span class="line">    MPI_Comm communicator)</span><br></pre></td></tr></table></figure></div>

<p>根节点和接收节点都是调用同样的这个 <code>MPI_Bcast</code> 函数来实现广播。当根节点(在我们的例子是节点0)调用 <code>MPI_Bcast</code> 函数的时候，<code>data</code> 变量里的值会被发送到其他的节点上。当其他的节点调用 <code>MPI_Bcast</code> 的时候，<code>data</code> 变量会被赋值成从根节点接受到的数据。</p>
<h2 id="使用-MPI-Send-和-MPI-Recv-来做广播"><a href="#使用-MPI-Send-和-MPI-Recv-来做广播" class="headerlink" title="使用 MPI_Send 和 MPI_Recv 来做广播"></a>使用 MPI_Send 和 MPI_Recv 来做广播</h2><div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">my_bcast</span><span class="params">(<span class="type">void</span>* data, <span class="type">int</span> count, MPI_Datatype datatype, <span class="type">int</span> root,</span></span><br><span class="line"><span class="params">              MPI_Comm communicator)</span> &#123;</span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(communicator, &amp;world_rank);</span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(communicator, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (world_rank == root) &#123;</span><br><span class="line">    <span class="comment">// If we are the root process, send our data to everyone</span></span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; world_size; i++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (i != world_rank) &#123;</span><br><span class="line">        MPI_Send(data, count, datatype, i, <span class="number">0</span>, communicator);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If we are a receiver process, receive the data from the root</span></span><br><span class="line">    MPI_Recv(data, count, datatype, root, <span class="number">0</span>, communicator, MPI_STATUS_IGNORE);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> data;</span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    data = <span class="number">100</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Process 0 broadcasting data %d\n&quot;</span>, data);</span><br><span class="line">    my_bcast(&amp;data, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    my_bcast(&amp;data, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Process %d received data %d from root process\n&quot;</span>, world_rank, data);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>上述的算法函数效率特别低！假设每个进程都只有一个「输出&#x2F;输入」网络连接。我们的方法只是使用了进程0的一个输出连接来传递数据。比较聪明的方法是使用一个基于树的沟通算法对网络进行更好的利用</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406192458314.png"
                      class="" title="image-20230406192458314"
                >

<p>在示意图里，<strong>进程0一开始传递数据给进程1</strong>。跟我们之前的例子类似，<strong>第二个阶段的时候进程0依旧会把数据传递给进程2。这个例子中不同的是进程1在第二阶段也会传递数据给进程3</strong>。在第二阶段，两个网络连接在同时发生了。在这个树形算法里，能够利用的网络连接每<strong>个阶段都会比前一阶段翻番</strong>，直到所有的进程接受到数据为止。</p>
<h2 id="MPI-Bcast-和-MPI-Send-以及-MPI-Recv-的比较"><a href="#MPI-Bcast-和-MPI-Send-以及-MPI-Recv-的比较" class="headerlink" title="MPI_Bcast 和 MPI_Send 以及 MPI_Recv 的比较"></a>MPI_Bcast 和 MPI_Send 以及 MPI_Recv 的比较</h2><div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">my_bcast</span><span class="params">(<span class="type">void</span>* data, <span class="type">int</span> count, MPI_Datatype datatype, <span class="type">int</span> root,</span></span><br><span class="line"><span class="params">              MPI_Comm communicator)</span> &#123;</span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(communicator, &amp;world_rank);</span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(communicator, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (world_rank == root) &#123;</span><br><span class="line">    <span class="comment">// If we are the root process, send our data to everyone</span></span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; world_size; i++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (i != world_rank) &#123;</span><br><span class="line">        MPI_Send(data, count, datatype, i, <span class="number">0</span>, communicator);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If we are a receiver process, receive the data from the root</span></span><br><span class="line">    MPI_Recv(data, count, datatype, root, <span class="number">0</span>, communicator, MPI_STATUS_IGNORE);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">3</span>) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;Usage: compare_bcast num_elements num_trials\n&quot;</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> num_elements = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="type">int</span> num_trials = atoi(argv[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"></span><br><span class="line">  <span class="type">double</span> total_my_bcast_time = <span class="number">0.0</span>;</span><br><span class="line">  <span class="type">double</span> total_mpi_bcast_time = <span class="number">0.0</span>;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="type">int</span>* data = (<span class="type">int</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>) * num_elements);</span><br><span class="line">  assert(data != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_trials; i++) &#123;</span><br><span class="line">    <span class="comment">// Time my_bcast</span></span><br><span class="line">    <span class="comment">// Synchronize before starting timing</span></span><br><span class="line">    MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">    total_my_bcast_time -= MPI_Wtime();</span><br><span class="line">    my_bcast(data, num_elements, MPI_INT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    <span class="comment">// Synchronize again before obtaining final time</span></span><br><span class="line">    MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">    total_my_bcast_time += MPI_Wtime();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Time MPI_Bcast</span></span><br><span class="line">    MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">    total_mpi_bcast_time -= MPI_Wtime();</span><br><span class="line">    MPI_Bcast(data, num_elements, MPI_INT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">    total_mpi_bcast_time += MPI_Wtime();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Print off timing information</span></span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Data size = %d, Trials = %d\n&quot;</span>, num_elements * (<span class="type">int</span>)<span class="keyword">sizeof</span>(<span class="type">int</span>),</span><br><span class="line">           num_trials);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Avg my_bcast time = %lf\n&quot;</span>, total_my_bcast_time / num_trials);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Avg MPI_Bcast time = %lf\n&quot;</span>, total_mpi_bcast_time / num_trials);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(data);</span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>代码里的 <code>num_trials</code> 是一个指明一共要运行多少次实验的变量。我们分别记录两个函数运行所需的累加时间，平均的时间会在程序结束的时候打印出来</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406201220958.png"
                      class="" title="image-20230406201220958"
                >

<p>可以看到，2个进程运行的时候是没有时间差异的。这是因为 <code>MPI_Bcast</code> 的树算法在使用两个进程的时候并没有提供额外的网络利用率。然而，进程数量稍微增加到即使只有16个的时候我们也可以看到明显的差异。</p>
<h1 id="MPI-Scatter-Gather-and-Allgather"><a href="#MPI-Scatter-Gather-and-Allgather" class="headerlink" title="MPI Scatter, Gather, and Allgather"></a>MPI Scatter, Gather, and Allgather</h1><h2 id="MPI-Scatter-的介绍"><a href="#MPI-Scatter-的介绍" class="headerlink" title="MPI_Scatter 的介绍"></a>MPI_Scatter 的介绍</h2><p><code>MPI_Scatter</code> 是一个跟 <code>MPI_Bcast</code> 类似的集体通信机制。<code>MPI_Scatter</code> 的操作会设计一个指定的根进程，根进程会将数据发送到 communicator 里面的所有进程。<code>MPI_Bcast</code> 和 <code>MPI_Scatter</code> 的主要区别很小但是很重要。<code>MPI_Bcast</code> 给每个进程发送的是<em>同样</em>的数据，<strong>然而 <code>MPI_Scatter</code> 给每个进程发送的是<em>一个数组的一部分数据</em></strong>。下图进一步展示了这个区别。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406201727561.png"
                      class="" title="image-20230406201727561"
                >

<p>在图中我们可以看到，<code>MPI_Bcast</code> 在根进程上接收一个单独的数据元素（红色的方块），然后把它复制到所有其他的进程。<code>MPI_Scatter</code> 接收一个数组，并把元素按进程的<strong>秩</strong>分发出去。第一个元素（红色方块）发往进程0，第二个元素（绿色方块）发往进程1，以此类推。尽管根进程（进程0）拥有整个数组的所有元素，<code>MPI_Scatter</code> 还是会把正确的属于进程0的元素放到这个进程的接收缓存中。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MPI_Scatter(</span><br><span class="line">    <span class="type">void</span>* send_data,				<span class="comment">//根进程上的一个数据数组</span></span><br><span class="line">    <span class="type">int</span> send_count,					<span class="comment">//发送给每个进程的数据数量.一般来说send_count会等于数组的长度除以进程的数量</span></span><br><span class="line">    MPI_Datatype send_datatype,		<span class="comment">//数据类型</span></span><br><span class="line">    <span class="type">void</span>* recv_data,				<span class="comment">//缓存：保存recv_count个recv_datatype数据类型的元素</span></span><br><span class="line">    <span class="type">int</span> recv_count,</span><br><span class="line">    MPI_Datatype recv_datatype,</span><br><span class="line">    <span class="type">int</span> root,						<span class="comment">//根进程</span></span><br><span class="line">    MPI_Comm communicator)			<span class="comment">//通讯器</span></span><br></pre></td></tr></table></figure></div>

<h2 id="MPI-Gather-的介绍"><a href="#MPI-Gather-的介绍" class="headerlink" title="MPI_Gather 的介绍"></a>MPI_Gather 的介绍</h2><p><code>MPI_Gather</code> 跟 <code>MPI_Scatter</code> 是相反的。**<code>MPI_Gather</code> 从好多进程里面收集数据到一个进程上面**而不是从一个进程分发数据到多个进程。这个机制对很多平行算法很有用，比如并行的排序和搜索。下图是这个算法的一个示例。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406202312516.png"
                      class="" title="image-20230406202312516"
                >

<p><code>MPI_Gather</code>的函数原型跟<code>MPI_Scatter</code>长的一样：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MPI_Gather(</span><br><span class="line">    void* send_data,</span><br><span class="line">    int send_count,</span><br><span class="line">    MPI_Datatype send_datatype,</span><br><span class="line">    void* recv_data,</span><br><span class="line">    int recv_count,</span><br><span class="line">    MPI_Datatype recv_datatype,</span><br><span class="line">    int root,</span><br><span class="line">    MPI_Comm communicator)</span><br></pre></td></tr></table></figure></div>

<p>在<code>MPI_Gather</code>中，只有根进程需要一个有效的接收缓存。所有其他的调用进程可以传递<code>NULL</code>给<code>recv_data</code>。另外，别忘记<em>recv_count</em>参数是从<em>每个进程</em>接收到的数据数量，而不是所有进程的数据总量之和。</p>
<h2 id="使用-MPI-Scatter-和-MPI-Gather-来计算平均数"><a href="#使用-MPI-Scatter-和-MPI-Gather-来计算平均数" class="headerlink" title="使用 MPI_Scatter 和 MPI_Gather 来计算平均数"></a>使用 <code>MPI_Scatter</code> 和 <code>MPI_Gather</code> 来计算平均数</h2><p>提供了一个用来计算数组里面所有数字的平均数的样例程序（<a class="link"   target="_blank" rel="noopener" href="https://github.com/mpitutorial/mpitutorial/tree/gh-pages/tutorials/mpi-scatter-gather-and-allgather/code/avg.c" >avg.c <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）。尽管这个程序十分简单，但是它展示了我们如何使用MPI来把工作拆分到不同的进程上，每个进程对一部分数据进行计算，然后再把每个部分计算出来的结果汇集成最终的答案。这个程序有以下几个步骤：</p>
<ol>
<li>在根进程（进程0）上生成一个充满随机数字的数组。</li>
<li>把所有数字用<code>MPI_Scatter</code>分发给每个进程，每个进程得到的同样多的数字。</li>
<li>每个进程计算它们各自得到的数字的平均数。</li>
<li>根进程收集所有的平均数，然后计算这个平均数的平均数，得出最后结果。</li>
</ol>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates an array of random numbers. Each number has a value from 0 - 1</span></span><br><span class="line"><span class="type">float</span> *<span class="title function_">create_rand_nums</span><span class="params">(<span class="type">int</span> num_elements)</span> &#123;</span><br><span class="line">  <span class="type">float</span> *rand_nums = (<span class="type">float</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">float</span>) * num_elements);</span><br><span class="line">  assert(rand_nums != <span class="literal">NULL</span>);</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_elements; i++) &#123;</span><br><span class="line">    rand_nums[i] = (rand() / (<span class="type">float</span>)RAND_MAX);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> rand_nums;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Computes the average of an array of numbers</span></span><br><span class="line"><span class="type">float</span> <span class="title function_">compute_avg</span><span class="params">(<span class="type">float</span> *<span class="built_in">array</span>, <span class="type">int</span> num_elements)</span> &#123;</span><br><span class="line">  <span class="type">float</span> sum = <span class="number">0.f</span>;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_elements; i++) &#123;</span><br><span class="line">    sum += <span class="built_in">array</span>[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> sum / num_elements;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;Usage: avg num_elements_per_proc\n&quot;</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> num_elements_per_proc = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="comment">// Seed the random number generator to get different results each time</span></span><br><span class="line">  srand(time(<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a random array of elements on the root process. Its total</span></span><br><span class="line">  <span class="comment">// size will be the number of elements per process times the number</span></span><br><span class="line">  <span class="comment">// of processes</span></span><br><span class="line">  <span class="type">float</span> *rand_nums = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    rand_nums = create_rand_nums(num_elements_per_proc * world_size);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// For each process, create a buffer that will hold a subset of the entire</span></span><br><span class="line">  <span class="comment">// array</span></span><br><span class="line">  <span class="type">float</span> *sub_rand_nums = (<span class="type">float</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">float</span>) * num_elements_per_proc);</span><br><span class="line">  assert(sub_rand_nums != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Scatter the random numbers from the root process to all processes in</span></span><br><span class="line">  <span class="comment">// the MPI world</span></span><br><span class="line">  MPI_Scatter(rand_nums, num_elements_per_proc, MPI_FLOAT, sub_rand_nums,</span><br><span class="line">              num_elements_per_proc, MPI_FLOAT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Compute the average of your subset</span></span><br><span class="line">  <span class="type">float</span> sub_avg = compute_avg(sub_rand_nums, num_elements_per_proc);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Gather all partial averages down to the root process</span></span><br><span class="line">  <span class="type">float</span> *sub_avgs = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    sub_avgs = (<span class="type">float</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">float</span>) * world_size);</span><br><span class="line">    assert(sub_avgs != <span class="literal">NULL</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  MPI_Gather(&amp;sub_avg, <span class="number">1</span>, MPI_FLOAT, sub_avgs, <span class="number">1</span>, MPI_FLOAT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Now that we have all of the partial averages on the root, compute the</span></span><br><span class="line">  <span class="comment">// total average of all numbers. Since we are assuming each process computed</span></span><br><span class="line">  <span class="comment">// an average across an equal amount of elements, this computation will</span></span><br><span class="line">  <span class="comment">// produce the correct answer.</span></span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="type">float</span> avg = compute_avg(sub_avgs, world_size);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Avg of all elements is %f\n&quot;</span>, avg);</span><br><span class="line">    <span class="comment">// Compute the average across the original data for comparison</span></span><br><span class="line">    <span class="type">float</span> original_data_avg =</span><br><span class="line">      compute_avg(rand_nums, num_elements_per_proc * world_size);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Avg computed across original data is %f\n&quot;</span>, original_data_avg);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Clean up</span></span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">free</span>(rand_nums);</span><br><span class="line">    <span class="built_in">free</span>(sub_avgs);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">free</span>(sub_rand_nums);</span><br><span class="line"></span><br><span class="line">  MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>代码开头根进程创建里一个随机数的数组。当<code>MPI_Scatter</code>被调用的时候，每个进程现在都持有<code>elements_per_proc</code>个原始数据里面的元素。每个进程计算子数组的平均数，然后根进程收集这些平均数。然后总的平均数就可以在这个小的多的平均数数组里面被计算出来。</p>
<h2 id="MPI-Allgather-以及修改后的平均程序"><a href="#MPI-Allgather-以及修改后的平均程序" class="headerlink" title="MPI_Allgather 以及修改后的平均程序"></a>MPI_Allgather 以及修改后的平均程序</h2><p>到目前为止，我们讲解了两个用来操作<em>多对一</em>或者<em>一对多</em>通信模式的MPI方法，也就是说多个进程要么向一个进程发送数据，要么从一个进程接收数据。很多时候发送多个元素到多个进程也很有用<strong>（也就是<em>多对多</em>通信模式）</strong>。<code>MPI_Allgather</code>就是这个作用。</p>
<p>对于分发在所有进程上的一组数据来说，**<code>MPI_Allgather</code>会收集所有数据到所有进程上。**从最基础的角度来看，<code>MPI_Allgather</code>相当于一个<code>MPI_Gather</code>操作之后跟着一个<code>MPI_Bcast</code>操作。下面的示意图显示了<code>MPI_Allgather</code>调用之后数据是如何分布的。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406203808578.png"
                      class="" title="image-20230406203808578"
                >

<p>就跟<code>MPI_Gather</code>一样，每个进程上的元素是根据他们的秩为顺序被收集起来的，只不过这次是收集到了所有进程上面。只不过<code>MPI_Allgather</code>不需要root这个参数来指定根节点。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MPI_Allgather(</span><br><span class="line">    <span class="type">void</span>* send_data,</span><br><span class="line">    <span class="type">int</span> send_count,</span><br><span class="line">    MPI_Datatype send_datatype,</span><br><span class="line">    <span class="type">void</span>* recv_data,</span><br><span class="line">    <span class="type">int</span> recv_count,</span><br><span class="line">    MPI_Datatype recv_datatype,</span><br><span class="line">    MPI_Comm communicator)</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>把计算平均数的代码修改成了使用<code>MPI_Allgather</code>来计算</p>
</blockquote>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates an array of random numbers. Each number has a value from 0 - 1</span></span><br><span class="line"><span class="type">float</span> *<span class="title function_">create_rand_nums</span><span class="params">(<span class="type">int</span> num_elements)</span> &#123;</span><br><span class="line">  <span class="type">float</span> *rand_nums = (<span class="type">float</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">float</span>) * num_elements);</span><br><span class="line">  assert(rand_nums != <span class="literal">NULL</span>);</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_elements; i++) &#123;</span><br><span class="line">    rand_nums[i] = (rand() / (<span class="type">float</span>)RAND_MAX);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> rand_nums;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Computes the average of an array of numbers</span></span><br><span class="line"><span class="type">float</span> <span class="title function_">compute_avg</span><span class="params">(<span class="type">float</span> *<span class="built_in">array</span>, <span class="type">int</span> num_elements)</span> &#123;</span><br><span class="line">  <span class="type">float</span> sum = <span class="number">0.f</span>;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_elements; i++) &#123;</span><br><span class="line">    sum += <span class="built_in">array</span>[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> sum / num_elements;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;Usage: avg num_elements_per_proc\n&quot;</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> num_elements_per_proc = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="comment">// Seed the random number generator to get different results each time</span></span><br><span class="line">  srand(time(<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line">  <span class="type">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a random array of elements on the root process. Its total</span></span><br><span class="line">  <span class="comment">// size will be the number of elements per process times the number</span></span><br><span class="line">  <span class="comment">// of processes</span></span><br><span class="line">  <span class="type">float</span> *rand_nums = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    rand_nums = create_rand_nums(num_elements_per_proc * world_size);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// For each process, create a buffer that will hold a subset of the entire</span></span><br><span class="line">  <span class="comment">// array</span></span><br><span class="line">  <span class="type">float</span> *sub_rand_nums = (<span class="type">float</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">float</span>) * num_elements_per_proc);</span><br><span class="line">  assert(sub_rand_nums != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Scatter the random numbers from the root process to all processes in</span></span><br><span class="line">  <span class="comment">// the MPI world</span></span><br><span class="line">  MPI_Scatter(rand_nums, num_elements_per_proc, MPI_FLOAT, sub_rand_nums,</span><br><span class="line">              num_elements_per_proc, MPI_FLOAT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Compute the average of your subset</span></span><br><span class="line">  <span class="type">float</span> sub_avg = compute_avg(sub_rand_nums, num_elements_per_proc);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Gather all partial averages down to all the processes</span></span><br><span class="line">  <span class="type">float</span> *sub_avgs = (<span class="type">float</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">float</span>) * world_size);</span><br><span class="line">  assert(sub_avgs != <span class="literal">NULL</span>);</span><br><span class="line">  MPI_Allgather(&amp;sub_avg, <span class="number">1</span>, MPI_FLOAT, sub_avgs, <span class="number">1</span>, MPI_FLOAT, MPI_COMM_WORLD);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Now that we have all of the partial averages, compute the</span></span><br><span class="line">  <span class="comment">// total average of all numbers. Since we are assuming each process computed</span></span><br><span class="line">  <span class="comment">// an average across an equal amount of elements, this computation will</span></span><br><span class="line">  <span class="comment">// produce the correct answer.</span></span><br><span class="line">  <span class="type">float</span> avg = compute_avg(sub_avgs, world_size);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Avg of all elements from proc %d is %f\n&quot;</span>, world_rank, avg);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Clean up</span></span><br><span class="line">  <span class="keyword">if</span> (world_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">free</span>(rand_nums);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">free</span>(sub_avgs);</span><br><span class="line">  <span class="built_in">free</span>(sub_rand_nums);</span><br><span class="line"></span><br><span class="line">  MPI_Barrier(MPI_COMM_WORLD);</span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h1 id="使用MPI计算并行排名"><a href="#使用MPI计算并行排名" class="headerlink" title="使用MPI计算并行排名"></a>使用MPI计算并行排名</h1><h2 id="并行排名-问题概述"><a href="#并行排名-问题概述" class="headerlink" title="并行排名 - 问题概述"></a>并行排名 - 问题概述</h2><p>当每一个进程都在其本地内存中存储了一个数，所有进程中存储的数字构成了一个<strong>数字集合</strong>（set of numbers），了解该数相对于整个数字集合的顺序是有用的。例如，用户可能正在对MPI群集中的处理器进行基准测试，并想知道每个处理器相对于其他处理器有多快。这个信息可用于安排、调度任务等。可以想象，如果所有其他数字分散在各个进程中，那么很难找出一个数字相对于所有其他数字的顺序。这个并行排名问题是我们在本课中要解决的问题。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230406204451384.png"
                      class="" title="image-20230406204451384"
                >

<p>图示中的进程（标记为0到3）开始时有四个数字—— 5、2、7和4。然后，并行排名算法算出进程1在数字集合中的排名为0（即第一个数字），进程3排名为1，进程0排名为2，进程2排在整个数字集合的最后。</p>
<h2 id="并行排名API定义"><a href="#并行排名API定义" class="headerlink" title="并行排名API定义"></a>并行排名API定义</h2><p>在深入研究并行排名问题之前，让我们首先确定函数的行为方式。我们的函数需要在每个进程上取一个数字，并返回其相对于所有其他进程中的数字的排名。与此同时，我们将需要其他各种信息，例如正在使用的通讯器（communicator）以及被排名的数字的数据类型。 给定这个函数定义后，我们的排名函数原型如下所示：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TMPI_Rank(</span><br><span class="line">    <span class="type">void</span> *send_data,				<span class="comment">//进程的数据</span></span><br><span class="line">    <span class="type">void</span> *recv_data,				<span class="comment">//排名</span></span><br><span class="line">    MPI_Datatype datatype,</span><br><span class="line">    MPI_Comm comm)					<span class="comment">//通讯器</span></span><br></pre></td></tr></table></figure></div>

<blockquote>
<p><strong>注意</strong> - MPI标准明确指出，用户不应以 <code>MPI</code> 起头命名自己的函数，如 <code>MPI_&lt;something&gt;</code>，以避免将用户函数与MPI标准本身的函数混淆。 因此，在这些教程中，我们将在函数前面加上 <code>T</code>。</p>
</blockquote>
<h2 id="解决并行排名问题"><a href="#解决并行排名问题" class="headerlink" title="解决并行排名问题"></a>解决并行排名问题</h2><p>解决并行排名问题的<strong>第一步是对所有进程中的数字进行排序</strong>。 这一点必须做到，以便我们找到整个数字集中每个数字的排名。我们可以通过多种方式来实现这一目标。最简单的方法是将所有数字收集到一个进程中并对数字进行排序。在示例代码(<a class="link"   target="_blank" rel="noopener" href="https://github.com/mpitutorial/mpitutorial/tree/gh-pages/tutorials/performing-parallel-rank-with-mpi/code/tmpi_rank.c" >tmpi_rank.c <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>)中，<code>gather_numbers_to_root</code> 函数负责将所有数字收集到根进程（root process）。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为进程0的TMPI_Rank收集数字。为MPI的数据类型分配空间</span></span><br><span class="line"><span class="comment">// 对进程0返回 void * 指向的缓冲区</span></span><br><span class="line"><span class="comment">// 对所有其他进程返回NULL</span></span><br><span class="line"><span class="type">void</span> *<span class="title function_">gather_numbers_to_root</span><span class="params">(<span class="type">void</span> *number, MPI_Datatype datatype,</span></span><br><span class="line"><span class="params">                             MPI_Comm comm)</span> &#123;</span><br><span class="line">  <span class="type">int</span> comm_rank, comm_size;</span><br><span class="line">  MPI_Comm_rank(comm, &amp;comm_rank);</span><br><span class="line">  MPI_Comm_size(comm, &amp;comm_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 在根进程上分配一个数组</span></span><br><span class="line">  <span class="comment">// 数组大小取决于所用的MPI数据类型</span></span><br><span class="line">  <span class="type">int</span> datatype_size;</span><br><span class="line">  MPI_Type_size(datatype, &amp;datatype_size);</span><br><span class="line">  <span class="type">void</span> *gathered_numbers;</span><br><span class="line">  <span class="keyword">if</span> (comm_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    gathered_numbers = <span class="built_in">malloc</span>(datatype_size * comm_size);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 在根进程上收集所有数字</span></span><br><span class="line">  MPI_Gather(number, <span class="number">1</span>, datatype, gathered_numbers, <span class="number">1</span>,</span><br><span class="line">             datatype, <span class="number">0</span>, comm);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> gathered_numbers;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><code>gather_numbers_to_root</code> 函数获取要收集的数字（即 <code>send_data</code> 变量）、数字的数据类型 <code>datatype</code> 和 <code>comm</code> 通讯器。根进程必须在此函数中收集 <code>comm_size</code> 个数字，因此它会分配 <code>datatype_size * comm_size</code> 长度的数组。在本教程中，通过使用新的MPI函数- <code>MPI_Type_size</code> 来收集<code>datatype_size</code>变量。尽管我们的代码仅支持将 <code>MPI_INT</code> 和 <code>MPI_FLOAT</code> 作为数据类型，但可将其扩展以支持不同大小的数据类型。 在使用 <code>MPI_Gather</code> 在根进程上收集了数字之后，必须在根进程上对数字进行排序，以确定它们排名。</p>
<h2 id="排序数字并维护所属"><a href="#排序数字并维护所属" class="headerlink" title="排序数字并维护所属"></a>排序数字并维护所属</h2><p>在我们的排名函数中，排序数字不一定是难题。 C标准库为我们提供了流行的排序算法，例如 <code>qsort</code>。 <strong>在并行排名问题中，排序的困难在于，我们必须维护各个进程将数字发送到根进程的次序。</strong> 如果我们要对收集到根进程的数组进行排序而不给数字附加信息，则根进程将不知道如何将数字的排名发送回原来请求的进程！</p>
<p>为了便于将所属进程附到对应数字上，我们在代码中创建了一个结构体（struct）来保存此信息。 我们的结构定义如下：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存进程在通讯器中的次序（rank）和对应数字</span></span><br><span class="line"><span class="comment">// 该结构体用于数组排序，</span></span><br><span class="line"><span class="comment">// 并同时完整保留所属进程信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="type">int</span> comm_rank;</span><br><span class="line">  <span class="class"><span class="keyword">union</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> f;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">  &#125; number;</span><br><span class="line">&#125; CommRankNumber;</span><br></pre></td></tr></table></figure></div>

<p><code>CommRankNumber</code> 结构体保存了我们要排序的数字（记住它可以是浮点数或整数，因此我们使用联合体union），并且它拥有该数字所属进程在通讯器中的次序（rank）。</p>
<p><code>get_ranks</code> 函数，负责创建这些结构体并对它们进行排序。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这个函数在根进程上对收集到的数字排序</span></span><br><span class="line"><span class="comment">// 返回一个数组，数组按进程在通讯器中的次序排序</span></span><br><span class="line"><span class="comment">// 注意 - 该函数只在根进程上运行</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> *<span class="title function_">get_ranks</span><span class="params">(<span class="type">void</span> *gathered_numbers, <span class="type">int</span> gathered_number_count,</span></span><br><span class="line"><span class="params">               MPI_Datatype datatype)</span> &#123;</span><br><span class="line">  <span class="type">int</span> datatype_size;</span><br><span class="line">  MPI_Type_size(datatype, &amp;datatype_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将收集到的数字数组转换为CommRankNumbers数组</span></span><br><span class="line">  <span class="comment">// 这允许我们在排序的同时，完整保留数字所属进程的信息</span></span><br><span class="line">  </span><br><span class="line">  CommRankNumber *comm_rank_numbers = <span class="built_in">malloc</span>(</span><br><span class="line">    gathered_number_count * <span class="keyword">sizeof</span>(CommRankNumber));</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; gathered_number_count; i++) &#123;</span><br><span class="line">    comm_rank_numbers[i].comm_rank = i;</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;(comm_rank_numbers[i].number),</span><br><span class="line">           gathered_numbers + (i * datatype_size),</span><br><span class="line">           datatype_size);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据数据类型对comm_rank_numbers排序</span></span><br><span class="line">  <span class="keyword">if</span> (datatype == MPI_FLOAT) &#123;</span><br><span class="line">    qsort(comm_rank_numbers, gathered_number_count,</span><br><span class="line">          <span class="keyword">sizeof</span>(CommRankNumber), &amp;compare_float_comm_rank_number);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    qsort(comm_rank_numbers, gathered_number_count,</span><br><span class="line">          <span class="keyword">sizeof</span>(CommRankNumber), &amp;compare_int_comm_rank_number);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 现在comm_rank_numbers是排好序的，下面生成一个数组，</span></span><br><span class="line">  <span class="comment">// 包含每个进程的排名，数组第i个元素是进程i的数字的排名</span></span><br><span class="line">  </span><br><span class="line">  <span class="type">int</span> *ranks = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>) * gathered_number_count);</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; gathered_number_count; i++) &#123;</span><br><span class="line">    ranks[comm_rank_numbers[i].comm_rank] = i;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 清理并返回排名数组</span></span><br><span class="line">  <span class="built_in">free</span>(comm_rank_numbers);</span><br><span class="line">  <span class="keyword">return</span> ranks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><code>get_ranks</code> 函数首先创建一个CommRankNumber结构体数组，并附上该数字所属进程在通讯器中的次序。 如果数据类型为 <code>MPI_FLOAT</code> ，则对我们的结构体数组调用 <code>qsort</code> 时，会使用特殊的排序函数，（代码见<a class="link"   target="_blank" rel="noopener" href="https://github.com/mpitutorial/mpitutorial/tree/gh-pages/tutorials/performing-parallel-rank-with-mpi/code" >tmpi_rank.c <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>。 类似的，如果数据类型为 <code>MPI_INT</code> ，我们将使用不同的排序函数。</p>
<p>在对数字进行排序之后，我们必须以适当的顺序创建一个排名数组（array of ranks），以便将它们分散（scatter）回到请求的进程中。这是通过创建 <code>ranks</code> 数组并为每个已排序的 <code>CommRankNumber</code> 结构体填充适当的排名来实现的。</p>
<h2 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h2><p>现在我们有了两个主要函数，我们可以将它们全部整合到我们的 <code>TMPI_Rank</code> 函数中。此函数将数字收集到根进程，并对数字进行排序以确定其排名，然后将排名分散回请求的进程。 代码如下所示：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">/ 获取recv_data的排名, 类型为datatype</span><br><span class="line"><span class="comment">// 排名用send_data返回，类型为datatype</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">TMPI_Rank</span><span class="params">(<span class="type">void</span> *send_data, <span class="type">void</span> *recv_data, MPI_Datatype datatype,</span></span><br><span class="line"><span class="params">             MPI_Comm comm)</span> &#123;</span><br><span class="line">  <span class="comment">// 首先检查基本情况 - 此函数只支持MPI_INT和MPI_FLOAT</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (datatype != MPI_INT &amp;&amp; datatype != MPI_FLOAT) &#123;</span><br><span class="line">    <span class="keyword">return</span> MPI_ERR_TYPE;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> comm_size, comm_rank;</span><br><span class="line">  MPI_Comm_size(comm, &amp;comm_size);</span><br><span class="line">  MPI_Comm_rank(comm, &amp;comm_rank);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为了计算排名，必须将数字收集到一个进程中</span></span><br><span class="line">  <span class="comment">// 对数字排序, 然后将排名结果分散传回</span></span><br><span class="line">  <span class="comment">// 首先在进程0上收集数字</span></span><br><span class="line">  <span class="type">void</span> *gathered_numbers = gather_numbers_to_root(send_data, datatype,</span><br><span class="line">                                                  comm);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取每个进程的次序（rank)</span></span><br><span class="line">  <span class="type">int</span> *ranks = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">if</span> (comm_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    ranks = get_ranks(gathered_numbers, comm_size, datatype);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分散发回排名结果</span></span><br><span class="line">  MPI_Scatter(ranks, <span class="number">1</span>, MPI_INT, recv_data, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, comm);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 清理</span></span><br><span class="line">  <span class="keyword">if</span> (comm_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">free</span>(gathered_numbers);</span><br><span class="line">    <span class="built_in">free</span>(ranks);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><code>TMPI_Rank</code> 函数使用我们刚刚创建的两个函数 <code>gather_numbers_to_root</code> 和 <code>get_ranks</code> 来获取数字的排名。然后，函数执行最后的 <code>MPI_Scatter</code>，以将所得的排名分散传回进程。</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Holds the communicator rank of a process along with the corresponding number.</span></span><br><span class="line"><span class="comment">// This struct is used for sorting the values and keeping the owning process information</span></span><br><span class="line"><span class="comment">// intact.</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">  <span class="type">int</span> comm_rank;</span><br><span class="line">  <span class="class"><span class="keyword">union</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> f;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">  &#125; number;</span><br><span class="line">&#125; CommRankNumber;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Gathers numbers for TMPI_Rank to process zero. Allocates enough space given the MPI datatype and</span></span><br><span class="line"><span class="comment">// returns a void * buffer to process 0. It returns NULL to all other processes.</span></span><br><span class="line"><span class="type">void</span> *<span class="title function_">gather_numbers_to_root</span><span class="params">(<span class="type">void</span> *number, MPI_Datatype datatype, MPI_Comm comm)</span> &#123;</span><br><span class="line">  <span class="type">int</span> comm_rank, comm_size;</span><br><span class="line">  MPI_Comm_rank(comm, &amp;comm_rank);</span><br><span class="line">  MPI_Comm_size(comm, &amp;comm_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Allocate an array on the root process of a size depending on the MPI datatype being used.</span></span><br><span class="line">  <span class="type">int</span> datatype_size;</span><br><span class="line">  MPI_Type_size(datatype, &amp;datatype_size);</span><br><span class="line">  <span class="type">void</span> *gathered_numbers;</span><br><span class="line">  <span class="keyword">if</span> (comm_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    gathered_numbers = <span class="built_in">malloc</span>(datatype_size * comm_size);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Gather all of the numbers on the root process</span></span><br><span class="line">  MPI_Gather(number, <span class="number">1</span>, datatype, gathered_numbers, <span class="number">1</span>, datatype, <span class="number">0</span>, comm);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> gathered_numbers;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A comparison function for sorting float CommRankNumber values</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">compare_float_comm_rank_number</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *a, <span class="type">const</span> <span class="type">void</span> *b)</span> &#123;</span><br><span class="line">  CommRankNumber *comm_rank_number_a = (CommRankNumber *)a;</span><br><span class="line">  CommRankNumber *comm_rank_number_b = (CommRankNumber *)b;</span><br><span class="line">  <span class="keyword">if</span> (comm_rank_number_a-&gt;number.f &lt; comm_rank_number_b-&gt;number.f) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (comm_rank_number_a-&gt;number.f &gt; comm_rank_number_b-&gt;number.f) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A comparison function for sorting int CommRankNumber values</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">compare_int_comm_rank_number</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *a, <span class="type">const</span> <span class="type">void</span> *b)</span> &#123;</span><br><span class="line">  CommRankNumber *comm_rank_number_a = (CommRankNumber *)a;</span><br><span class="line">  CommRankNumber *comm_rank_number_b = (CommRankNumber *)b;</span><br><span class="line">  <span class="keyword">if</span> (comm_rank_number_a-&gt;number.i &lt; comm_rank_number_b-&gt;number.i) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (comm_rank_number_a-&gt;number.i &gt; comm_rank_number_b-&gt;number.i) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This function sorts the gathered numbers on the root process and returns an array of</span></span><br><span class="line"><span class="comment">// ordered by the process&#x27;s rank in its communicator. Note - this function is only</span></span><br><span class="line"><span class="comment">// executed on the root process.</span></span><br><span class="line"><span class="type">int</span> *<span class="title function_">get_ranks</span><span class="params">(<span class="type">void</span> *gathered_numbers, <span class="type">int</span> gathered_number_count, MPI_Datatype datatype)</span> &#123;</span><br><span class="line">  <span class="type">int</span> datatype_size;</span><br><span class="line">  MPI_Type_size(datatype, &amp;datatype_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Convert the gathered number array to an array of CommRankNumbers. This allows us to</span></span><br><span class="line">  <span class="comment">// sort the numbers and also keep the information of the processes that own the numbers</span></span><br><span class="line">  <span class="comment">// intact.</span></span><br><span class="line">  CommRankNumber *comm_rank_numbers = <span class="built_in">malloc</span>(gathered_number_count * <span class="keyword">sizeof</span>(CommRankNumber));</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; gathered_number_count; i++) &#123;</span><br><span class="line">    comm_rank_numbers[i].comm_rank = i;</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;(comm_rank_numbers[i].number), gathered_numbers + (i * datatype_size), datatype_size);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Sort the comm rank numbers based on the datatype</span></span><br><span class="line">  <span class="keyword">if</span> (datatype == MPI_FLOAT) &#123;</span><br><span class="line">    qsort(comm_rank_numbers, gathered_number_count, <span class="keyword">sizeof</span>(CommRankNumber), &amp;compare_float_comm_rank_number);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    qsort(comm_rank_numbers, gathered_number_count, <span class="keyword">sizeof</span>(CommRankNumber), &amp;compare_int_comm_rank_number);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Now that the comm_rank_numbers are sorted, create an array of rank values for each process. The ith</span></span><br><span class="line">  <span class="comment">// element of this array contains the rank value for the number sent by process i.</span></span><br><span class="line">  <span class="type">int</span> *ranks = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>) * gathered_number_count);</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; gathered_number_count; i++) &#123;</span><br><span class="line">    ranks[comm_rank_numbers[i].comm_rank] = i;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Clean up and return the rank array</span></span><br><span class="line">  <span class="built_in">free</span>(comm_rank_numbers);</span><br><span class="line">  <span class="keyword">return</span> ranks;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Gets the rank of the recv_data, which is of type datatype. The rank is returned</span></span><br><span class="line"><span class="comment">// in send_data and is of type datatype.</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">TMPI_Rank</span><span class="params">(<span class="type">void</span> *send_data, <span class="type">void</span> *recv_data, MPI_Datatype datatype, MPI_Comm comm)</span> &#123;</span><br><span class="line">  <span class="comment">// Check base cases first - Only support MPI_INT and MPI_FLOAT for this function.</span></span><br><span class="line">  <span class="keyword">if</span> (datatype != MPI_INT &amp;&amp; datatype != MPI_FLOAT) &#123;</span><br><span class="line">    <span class="keyword">return</span> MPI_ERR_TYPE;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> comm_size, comm_rank;</span><br><span class="line">  MPI_Comm_size(comm, &amp;comm_size);</span><br><span class="line">  MPI_Comm_rank(comm, &amp;comm_rank);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// To calculate the rank, we must gather the numbers to one process, sort the numbers, and then</span></span><br><span class="line">  <span class="comment">// scatter the resulting rank values. Start by gathering the numbers on process 0 of comm.</span></span><br><span class="line">  <span class="type">void</span> *gathered_numbers = gather_numbers_to_root(send_data, datatype, comm);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get the ranks of each process</span></span><br><span class="line">  <span class="type">int</span> *ranks = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">if</span> (comm_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    ranks = get_ranks(gathered_numbers, comm_size, datatype);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Scatter the rank results</span></span><br><span class="line">  MPI_Scatter(ranks, <span class="number">1</span>, MPI_INT, recv_data, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, comm);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Do clean up</span></span><br><span class="line">  <span class="keyword">if</span> (comm_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">free</span>(gathered_numbers);</span><br><span class="line">    <span class="built_in">free</span>(ranks);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h1 id="—————————————"><a href="#—————————————" class="headerlink" title="—————————————"></a>—————————————</h1><h1 id="基于进程的并行"><a href="#基于进程的并行" class="headerlink" title="基于进程的并行"></a>基于进程的并行</h1><p><code>multiprocessing</code> 是Python标准库中的模块，<strong>实现了共享内存机制</strong>，也就是说，可以让运行在不同处理器核心的进程能读取共享内存。</p>
<p><code>mpi4py</code> 库实现了消息传递的编程范例（设计模式）。简单来说，就是进程之间不靠任何共享信息来进行通讯（也叫做shared nothing），<strong>所有的交流都通过传递信息代替。</strong>在信息传递的代码中，进程通过 <code>send()</code> 和 <code>receive</code> 进行交流。</p>
<p>在Python多进程的官方文档中，明确指出 <code>multiprocessing</code> 模块要求，使用此模块的函数的main模块对子类来说必须是可导入的（ <a class="link"   target="_blank" rel="noopener" href="https://docs.python.org/3.3/library/multiprocessing.html" >https://docs.python.org/3.3/library/multiprocessing.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）。</p>
<p><code>__main__</code> 在IDLE中并不是可以导入的，即使你在IDLE中将文件当做一个脚本来运行。为了能正确使用此模块，本章我们将在命令行使用下面的命令运行脚本：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python multiprocessing example.py</span><br></pre></td></tr></table></figure></div>

<p>这里， <code>multiprocessing_example.py</code> 是脚本的文件名</p>
<p><strong>多进程运行一个函数，这个函数必须从外部文件导入。</strong>比如说下面这样就不行：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = Pool(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x*x</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.<span class="built_in">map</span>(f, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">Process PoolWorker-<span class="number">1</span>:</span><br><span class="line">Process PoolWorker-<span class="number">2</span>:</span><br><span class="line">Process PoolWorker-<span class="number">3</span>:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">AttributeError: <span class="string">&#x27;module&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;f&#x27;</span></span><br><span class="line">AttributeError: <span class="string">&#x27;module&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;f&#x27;</span></span><br><span class="line">AttributeError: <span class="string">&#x27;module&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;f&#x27;</span></span><br></pre></td></tr></table></figure></div>

<p>上面脚本来自Python官网文档。如果稍作修改，将需要多进程运行的目标函数放到文件里导入运行，就没有问题了：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: p = Pool(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="keyword">import</span> func</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: p.<span class="built_in">map</span>(func.f, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">Out[<span class="number">5</span>]: [<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure></div>

<h1 id="如何产生一个进程"><a href="#如何产生一个进程" class="headerlink" title="如何产生一个进程"></a>如何产生一个进程</h1><p>“产生”（spawn）的意思是，由父进程创建子进程。父进程既可以在产生子进程之后继续异步执行，也可以暂停等待子进程创建完成之后再继续执行。Python的multiprocessing库通过以下几步创建进程：</p>
<ol>
<li>创建进程对象</li>
<li>调用 <code>start()</code> 方法，开启进程的活动</li>
<li>调用 <code>join()</code> 方法，在进程结束之前一直等待</li>
</ol>
<p>下面的例子创建了5个进程，每一个进程都分配了 <code>foo(i)</code> 函数， <code>i</code> 表示进程的id：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">i</span>):</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;called function in process: %s&#x27;</span> %i)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Process_jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=foo, args=(i,))		<span class="comment">#创建进程。进程对象的时候需要分配一个函数，作为进程的执行任务，args是传入函数的参数</span></span><br><span class="line">        Process_jobs.append(p)									<span class="comment">#将进程加入</span></span><br><span class="line">        p.start()</span><br><span class="line">        p.join()</span><br></pre></td></tr></table></figure></div>

<p>执行本例需要打开命令行，到文件 <code>spawn_a_process.py</code> （脚本名字）所在的目录下，然后输入下面的命令执行：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python spawn_a_process.py</span><br></pre></td></tr></table></figure></div>

<p>我们会得到以下结果：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ python process_2.py</span><br><span class="line">called function in process: 0</span><br><span class="line">called function in process: 1</span><br><span class="line">called function in process: 2</span><br><span class="line">called function in process: 3</span><br><span class="line">called function in process: 4</span><br></pre></td></tr></table></figure></div>

<p>子进程创建的时候需要导入包含目标函数的脚本。通过在 <code>__main__</code> 代码块中实例化进程对象，我们可以预防无限递归调用。<strong>最佳实践是在不同的脚本文件中定义目标函数，然后导入进来使用</strong>。所以上面的代码可以修改为：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> target_function</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Process_jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=target_function.function,args=(i,))</span><br><span class="line">        Process_jobs.append(p)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join()</span><br></pre></td></tr></table></figure></div>

<p><code>target_function.py</code> 的内容如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">function</span>(<span class="params">i</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;called function in process: %s&#x27;</span> %i)</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure></div>

<h1 id="如何为一个进程命名"><a href="#如何为一个进程命名" class="headerlink" title="如何为一个进程命名"></a>如何为一个进程命名</h1><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 命名一个进程</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    name = multiprocessing.current_process().name</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting %s \n&quot;</span> % name)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exiting %s \n&quot;</span> % name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    process_with_name = multiprocessing.Process(name=<span class="string">&#x27;foo_process&#x27;</span>, target=foo)</span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在 Python 中，进程可以分为两种类型：前台进程和守护进程。前台进程是指在主进程中创建的进程，它们会在主进程退出前完成运行。而守护进程是指在主进程中创建的进程，它们会在主进程退出时自动终止。</span></span><br><span class="line"><span class="string">    第一个进程（使用自定义名称）设置为守护进程，即 process_with_name.daemon = True。这意味着在主进程退出时，该进程会自动终止，而不管它是否完成运行。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    process_with_name.daemon = <span class="literal">True</span>  <span class="comment"># 注意原代码有这一行，但是译者发现删掉这一行才能得到正确输出</span></span><br><span class="line">    process_with_default_name = multiprocessing.Process(target=foo)</span><br><span class="line">    process_with_name.start()</span><br><span class="line">    process_with_default_name.start()</span><br></pre></td></tr></table></figure></div>

<p>运行上面的代码，打开终端输入:</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python naming_process.py</span><br></pre></td></tr></table></figure></div>

<p>输出的结果如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ python naming_process.py</span><br><span class="line">Starting foo_process</span><br><span class="line">Starting Process-2</span><br><span class="line">Exiting foo_process</span><br><span class="line">Exiting Process-2</span><br></pre></td></tr></table></figure></div>

<h1 id="如何在后台运行一个进程"><a href="#如何在后台运行一个进程" class="headerlink" title="如何在后台运行一个进程"></a>如何在后台运行一个进程</h1><p>如果需要处理比较巨大的任务，又不需要人为干预，将其作为后台进程执行是个非常常用的编程模型。此进程又可以和其他进程并发执行。通过Python的multiprocessing模块的后台进程选项，我们可以让进程在后台运行。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    name = multiprocessing.current_process().name</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting %s &quot;</span> % name)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exiting %s &quot;</span> % name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    background_process = multiprocessing.Process(name=<span class="string">&#x27;background_process&#x27;</span>, target=foo)</span><br><span class="line">    background_process.daemon = <span class="literal">True</span>			<span class="comment">#守护进程就是后台进程，不会显示相关信息。后台运行进程在主进程结束之后会自动结束。</span></span><br><span class="line">    NO_background_process = multiprocessing.Process(name=<span class="string">&#x27;NO_background_process&#x27;</span>, target=foo)</span><br><span class="line">    NO_background_process.daemon = <span class="literal">False</span></span><br><span class="line">    background_process.start()</span><br><span class="line">    NO_background_process.start()</span><br></pre></td></tr></table></figure></div>

<p>运行上面的脚本，需要使用下面的命令：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python background_process.py</span><br></pre></td></tr></table></figure></div>

<p>最后的输出如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python background_process.py</span><br><span class="line">Starting NO_background_process</span><br><span class="line">Exiting NO_background_process</span><br></pre></td></tr></table></figure></div>

<h1 id="如何杀掉一个进程"><a href="#如何杀掉一个进程" class="headerlink" title="如何杀掉一个进程"></a>如何杀掉一个进程</h1><p>我们可以使用 <code>terminate()</code> 方法立即杀死一个进程。另外，我们可以使用 <code>is_alive()</code> 方法来判断一个进程是否还存活。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 杀死一个进程</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Starting function&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Finished function&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        p = multiprocessing.Process(target=foo)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Process before execution:&#x27;</span>, p, p.is_alive())</span><br><span class="line">        p.start()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Process running:&#x27;</span>, p, p.is_alive())</span><br><span class="line">        p.terminate()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Process terminated:&#x27;</span>, p, p.is_alive())</span><br><span class="line">        p.join()					<span class="comment">#join() 方法会阻塞主进程，直到子进程完成为止。</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Process joined:&#x27;</span>, p, p.is_alive())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Process exit code:&#x27;</span>, p.exitcode)</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407102050741.png"
                      class="" title="image-20230407102050741"
                >

<p>最后，我们通过读进程的 <code>ExitCode</code> 状态码（status code）验证进程已经结束， <code>ExitCode</code> 可能的值如下：</p>
<ul>
<li>&#x3D;&#x3D; 0: 没有错误正常退出</li>
<li>&gt; 0: 进程有错误，并以此状态码退出</li>
<li>&lt; 0: 进程被 <code>-1 *</code> 的信号杀死并以此作为 ExitCode 退出</li>
</ul>
<p>在我们的例子中，输出的 <code>ExitCode</code> 是 <code>-15</code> 。负数表示子进程被数字为15的信号杀死。</p>
<h1 id="如何在子类中使用进程"><a href="#如何在子类中使用进程" class="headerlink" title="如何在子类中使用进程"></a>如何在子类中使用进程</h1><p>实现一个自定义的进程子类，需要以下三步：</p>
<ul>
<li>定义 <code>Process</code> 的子类</li>
<li>覆盖 <code>__init__(self [,args])</code> 方法来添加额外的参数</li>
<li>覆盖 <code>run(self, [.args])</code> 方法来实现 <code>Process</code> 启动的时候执行的任务</li>
</ul>
<p>创建 <code>Porcess</code> 子类之后，你可以创建它的实例并通过 <code>start()</code> 方法启动它，启动之后会运行 <code>run()</code> 方法。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 自定义子类进程</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyProcess</span>(multiprocessing.Process):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&#x27;called run method in process: %s&#x27;</span> % self.name)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        jobs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">                p = MyProcess()</span><br><span class="line">                jobs.append(p)</span><br><span class="line">                p.start()</span><br><span class="line">                p.join()</span><br></pre></td></tr></table></figure></div>

<p>输入以下命令运行脚本：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python subclass_process.py</span><br></pre></td></tr></table></figure></div>

<p>运行结果如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ python subclass.py</span><br><span class="line">called run method in process: MyProcess-1</span><br><span class="line">called run method in process: MyProcess-2</span><br><span class="line">called run method in process: MyProcess-3</span><br><span class="line">called run method in process: MyProcess-4</span><br><span class="line">called run method in process: MyProcess-5</span><br></pre></td></tr></table></figure></div>

<h1 id="如何在进程之间交换对象"><a href="#如何在进程之间交换对象" class="headerlink" title="如何在进程之间交换对象"></a>如何在进程之间交换对象</h1><p>并行应用常常需要在进程之间交换数据。Multiprocessing库有两个Communication Channel可以交换对象：<strong>队列(queue)和管道（pipe）。</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407103300194.png"
                      class="" title="image-20230407103300194"
                >

<h2 id="使用队列交换对象"><a href="#使用队列交换对象" class="headerlink" title="使用队列交换对象"></a>使用队列交换对象</h2><p>我们可以通过队列数据结构来共享对象。</p>
<p><code>Queue</code> 返回一个进程共享的队列，是线程安全的，也是进程安全的。<strong>任何可序列化的对象（Python通过 <code>pickable</code> 模块序列化对象）都可以通过它进行交换。</strong></p>
<blockquote>
<p>序列化是指将数据结构或对象转换为可存储或传输的格式的过程。。可序列化的对象是指可以被转换为字节流或字符串的对象。</p>
</blockquote>
<p>在下面的例子中，我们将展示如何使用队列来实现生产者-消费者问题。 <code>Producer</code> 类生产item放到队列中，然后 <code>Consumer</code> 类从队列中移除它们。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Producer</span>(multiprocessing.Process):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, queue</span>):</span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.queue = queue</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            item = random.randint(<span class="number">0</span>, <span class="number">256</span>)</span><br><span class="line">            self.queue.put(item)			<span class="comment">#将item放进queue中</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Process Producer : item %d appended to queue %s&quot;</span> % (item, self.name))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;The size of queue is %s&quot;</span> % self.queue.qsize())</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Consumer</span>(multiprocessing.Process):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, queue</span>):</span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.queue = queue</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> self.queue.empty():</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;the queue is empty&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                item = self.queue.get()			<span class="comment">#将queue、中的item取出</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Process Consumer : item %d popped from by %s \n&#x27;</span> % (item, self.name))</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    queue = multiprocessing.Queue()</span><br><span class="line">    process_producer = Producer(queue)</span><br><span class="line">    process_consumer = Consumer(queue)</span><br><span class="line">    process_producer.start()</span><br><span class="line">    process_consumer.start()</span><br><span class="line">    process_producer.join()</span><br><span class="line">    process_consumer.join()</span><br></pre></td></tr></table></figure></div>

<p>队列还有一个 <code>JoinableQueue</code> 子类，它有以下两个额外的方法：</p>
<ul>
<li><code>task_done()</code>: 此方法意味着之前入队的一个任务已经完成，比如， <code>get()</code> 方法从队列取回item之后调用。所以此方法只能被队列的消费者调用。</li>
<li><code>join()</code>: 此方法将进程阻塞，直到队列中的item全部被取出并执行。</li>
</ul>
<p>（ <a class="link"   target="_blank" rel="noopener" href="https://github.com/laixintao/python-parallel-programming-cookbook-cn/issues/17#issuecomment-335668371" >Microndgt <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 注：因为使用队列进行通信是一个单向的，不确定的过程，所以你不知道什么时候队列的元素被取出来了，所以使用task_done来表示队列里的一个任务已经完成。</p>
<p><strong>这个方法一般和join一起使用，当队列的所有任务都处理之后，也就是说put到队列的每个任务都调用了task_done方法后，join才会完成阻塞。）</strong></p>
<h2 id="使用管道交换对象"><a href="#使用管道交换对象" class="headerlink" title="使用管道交换对象"></a>使用管道交换对象</h2><p>第二种Communication Channel是管道。</p>
<p>一个管道可以做以下事情：</p>
<ul>
<li>返回一对被管道连接的连接对象</li>
<li>然后对象就有了 send&#x2F;receive 方法可以在进程之间通信</li>
</ul>
<p>下面是管道用法的一个简单示例。这里有一个进程管道从0到9发出数字，另一个进程接收数字并进行平方计算。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_items</span>(<span class="params">pipe</span>):</span><br><span class="line">    output_pipe, _ = pipe</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        output_pipe.send(item)</span><br><span class="line">    output_pipe.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply_items</span>(<span class="params">pipe_1, pipe_2</span>):</span><br><span class="line">    close, input_pipe = pipe_1</span><br><span class="line">    close.close()</span><br><span class="line">    output_pipe, _ = pipe_2</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            item = input_pipe.recv()</span><br><span class="line">            output_pipe.send(item * item)</span><br><span class="line">    <span class="keyword">except</span> EOFError:</span><br><span class="line">        output_pipe.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 第一个进程管道发出数字</span></span><br><span class="line">    pipe_1 = multiprocessing.Pipe(<span class="literal">True</span>)</span><br><span class="line">    process_pipe_1 = multiprocessing.Process(target=create_items, args=(pipe_1,))</span><br><span class="line">    process_pipe_1.start()</span><br><span class="line">    <span class="comment"># 第二个进程管道接收数字并计算</span></span><br><span class="line">    pipe_2 = multiprocessing.Pipe(<span class="literal">True</span>)</span><br><span class="line">    process_pipe_2 = multiprocessing.Process(target=multiply_items, args=(pipe_1, pipe_2,))</span><br><span class="line">    process_pipe_2.start()</span><br><span class="line">    pipe_1[<span class="number">0</span>].close()</span><br><span class="line">    pipe_2[<span class="number">0</span>].close()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="built_in">print</span>(pipe_2[<span class="number">1</span>].recv())</span><br><span class="line">    <span class="keyword">except</span> EOFError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;End&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407105421091.png"
                      class="" title="image-20230407105421091"
                >

<p><code>Pipe()</code> 函数返回一对通过双向管道连接起来的对象。在本例中， <code>out_pipe</code> 包含数字0-9，通过目标函数 <code>create_items()</code> 产生:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_items</span>(<span class="params">pipe</span>):</span><br><span class="line">    output_pipe, _ = pipe</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        output_pipe.send(item)</span><br><span class="line">    output_pipe.close()</span><br></pre></td></tr></table></figure></div>

<p>在第二个进程中，我们有两个管道，输入管道和包含结果的输出管道：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">process_pipe_2 = multiprocessing.Process(target=multiply_items, args=(pipe_1, pipe_2,))</span><br></pre></td></tr></table></figure></div>

<p>最后打印出结果:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="built_in">print</span>(pipe_2[<span class="number">1</span>].recv())</span><br><span class="line"><span class="keyword">except</span> EOFError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;End&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<h1 id="进程如何同步"><a href="#进程如何同步" class="headerlink" title="进程如何同步"></a>进程如何同步</h1><p>多个进程可以协同工作来完成一项任务。通常需要共享数据。所以在多进程之间保持数据的一致性就很重要了。需要共享数据协同的进程必须以适当的策略来读写数据。相关的同步原语和线程的库很类似。</p>
<p>进程的同步原语如下：</p>
<ul>
<li><strong>Lock</strong>: 这个对象可以有两种装填：锁住的（locked）和没锁住的（unlocked）。一个Lock对象有两个方法， <code>acquire()</code> 和 <code>release()</code> ，来控制共享数据的读写权限。</li>
<li><strong>Event</strong>: 实现了进程间的简单通讯，一个进程发事件的信号，另一个进程等待事件的信号。 <code>Event</code> 对象有两个方法， <code>set()</code> 和 <code>clear()</code> ，来管理自己内部的变量。</li>
<li><strong>Condition</strong>: 此对象用来同步部分工作流程，在并行的进程中，有两个基本的方法： <code>wait()</code> 用来等待进程， <code>notify_all()</code> 用来通知所有等待此条件的进程。</li>
<li><strong>Semaphore</strong>: 用来共享资源，例如，支持固定数量的共享连接。</li>
<li><strong>Rlock</strong>: 递归锁对象。其用途和方法同 <code>Threading</code> 模块一样。</li>
<li><strong>Barrier</strong>: 将程序分成几个阶段，适用于有些进程必须在某些特定进程之后执行。处于障碍（Barrier）之后的代码不能同处于障碍之前的代码并行。</li>
</ul>
<p>下面的代码展示了如何使用 <code>barrier()</code> 函数来同步两个进程。我们有4个进程，进程1和进程2由barrier语句管理，进程3和进程4没有同步策略。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Barrier, Lock, Process</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_with_barrier</span>(<span class="params">synchronizer, serializer</span>):</span><br><span class="line">    name = multiprocessing.current_process().name</span><br><span class="line">    synchronizer.wait()		<span class="comment">#当两个进程都调用 wait() 方法的时候，它们会一起继续执行下面的代码</span></span><br><span class="line">    now = time()</span><br><span class="line">    <span class="keyword">with</span> serializer:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process %s ----&gt; %s&quot;</span> % (name, datetime.fromtimestamp(now)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_without_barrier</span>():</span><br><span class="line">    name = multiprocessing.current_process().name</span><br><span class="line">    now = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process %s ----&gt; %s&quot;</span> % (name, datetime.fromtimestamp(now)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    synchronizer = Barrier(<span class="number">2</span>)</span><br><span class="line">    serializer = Lock()</span><br><span class="line">    <span class="comment">#进程1，2加了锁，实现了同步</span></span><br><span class="line">    Process(name=<span class="string">&#x27;p1 - test_with_barrier&#x27;</span>, target=test_with_barrier, args=(synchronizer,serializer)).start()</span><br><span class="line">    Process(name=<span class="string">&#x27;p2 - test_with_barrier&#x27;</span>, target=test_with_barrier, args=(synchronizer,serializer)).start()</span><br><span class="line">    Process(name=<span class="string">&#x27;p3 - test_without_barrier&#x27;</span>, target=test_without_barrier).start()</span><br><span class="line">    Process(name=<span class="string">&#x27;p4 - test_without_barrier&#x27;</span>, target=test_without_barrier).start()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ python process_barrier.py</span><br><span class="line">process p1 - test_with_barrier ----&gt; 2015-05-09 11:11:33.291229</span><br><span class="line">process p2 - test_with_barrier ----&gt; 2015-05-09 11:11:33.291229</span><br><span class="line">process p3 - test_without_barrier ----&gt; 2015-05-09 11:11:33.310230</span><br><span class="line">process p4 - test_without_barrier ----&gt; 2015-05-09 11:11:33.333231</span><br></pre></td></tr></table></figure></div>

<h1 id="如何在进程之间管理状态"><a href="#如何在进程之间管理状态" class="headerlink" title="如何在进程之间管理状态"></a>如何在进程之间管理状态</h1><p>Python的多进程模块提供了在所有的用户间管理共享信息的管理者(Manager)。一个管理者对象控制着持有Python对象的服务进程，并允许其它进程操作共享对象。</p>
<p>管理者有以下特性：</p>
<ul>
<li>它控制着管理共享对象的服务进程</li>
<li>它确保当某一进程修改了共享对象之后，所有的进程拿到额共享对象都得到了更新</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">dictionary, key, item</span>):</span><br><span class="line">   dictionary[key] = item</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;key = %d value = %d&quot;</span> % (key, item))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    mgr = multiprocessing.Manager()</span><br><span class="line">    dictionary = mgr.<span class="built_in">dict</span>()</span><br><span class="line">    <span class="comment">#manager创立了10个进程，更新字典</span></span><br><span class="line">    jobs = [multiprocessing.Process(target=worker, args=(dictionary, i, i*<span class="number">2</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">        j.start()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">        j.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Results:&#x27;</span>, dictionary)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ python manager.py</span><br><span class="line">key = 0 value = 0</span><br><span class="line">key = 3 value = 6</span><br><span class="line">key = 2 value = 4</span><br><span class="line">key = 1 value = 2</span><br><span class="line">key = 4 value = 8</span><br><span class="line">key = 5 value = 10</span><br><span class="line">key = 8 value = 16</span><br><span class="line">key = 6 value = 12</span><br><span class="line">key = 7 value = 14</span><br><span class="line">key = 9 value = 18</span><br><span class="line">Results: &#123;0: 0, 3: 6, 2: 4, 1: 2, 4: 8, 5: 10, 8: 16, 6: 12, 7: 14, 9: 18&#125;</span><br></pre></td></tr></table></figure></div>

<h1 id="如何使用进程池"><a href="#如何使用进程池" class="headerlink" title="如何使用进程池"></a>如何使用进程池</h1><p>多进程库提供了 <code>Pool</code> 类来实现简单的多进程任务。 <code>Pool</code> 类有以下方法：</p>
<ul>
<li><code>apply()</code>: 直到得到结果之前一直阻塞。</li>
<li><code>apply_async()</code>: 这是 <code>apply()</code> 方法的一个变体，返回的是一个result对象。这是一个异步的操作，<strong>在所有的子类执行之前不会锁住主进程。</strong></li>
<li><code>map()</code>: 这是内置的 <code>map()</code> 函数的并行版本。在得到结果之前一直阻塞，此方法将可迭代的数据的每一个元素作为进程池的一个任务来执行。</li>
<li><code>map_async()</code>: 这是 <code>map()</code> 方法的一个变体，返回一个result对象。如果指定了回调函数，回调函数应该是callable的，并且只接受一个参数。当result准备好时会自动调用回调函数（除非调用失败）。回调函数应该立即完成，否则，持有result的进程将被阻塞。</li>
</ul>
<p>下面的例子展示了如果通过进程池来执行一个并行应用。我们创建了有4个进程的进程池，然后使用 <code>map()</code> 方法进行一个简单的计算。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">function_square</span>(<span class="params">data</span>):</span><br><span class="line">    result = data*data</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inputs = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">    pool = multiprocessing.Pool(processes=<span class="number">4</span>)</span><br><span class="line">    pool_outputs = pool.<span class="built_in">map</span>(function_square, inputs)		<span class="comment">#pool.map 方法将一些独立的任务提交给进程池：</span></span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;Pool    :&#x27;</span>, pool_outputs)</span><br></pre></td></tr></table></figure></div>

<p>计算的结果如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python poll.py</span><br><span class="line">(&#x27;Pool    :&#x27;, [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441,</span><br></pre></td></tr></table></figure></div>

<p>需要注意的是， <code>pool.map()</code> 方法的结果和Python内置的 <code>map()</code> 结果是相同的，不同的是 <code>pool.map()</code> 是通过多个并行进程计算的。</p>
<h1 id="使用Python的mpi4py模块"><a href="#使用Python的mpi4py模块" class="headerlink" title="使用Python的mpi4py模块"></a>使用Python的mpi4py模块</h1><p>Python 提供了很多MPI模块写并行程序。其中 <code>mpi4py</code> 是一个又有意思的库。它在MPI-1&#x2F;2顶层构建，提供了面向对象的接口，紧跟C++绑定的 MPI-2。MPI的C语言用户可以无需学习新的接口就可以上手这个库。所以，它成为了Python中最广泛使用的MPI库。</p>
<p>此模块包含的主要应用有：</p>
<ul>
<li>点对点通讯</li>
<li>集体通讯</li>
<li>拓扑</li>
</ul>
<p>我们通过打印“Hello world”来开始MPI之旅：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hello.py</span></span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world from process &quot;</span>, rank)</span><br></pre></td></tr></table></figure></div>

<p>通过下面的命令执行代码：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:&gt; mpiexec  -n 5 python  helloWorld_MPI.py</span><br></pre></td></tr></table></figure></div>

<p>执行结果将得到如下的输出：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;hello world from process &#x27;, 1)</span><br><span class="line">(&#x27;hello world from process &#x27;, 0)</span><br><span class="line">(&#x27;hello world from process &#x27;, 2)</span><br><span class="line">(&#x27;hello world from process &#x27;, 3)</span><br><span class="line">(&#x27;hello world from process &#x27;, 4)</span><br></pre></td></tr></table></figure></div>

<h1 id="点对点通讯"><a href="#点对点通讯" class="headerlink" title="点对点通讯"></a>点对点通讯</h1><p>MPI提供的最实用的一个特性是<strong>点对点通讯</strong>。两个不同的进程之间可以通过点对点通讯交换数据：一个进程是接收者，一个进程是发送者。</p>
<p>Python的 <code>mpi4py</code> 通过下面两个函数提供了点对点通讯功能：</p>
<ul>
<li><code>Comm.Send(data, process_destination)</code>: 通过它在交流组中的rank来区分发送给不同进程的数据</li>
<li><code>Comm.Recv(process_source)</code>: 接收来自源进程的数据，也是通过在交流组中的rank来区分的</li>
</ul>
<p><code>Comm</code> 变量表示交流者，定义了可以互相通讯的进程组：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">comm = MKPI.COMM_WORLD</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.rank</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;my rank is : &quot;</span> , rank)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = <span class="number">10000000</span></span><br><span class="line">    destination_process = <span class="number">4</span></span><br><span class="line">    comm.send(data,dest=destination_process)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sending data % s &quot;</span> % data + <span class="string">&quot;to process % d&quot;</span> % destination_process)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">1</span>:</span><br><span class="line">    destination_process = <span class="number">8</span></span><br><span class="line">    data = <span class="string">&quot;hello&quot;</span></span><br><span class="line">    comm.send(data,dest=destination_process)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sending data % s :&quot;</span> % data + <span class="string">&quot;to process % d&quot;</span> % destination_process)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">4</span>:</span><br><span class="line">   data = comm.recv(source = <span class="number">0</span>)			<span class="comment">#source表示从哪里接受数据</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;data received is = % s&quot;</span> % data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">8</span>:</span><br><span class="line">   data1 = comm.recv(source = <span class="number">1</span>)</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;data1 received is = % s&quot;</span> % data1)</span><br></pre></td></tr></table></figure></div>

<p>运行脚本的命令如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mpiexec -n 9 python pointToPointCommunication.py</span><br></pre></td></tr></table></figure></div>

<p>得到的输出如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;my rank is : &#x27;, 5)</span><br><span class="line">(&#x27;my rank is : &#x27;, 1)</span><br><span class="line">sending data hello :to process 8</span><br><span class="line">(&#x27;my rank is : &#x27;, 3)</span><br><span class="line">(&#x27;my rank is : &#x27;, 0)</span><br><span class="line">sending data 10000000 to process 4</span><br><span class="line">(&#x27;my rank is : &#x27;, 2)</span><br><span class="line">(&#x27;my rank is : &#x27;, 7)</span><br><span class="line">(&#x27;my rank is : &#x27;, 4)</span><br><span class="line">data received is = 10000000</span><br><span class="line">(&#x27;my rank is : &#x27;, 8)</span><br><span class="line">data1 received is = hello</span><br><span class="line">(&#x27;my rank is : &#x27;, 6)</span><br></pre></td></tr></table></figure></div>

<p><code>comm.send()</code> 和 <code>comm.recv()</code> 函数都是阻塞的函数。他们会一直阻塞调用者，直到数据使用完成。同时在MPI中，有两种方式发送和接收数据：</p>
<ul>
<li>buffer模式</li>
<li>同步模式</li>
</ul>
<p><strong>在buffer模式中，只要需要发送的数据被拷贝到buffer中，执行权就会交回到主程序，此时数据并非已经发送&#x2F;接收完成。在同步模式中，只有函数真正的结束发送&#x2F;接收任务之后才会返回。</strong></p>
<p>下面的代码中介绍了一种典型的死锁问题；我们有两个进程，一个 <code>rank</code> 等于1，另一个 <code>rank</code> 等于5，他们互相通讯，并且每一个都有发送和接收的函数。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line">comm=MPI.COMM_WORLD</span><br><span class="line">rank = comm.rank</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;my rank is : &quot;</span> , rank)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank==<span class="number">1</span>:</span><br><span class="line">    data_send= <span class="string">&quot;a&quot;</span></span><br><span class="line">    destination_process = <span class="number">5</span></span><br><span class="line">    source_process = <span class="number">5</span></span><br><span class="line">    data_received=comm.recv(source=source_process)</span><br><span class="line">    comm.send(data_send,dest=destination_process)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sending data %s &quot;</span> %data_send + <span class="string">&quot;to process %d&quot;</span> %destination_process)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data received is = %s&quot;</span> %data_received)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank==<span class="number">5</span>:</span><br><span class="line">    data_send= <span class="string">&quot;b&quot;</span></span><br><span class="line">    destination_process = <span class="number">1</span></span><br><span class="line">    source_process = <span class="number">1</span></span><br><span class="line">    data_received=comm.recv(source=source_process)</span><br><span class="line">    comm.send(data_send,dest=destination_process)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sending data %s :&quot;</span> %data_send + <span class="string">&quot;to process %d&quot;</span> %destination_process)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data received is = %s&quot;</span> %data_received)</span><br></pre></td></tr></table></figure></div>

<p>果我们尝试运行这个程序（只有两个进程足够了），会发现这两个进程都不会完成：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ mpiexec -n 9 python deadLockProblems.py</span><br><span class="line">(&#x27;my rank is : &#x27;, 8)</span><br><span class="line">(&#x27;my rank is : &#x27;, 3)</span><br><span class="line">(&#x27;my rank is : &#x27;, 2)</span><br><span class="line">(&#x27;my rank is : &#x27;, 7)</span><br><span class="line">(&#x27;my rank is : &#x27;, 0)</span><br><span class="line">(&#x27;my rank is : &#x27;, 4)</span><br><span class="line">(&#x27;my rank is : &#x27;, 6)</span><br></pre></td></tr></table></figure></div>

<p>此时连个进程都在等待对方，都被阻塞住了。会发生这种情况是因为MPI的 <code>comm.recv()</code> 函数和 <code>comm.send()</code> 函数都是阻塞的。它们的调用者都在等待它们完成。<strong>对 <code>comm.send()</code> MPI来说，只有数据发出之后函数才会结束，对于 <code>comm.recv()</code> 函数来说，只有接收到数据函数才会结束。</strong>为了解决这个问题，我们可以将这连个函数这样写：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> rank==<span class="number">1</span>:</span><br><span class="line">    data_send= <span class="string">&quot;a&quot;</span></span><br><span class="line">    destination_process = <span class="number">5</span></span><br><span class="line">    source_process = <span class="number">5</span></span><br><span class="line">    comm.send(data_send,dest=destination_process)</span><br><span class="line">    data_received=comm.recv(source=source_process)</span><br><span class="line"><span class="keyword">if</span> rank==<span class="number">5</span>:</span><br><span class="line">    data_send= <span class="string">&quot;b&quot;</span></span><br><span class="line">    destination_process = <span class="number">1</span></span><br><span class="line">    source_process = <span class="number">1</span></span><br><span class="line">    data_received=comm.recv(source=source_process)</span><br><span class="line">    comm.send(data_send,dest=destination_process)</span><br></pre></td></tr></table></figure></div>

<p>虽然这个解决方法从逻辑上纠正了，但是并不保证一定可以避免死锁问题。鉴于通讯是发生在buffer的， <code>comm.send()</code> 函数将要发送的数据完全拷贝到buffer里，只有buffer里有完整的数据之后程序才能继续运行。否则，依然会产生死锁：发送者不能发送，因为buffer已经提交但是接收不到接收者，不能接收数据，因为它被 <code>comm.send()</code> 阻塞住了。因此，我们可以交换一下发送者和接收者的顺序来解决这个问题。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> rank==<span class="number">1</span>:</span><br><span class="line">    data_send= <span class="string">&quot;a&quot;</span></span><br><span class="line">    destination_process = <span class="number">5</span></span><br><span class="line">    source_process = <span class="number">5</span></span><br><span class="line">    comm.send(data_send,dest=destination_process)</span><br><span class="line">    data_received=comm.recv(source=source_process)</span><br><span class="line"><span class="keyword">if</span> rank==<span class="number">5</span>:</span><br><span class="line">    data_send= <span class="string">&quot;b&quot;</span></span><br><span class="line">    destination_process = <span class="number">1</span></span><br><span class="line">    source_process = <span class="number">1</span></span><br><span class="line">    comm.send(data_send,dest=destination_process)</span><br><span class="line">    data_received=comm.recv(source=source_process)</span><br></pre></td></tr></table></figure></div>

<p>最后，我们得到正确的输出如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ mpiexec -n 9 python deadLockProblems.py</span><br><span class="line">(&#x27;my rank is : &#x27;, 7)</span><br><span class="line">(&#x27;my rank is : &#x27;, 0)</span><br><span class="line">(&#x27;my rank is : &#x27;, 8)</span><br><span class="line">(&#x27;my rank is : &#x27;, 1)</span><br><span class="line">sending data a to process 5</span><br><span class="line">data received is = b</span><br><span class="line">(&#x27;my rank is : &#x27;, 5)</span><br><span class="line">sending data b :to process 1</span><br><span class="line">data received is = a</span><br><span class="line">(&#x27;my rank is : &#x27;, 2)</span><br><span class="line">(&#x27;my rank is : &#x27;, 3)</span><br><span class="line">(&#x27;my rank is : &#x27;, 4)</span><br><span class="line">(&#x27;my rank is : &#x27;, 6)</span><br></pre></td></tr></table></figure></div>

<h1 id="集体通讯：使用broadcast通讯"><a href="#集体通讯：使用broadcast通讯" class="headerlink" title="集体通讯：使用broadcast通讯"></a>集体通讯：使用broadcast通讯</h1><p>在并行代码的开发中，我们会经常发现需要在多个进程间共享某个变量运行时的值，或操作多个进程提供的变量（可能具有不同的值)。</p>
<p>为了解决这个问题，使用了通讯数。举例说，如果进程0要发送信息给进程1和进程2，同时也会发送信息给进程3，4，5，6，即使这些进程并不需要这些信息。</p>
<p>另外，MPI库提供了在多个进程之间交换信息的方法，针对执行的机器做了优化。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407145700539.png"
                      class="" title="image-20230407145700539"
                >

<p> <code>mpi4py</code> 模块通过以下的方式提供广播的功能：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">buf = comm.bcast(data_to_share, rank_of_root_process)</span><br></pre></td></tr></table></figure></div>

<p>这个函数将root消息中包含的信息发送给属于 <code>comm</code> 通讯组其他的进程，每个进程必须通过相同的 <code>root</code> 和 <code>comm</code> 来调用它。</p>
<p>下面来通过一个例子理解广播函数。我们有一个root进程， <code>rank</code> 等于0，保存自己的数据 <code>variable_to_share</code> ，以及其他定义在通讯组中的进程。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from mpi4py import MPI</span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">if rank == 0:</span><br><span class="line">    variable_to_share = 100</span><br><span class="line">else:</span><br><span class="line">    variable_to_share = None</span><br><span class="line">variable_to_share = comm.bcast(variable_to_share, root=0)</span><br><span class="line">print(&quot;process = %d&quot; %rank + &quot; variable shared  = %d &quot; %variable_to_share)</span><br></pre></td></tr></table></figure></div>

<p>和一个拥有10个进程的通讯组的执行输出结果如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">C:\&gt;mpiexec -n 10 python broadcast.py</span><br><span class="line">process = 0 variable shared  = 100</span><br><span class="line">process = 8 variable shared  = 100</span><br><span class="line">process = 2 variable shared  = 100</span><br><span class="line">process = 3 variable shared  = 100</span><br><span class="line">process = 4 variable shared  = 100</span><br><span class="line">process = 5 variable shared  = 100</span><br><span class="line">process = 9 variable shared  = 100</span><br><span class="line">process = 6 variable shared  = 100</span><br><span class="line">process = 1 variable shared  = 100</span><br><span class="line">process = 7 variable shared  = 100</span><br></pre></td></tr></table></figure></div>



<p><code>rank</code> 等于0的root进程初始化了一个变量， <code>variable_to_share</code> ，值为100.这个变量将通过通讯组发送给其他进程。:</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if rank == 0:</span><br><span class="line">    variable_to_share = 100</span><br></pre></td></tr></table></figure></div>

<p>为了发送消息，我们声明了一个广播：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">variable_to_share = comm.bcast(variable_to_share, root=0)</span><br></pre></td></tr></table></figure></div>

<p>这里，<strong>函数的变量是要发送的数据和发送者的进程</strong>。当我们执行代码的时候，在我们的例子中，我们有一个10个进程的通讯组， <code>variable_to_share</code> 变量将发送给组中的其他进程。最后 <code>print</code> 函数打印出来运行的进程和它们的变量：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;process = %d&quot; %rank + &quot; variable shared  = %d &quot; %variable_to_share)</span><br></pre></td></tr></table></figure></div>

<p>集体通讯允许组中的多个进程同时进行数据交流。在 <code>mpi4py</code> 模块中，只提供了阻塞版本的集体通讯（阻塞调用者，直到缓存中的数据全部安全发送。）</p>
<p>广泛应用的集体通讯应该是：</p>
<ol>
<li>组中的进程提供通讯的屏障</li>
<li>通讯方式包括：<ul>
<li>将一个进程的数据广播到组中其他进程中</li>
<li>从其他进程收集数据发给一个进程</li>
<li>从一个进程散播数据散播到其他进程中</li>
</ul>
</li>
<li>减少操作</li>
</ol>
<h1 id="集体通讯：使用scatter通讯"><a href="#集体通讯：使用scatter通讯" class="headerlink" title="集体通讯：使用scatter通讯"></a>集体通讯：使用scatter通讯</h1><p>scatter函数和广播很像，但是有一个很大的不同， <code>comm.bcast</code> 将相同的数据发送给所有在监听的进程， <code>comm.scatter</code> 可以将数据放在数组中，发送给不同的进程。下图展示了scatter的功能：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407150118235.png"
                      class="" title="image-20230407150118235"
                >

<p><code>comm.scatter</code> 函数接收一个array，根据进程的rank将其中的元素发送给不同的进程。比如第一个元素将发送给进程0，第二个元素将发送给进程1，等等。 <code>mpi4py</code> 中的函数原型如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recvbuf  = comm.scatter(sendbuf, rank_of_root_process)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    array_to_share = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span> ,<span class="number">5</span> ,<span class="number">6</span> ,<span class="number">7</span>, <span class="number">8</span> ,<span class="number">9</span> ,<span class="number">10</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    array_to_share = <span class="literal">None</span></span><br><span class="line">recvbuf = comm.scatter(array_to_share, root=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process = %d&quot;</span> %rank + <span class="string">&quot; recvbuf = %d &quot;</span> %recvbuf)</span><br></pre></td></tr></table></figure></div>

<p>运行代码的输出如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">C:\&gt;mpiexec -n 10 python scatter.py</span><br><span class="line">process = 0 variable shared  = 1</span><br><span class="line">process = 4 variable shared  = 5</span><br><span class="line">process = 6 variable shared  = 7</span><br><span class="line">process = 2 variable shared  = 3</span><br><span class="line">process = 5 variable shared  = 6</span><br><span class="line">process = 3 variable shared  = 4</span><br><span class="line">process = 7 variable shared  = 8</span><br><span class="line">process = 1 variable shared  = 2</span><br><span class="line">process = 8 variable shared  = 9</span><br><span class="line">process = 9 variable shared  = 10</span><br></pre></td></tr></table></figure></div>

<p>这里需要注意， <code>comm.scatter</code> 有一个限制<strong>，发送数据的列表中元素的个数必须和接收的进程数相等。</strong></p>
<h1 id="体通讯：使用gather通讯"><a href="#体通讯：使用gather通讯" class="headerlink" title="体通讯：使用gather通讯"></a>体通讯：使用gather通讯</h1><p><code>gather</code> 函数基本上是反向的 <code>scatter</code> ，即手机所有进程发送向root进程的数据。 <code>mpi4py</code> 实现的 <code>gather</code> 函数如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recvbuf = comm.gather(sendbuf, rank_of_root_process)</span><br></pre></td></tr></table></figure></div>

<p>这里， <code>sendbuf</code> 是要发送的数据， <code>rank_of_root_process</code> 代表要接收数据进程。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407151502131.png"
                      class="" title="image-20230407151502131"
                >

<p>在接下来的例子中，我们想实现上图表示的过程。每一个进程都构建自己的数据，发送给root进程（rank为0）。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">data = (rank+<span class="number">1</span>)**<span class="number">2</span></span><br><span class="line">data = comm.gather(data, root=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;rank = %s &quot;</span> %rank + <span class="string">&quot;...receiving data to other process&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, size):</span><br><span class="line">        data[i] = (i+<span class="number">1</span>)**<span class="number">2</span></span><br><span class="line">        value = data[i]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot; process %s receiving %s from process %s&quot;</span> % (rank , value , i))</span><br></pre></td></tr></table></figure></div>

<p>最后，我们用5个进程来演示：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C:\&gt;mpiexec -n 5 python gather.py</span><br><span class="line">rank = 0 ...receiving data to other process</span><br><span class="line">process 0 receiving 4 from process 1</span><br><span class="line">process 0 receiving 9 from process 2</span><br><span class="line">process 0 receiving 16 from process 3</span><br><span class="line">process 0 receiving 25 from process 4</span><br></pre></td></tr></table></figure></div>

<h1 id="使用Alltoall通讯"><a href="#使用Alltoall通讯" class="headerlink" title="使用Alltoall通讯"></a>使用Alltoall通讯</h1><p><code>Alltoall</code> 集体通讯结合了 <code>scatter</code> 和 <code>gather</code> 的功能。在 <code>mpi4py</code> 中，有以下三种类型的 <code>Alltoall</code> 集体通讯。</p>
<ul>
<li><code>comm.Alltoall(sendbuf, recvbuf)</code> :</li>
<li><code>comm.Alltoallv(sendbuf, recvbuf)</code> :</li>
<li><code>comm.Alltoallw(sendbuf, recvbuf)</code> :</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">a_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">senddata = (rank+<span class="number">1</span>)*numpy.arange(size,dtype=<span class="built_in">int</span>)</span><br><span class="line">recvdata = numpy.empty(size*a_size,dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">comm.Alltoall(senddata,recvdata)			<span class="comment">#comm.alltoall 方法将 task j 的中 sendbuf 的第i个对象拷贝到 task i 中 recvbuf 的第j个对象</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; process %s sending %s receiving %s&quot;</span> % (rank , senddata , recvdata))</span><br></pre></td></tr></table></figure></div>

<p>运行代码，设定通讯者组的进程数为5，输出如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C:\&gt;mpiexec -n 5 python alltoall.py</span><br><span class="line">process 0 sending [0 1 2 3 4] receiving [0 0 0 0 0]</span><br><span class="line">process 1 sending [0 2 4 6 8] receiving [1 2 3 4 5]</span><br><span class="line">process 2 sending [0 3 6 9 12] receiving [2 4 6 8 10]</span><br><span class="line">process 3 sending [0 4 8 12 16] receiving [3 6 9 12 15]</span><br><span class="line">process 4 sending [0 5 10 15 20] receiving [4 8 12 16 20]</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/04/06/MPI/image-20230407152146975.png"
                      class="" title="image-20230407152146975"
                >

<h1 id="使用Reduce"><a href="#使用Reduce" class="headerlink" title="使用Reduce"></a>使用Reduce</h1><p>同 <code>comm.gather</code> 一样， <code>comm.reduce</code> 接收一个数组，每一个元素是一个进程的输入，然后返回一个数组，每一个元素是进程的输出，返回给 root 进程。输出的元素中包含了简化的结果。</p>
<p>在 <code>mpi4py</code> 中，我们将简化操作定义如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">comm.Reduce(sendbuf, recvbuf, rank_of_root_process, op = type_of_reduction_operation)</span><br></pre></td></tr></table></figure></div>

<p>这里需要注意的是，<strong>这里有个参数 <code>op</code> 和 <code>comm.gather</code> 不同，它代表你想应用在数据上的操作</strong>， <code>mpi4py</code> 模块代表定义了一系列的简化操作，其中一些如下：</p>
<ul>
<li><code>MPI.MAX</code> : 返回最大的元素</li>
<li><code>MPI.MIN</code> : 返回最小的元素</li>
<li><code>MPI.SUM</code> : 对所有元素相加</li>
<li><code>MPI.PROD</code> : 对所有元素相乘</li>
<li><code>MPI.LAND</code> : 对所有元素进行逻辑操作</li>
<li><code>MPI.MAXLOC</code> : 返回最大值，以及拥有它的进程</li>
<li><code>MPI.MINLOC</code> : 返回最小值，以及拥有它的进程</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.size</span><br><span class="line">rank = comm.rank</span><br><span class="line">array_size = <span class="number">3</span></span><br><span class="line">recvdata = numpy.zeros(array_size, dtype=numpy.<span class="built_in">int</span>)</span><br><span class="line">senddata = (rank+<span class="number">1</span>)*numpy.arange(size,dtype=numpy.<span class="built_in">int</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process %s sending %s &quot;</span> % (rank , senddata))</span><br><span class="line">comm.Reduce(senddata, recvdata, root=<span class="number">0</span>, op=MPI.SUM)			<span class="comment">#将每个 task 的第 i 个元素相加，然后放回到 P0 进程的第 i 个元素中。在接收操作中， P0 收到数据 [0 6 12]。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;on task&#x27;</span>, rank, <span class="string">&#x27;after Reduce:    data = &#x27;</span>, recvdata)</span><br></pre></td></tr></table></figure></div>

<p>我们用通讯组进程数为 3 来运行，等于维护的数组的大小。输出的结果如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">C:\&gt;mpiexec -n 3 python reduction2.py</span><br><span class="line"> process 2 sending [0 3 6]</span><br><span class="line">on task 2 after Reduce:    data =  [0 0 0]</span><br><span class="line"> process 1 sending [0 2 4]</span><br><span class="line">on task 1 after Reduce:    data =  [0 0 0]</span><br><span class="line"> process 0 sending [0 1 2]</span><br><span class="line">on task 0 after Reduce:    data =  [ 0  6 12]</span><br></pre></td></tr></table></figure></div>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> MPI</li>
        <li><strong>作者:</strong> Airex Yu</li>
        <li><strong>创建于:</strong> 2023-04-06 20:41:56</li>
        
            <li>
                <strong>更新于:</strong> 2023-05-25 20:56:41
            </li>
        
        <li>
            <strong>链接:</strong> http://example.com/2023/04/06/MPI/
        </li>
        <li>
            <strong>版权声明:</strong> 本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/%E8%BF%9B%E7%A8%8B/">#进程</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/%E9%80%9A%E8%AE%AF/">#通讯</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2023/04/13/J5/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">J5</span>
                                    <span class="post-nav-item">上一篇</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/03/27/yolo/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">yolo</span>
                                    <span class="post-nav-item">下一篇</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">MPI</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-%E7%9A%84%E5%8E%86%E5%8F%B2%E7%AE%80%E4%BB%8B"><span class="nav-text">MPI 的历史简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-%E5%AF%B9%E4%BA%8E%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="nav-text">MPI 对于消息传递模型的设计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MPI-Hello-World"><span class="nav-text">MPI Hello World</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Hello-world-%E4%BB%A3%E7%A0%81%E6%A1%88%E4%BE%8B"><span class="nav-text">Hello world 代码案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C-MPI-hello-world-%E7%A8%8B%E5%BA%8F"><span class="nav-text">运行 MPI hello world 程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MPI-Send-and-Receive"><span class="nav-text">MPI Send and Receive</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-%E7%9A%84%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E7%AE%80%E4%BB%8B"><span class="nav-text">MPI 的发送和接收简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80-MPI-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="nav-text">基础 MPI 数据结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-%E5%8F%91%E9%80%81-x2F-%E6%8E%A5%E6%94%B6-%E7%A8%8B%E5%BA%8F"><span class="nav-text">MPI 发送 &#x2F; 接收 程序</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-%E4%B9%92%E4%B9%93%E7%A8%8B%E5%BA%8F"><span class="nav-text">MPI 乒乓程序</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E7%A8%8B%E5%BA%8F"><span class="nav-text">环程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dynamic-Receiving-with-MPI-Probe-and-MPI-Status"><span class="nav-text">Dynamic Receiving with MPI Probe (and MPI Status)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-Status-%E7%BB%93%E6%9E%84%E4%BD%93"><span class="nav-text">MPI_Status 结构体</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-Status-%E7%BB%93%E6%9E%84%E4%BD%93%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="nav-text">MPI_Status 结构体查询的示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-MPI-Probe-%E6%89%BE%E5%87%BA%E6%B6%88%E6%81%AF%E5%A4%A7%E5%B0%8F"><span class="nav-text">使用 MPI_Probe 找出消息大小</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MPI-%E5%B9%BF%E6%92%AD%E4%BB%A5%E5%8F%8A%E9%9B%86%E4%BD%93-collective-%E9%80%9A%E4%BF%A1"><span class="nav-text">MPI 广播以及集体(collective)通信</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E4%BD%93%E9%80%9A%E4%BF%A1%E4%BB%A5%E5%8F%8A%E5%90%8C%E6%AD%A5%E7%82%B9"><span class="nav-text">集体通信以及同步点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-MPI-Bcast-%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%B9%BF%E6%92%AD"><span class="nav-text">使用 MPI_Bcast 来进行广播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-MPI-Send-%E5%92%8C-MPI-Recv-%E6%9D%A5%E5%81%9A%E5%B9%BF%E6%92%AD"><span class="nav-text">使用 MPI_Send 和 MPI_Recv 来做广播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-Bcast-%E5%92%8C-MPI-Send-%E4%BB%A5%E5%8F%8A-MPI-Recv-%E7%9A%84%E6%AF%94%E8%BE%83"><span class="nav-text">MPI_Bcast 和 MPI_Send 以及 MPI_Recv 的比较</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MPI-Scatter-Gather-and-Allgather"><span class="nav-text">MPI Scatter, Gather, and Allgather</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-Scatter-%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="nav-text">MPI_Scatter 的介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-Gather-%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="nav-text">MPI_Gather 的介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-MPI-Scatter-%E5%92%8C-MPI-Gather-%E6%9D%A5%E8%AE%A1%E7%AE%97%E5%B9%B3%E5%9D%87%E6%95%B0"><span class="nav-text">使用 MPI_Scatter 和 MPI_Gather 来计算平均数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPI-Allgather-%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9%E5%90%8E%E7%9A%84%E5%B9%B3%E5%9D%87%E7%A8%8B%E5%BA%8F"><span class="nav-text">MPI_Allgather 以及修改后的平均程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8MPI%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E6%8E%92%E5%90%8D"><span class="nav-text">使用MPI计算并行排名</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%8E%92%E5%90%8D-%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0"><span class="nav-text">并行排名 - 问题概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%8E%92%E5%90%8DAPI%E5%AE%9A%E4%B9%89"><span class="nav-text">并行排名API定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%B9%B6%E8%A1%8C%E6%8E%92%E5%90%8D%E9%97%AE%E9%A2%98"><span class="nav-text">解决并行排名问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%92%E5%BA%8F%E6%95%B0%E5%AD%97%E5%B9%B6%E7%BB%B4%E6%8A%A4%E6%89%80%E5%B1%9E"><span class="nav-text">排序数字并维护所属</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E5%90%88"><span class="nav-text">整合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="nav-text">完整代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94"><span class="nav-text">—————————————</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%BF%9B%E7%A8%8B%E7%9A%84%E5%B9%B6%E8%A1%8C"><span class="nav-text">基于进程的并行</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BA%A7%E7%94%9F%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B"><span class="nav-text">如何产生一个进程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%B8%BA%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%91%BD%E5%90%8D"><span class="nav-text">如何为一个进程命名</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B"><span class="nav-text">如何在后台运行一个进程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%9D%80%E6%8E%89%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B"><span class="nav-text">如何杀掉一个进程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8%E5%AD%90%E7%B1%BB%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9B%E7%A8%8B"><span class="nav-text">如何在子类中使用进程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8%E8%BF%9B%E7%A8%8B%E4%B9%8B%E9%97%B4%E4%BA%A4%E6%8D%A2%E5%AF%B9%E8%B1%A1"><span class="nav-text">如何在进程之间交换对象</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E9%98%9F%E5%88%97%E4%BA%A4%E6%8D%A2%E5%AF%B9%E8%B1%A1"><span class="nav-text">使用队列交换对象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%AE%A1%E9%81%93%E4%BA%A4%E6%8D%A2%E5%AF%B9%E8%B1%A1"><span class="nav-text">使用管道交换对象</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%9B%E7%A8%8B%E5%A6%82%E4%BD%95%E5%90%8C%E6%AD%A5"><span class="nav-text">进程如何同步</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8%E8%BF%9B%E7%A8%8B%E4%B9%8B%E9%97%B4%E7%AE%A1%E7%90%86%E7%8A%B6%E6%80%81"><span class="nav-text">如何在进程之间管理状态</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%BF%9B%E7%A8%8B%E6%B1%A0"><span class="nav-text">如何使用进程池</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Python%E7%9A%84mpi4py%E6%A8%A1%E5%9D%97"><span class="nav-text">使用Python的mpi4py模块</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%82%B9%E5%AF%B9%E7%82%B9%E9%80%9A%E8%AE%AF"><span class="nav-text">点对点通讯</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E4%BD%93%E9%80%9A%E8%AE%AF%EF%BC%9A%E4%BD%BF%E7%94%A8broadcast%E9%80%9A%E8%AE%AF"><span class="nav-text">集体通讯：使用broadcast通讯</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E4%BD%93%E9%80%9A%E8%AE%AF%EF%BC%9A%E4%BD%BF%E7%94%A8scatter%E9%80%9A%E8%AE%AF"><span class="nav-text">集体通讯：使用scatter通讯</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%93%E9%80%9A%E8%AE%AF%EF%BC%9A%E4%BD%BF%E7%94%A8gather%E9%80%9A%E8%AE%AF"><span class="nav-text">体通讯：使用gather通讯</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Alltoall%E9%80%9A%E8%AE%AF"><span class="nav-text">使用Alltoall通讯</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Reduce"><span class="nav-text">使用Reduce</span></a></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Airex Yu</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a> 驱动</span>
                <br>
            <span class="theme-version-container">主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.5</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/01/05 11:45:14
            </div>
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>




    
<script src="/js/libs/mermaid.min.js"></script>

    
<script src="/js/plugins/mermaid.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
