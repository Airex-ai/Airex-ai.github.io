<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Airex Yu">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/02/20/tensorrt/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="TensorRTTensorRT是什么TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT">
<meta property="og:url" content="http://example.com/2023/02/20/TensorRT/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="TensorRTTensorRT是什么TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221202104187-168493652323424.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221200627437-168493652323425.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221200708531-168493652323426.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221203344296-168493652323427.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221151604138-168493652323428.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221210110614-168493652323429.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221210123906-168493652323430.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221151952678-168493652323431.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221154127768-168493652323432.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221155924683-168493652323433.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221162707928-168493652323434.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224092537580-168493652323435.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224093014841-168493652323436.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224094858275-168493652323437.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224095026719-168493652323438.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224095059801-168493652323439.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224095136553-168493652323541.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224095222434-168493652323540.png">
<meta property="og:image" content="https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese/raw/main/7.TensorRT%E4%B8%AD%E7%9A%84INT8/q-dq-placement6.png">
<meta property="og:image" content="https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese/raw/main/7.TensorRT%E4%B8%AD%E7%9A%84INT8/sub-optimal.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224100753426-168493652323542.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230224104755566-168493652323543.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230220094750628-16849365232331.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230220101057791-16849365232332.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230220103617774-16849365232333.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221082244538-16849365232334.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221082316063-16849365232335.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221082544827-16849365232336.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221083048073-16849365232337.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221085107849-16769406687161-16849365232338.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221090526167-16849365232339.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221090939235-168493652323310.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221092555385-168493652323411.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221092623949-168493652323412.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221092806781-168493652323413.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221095444973-168493652323414.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221095945685-168493652323415.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221105750138-168493652323416.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221110212885-168493652323417.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221141202162-168493652323418.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221141607655-168493652323419.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221141916977-168493652323420.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221142034328-168493652323421.png">
<meta property="og:image" content="http://example.com/2023/02/20/TensorRT/image-20230221142046759-168493652323422.png">
<meta property="og:image" content="http://example.com/image-20230221142725737-168493652323423.png">
<meta property="article:published_time" content="2023-02-20T05:39:03.000Z">
<meta property="article:modified_time" content="2023-05-30T14:00:08.184Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="ONNX">
<meta property="article:tag" content="TensorRT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/02/20/TensorRT/image-20230221202104187-168493652323424.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/%E9%B1%BC.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/%E9%B1%BC.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/%E9%B1%BC.svg">
    <!--- Page Info-->
    
    <title>
        
            TensorRT -
        
        Airex-Daily
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    <!--- Font Part-->
    
    
    
        <link href="" rel="stylesheet">
    
    
        <link href="" rel="stylesheet">
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":true,"family":null,"url":null},"english":{"enable":true,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fix","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-rry6gw.png"},"title":"伸手也握不住彩虹🌈","subtitle":{"text":["——我期待"],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/Airex-ai","instagram":null,"zhihu":null,"twitter":null,"email":"airex.yu@foxmail.com"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":"https://music.163.com/song?id=1501530173&userid=253099352","cover":null}]},"mermaid":{"enable":true,"version":"9.3.0"}},"version":"2.1.5","navbar":{"auto_hide":true,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"About":{"icon":"fa-regular fa-user","submenus":{"Github":"https://github.com/Airex-ai?tab=repositories"}},"随记":{"icon":"fa-solid fa-tree-palm","path":"/masonry/"}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Airex-Daily
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        归档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/Airex-ai?tab=repositories">GITHUB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/masonry/"  >
                                    
                                        
                                            <i class="fa-solid fa-tree-palm"></i>
                                        
                                        随记
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                归档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://github.com/Airex-ai?tab=repositories">GITHUB</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/masonry/"  >
                             
                                
                                    <i class="fa-solid fa-tree-palm"></i>
                                
                                随记
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
             
                <div class="article-title">         
                    <img src="/images/TensorRT.png" alt="TensorRT" />
                    <h1 class="article-title-cover">TensorRT</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/%E5%A4%B4%E5%83%8F.JPG">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Airex Yu</span>
                            
                                <span class="author-label">Lv3</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-02-20 13:39:03</span>
        <span class="mobile">2023-02-20 13:39</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-05-30 22:08</span>
            <span class="mobile">2023-05-30 22</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
                    <li>
                        &gt; <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F/">模型训练加速</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/ONNX/">ONNX</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/TensorRT/">TensorRT</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="TensorRT"><a href="#TensorRT" class="headerlink" title="TensorRT"></a>TensorRT</h1><h2 id="TensorRT是什么"><a href="#TensorRT是什么" class="headerlink" title="TensorRT是什么"></a>TensorRT是什么</h2><p>TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。</p>
<p>TensorRT可以对网络进行压缩、优化以及运行时部署，并且没有框架的开销。TensorRT通过combines layers，kernel优化选择，以及根据指定的精度执行归一化和转换成最优的matrix math方法，改善网络的延迟、吞吐量以及效率。<strong>TensorRT可以直接从这些深度学习框架中获取深度学习模型的定义和权重</strong>。</p>
<p>一般的深度学习项目，训练时为了加快速度，会使用<strong>多GPU分布式训练</strong>。但在部署推理时，为了降低成本，往往使用单个GPU机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如caffe，TensorFlow等。<strong>可以认为tensorRT是一个只有前向传播的深度学习框架，这个框架可以将 Caffe，TensorFlow的网络模型解析，然后与tensorRT中对应的层进行一一映射，把其他框架的模型统一全部 转换到tensorRT中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略，并进行部署加速。</strong></p>
<p>目前TensorRT4.0 几乎可以支持所有常用的深度学习框架，<strong>对于caffe和TensorFlow来说，tensorRT可以直接解析他们的网络模型</strong>；对于caffe2，pytorch，mxnet，chainer，CNTK等框架则是首先要将模型转为 ONNX 的通用深度学习模型，然后对ONNX模型做解析。而tensorflow和MATLAB已经将TensorRT集成到框架中去了。</p>
<p><strong>tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221202104187-168493652323424.png"
                      class="" title="image-20230221202104187"
                >



<h2 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h2><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221200627437-168493652323425.png"
                      class="" title="image-20230221200627437"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221200708531-168493652323426.png"
                      class="" title="image-20230221200708531"
                >

<ul>
<li>训练（training）包含了<strong>前向传播和后向传播</strong>两个阶段，针对的是训练集。训练时通过误差反向传播来不断修改网络权值（weights）。</li>
<li>推理（inference）只包含前向传播一个阶段，针对的是除了训练集之外的新数据。可以是测试集，但不完全是，更多的是整个数据集之外的数据。其实就是针对新数据进行预测，预测时，速度是一个很重要的因素。</li>
</ul>
<h2 id="TensorRT加速原理"><a href="#TensorRT加速原理" class="headerlink" title="TensorRT加速原理"></a>TensorRT加速原理</h2><p>加速原理比较复杂，它将会根据显卡来优化算子，以起到加速作用（如下图所示）。简单的来说，就是类似于你出一个公式1+1+1，而你的显卡支持乘法，直接给你把这个公式优化成了1*3，一步算完，所以自然更快。当然这个加速程度很大程度上取决于你的<strong>显卡算力</strong>还有<strong>模型结构复杂程度</strong>，也有可能因为没有优化的地方，从而没有多大的提升</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221203344296-168493652323427.png"
                      class="" title="image-20230221203344296"
                >

<h3 id="层间融合或张量融合（Layer-amp-Tensor-Fusion）"><a href="#层间融合或张量融合（Layer-amp-Tensor-Fusion）" class="headerlink" title="层间融合或张量融合（Layer&amp;Tensor Fusion）"></a>层间融合或张量融合（Layer&amp;Tensor Fusion）</h3><p>如下图左侧是GoogLeNetInception模块的计算图。这个结构中有很多层，在部署模型推理时，这每一层的运算操作都是由GPU完成的，但实际上是<strong>GPU通过启动不同的CUDA（Compute unified device architecture）核心来完成计算的</strong>，CUDA核心计算张量的速度是很快的，<strong>但是往往大量的时间是浪费在CUDA核心的启动和对每一层输入&#x2F;输出张量的<code>读写</code>操作上面</strong>，这造成了内存带宽的瓶颈和GPU资源的浪费。TensorRT通过对层间的横向或纵向合并（合并后的结构称为CBR，意指 <strong>convolution, bias, and ReLU layers are fused to form a single layer</strong>），使得层的数量大大减少。<strong>纵向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。横向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心。</strong>合并之后的计算图（图4右侧）的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221151604138-168493652323428.png"
                      class="" title="image-20230221151604138"
                >



<h3 id="数据精度校准"><a href="#数据精度校准" class="headerlink" title="数据精度校准"></a><strong>数据精度校准</strong></h3><p><strong>大部分深度学习框架在训练神经网络时网络中的张量（Tensor）都是32位浮点数的精度（Full 32-bit precision，FP32），一旦网络训练完成，在部署推理的过程中由于不需要反向传播（可以理解成，训练时反向传播数据需要更高的精度），完全可以适当降低数据精度，比如降为FP16或INT8的精度。</strong>更低的数据精度将会使得内存占用和延迟更低，模型体积更小。</p>
<p>如下表为不同精度的动态范围：</p>
<table>
<thead>
<tr>
<th align="left">Precision</th>
<th align="left">Dynamic Range</th>
</tr>
</thead>
<tbody><tr>
<td align="left">FP32</td>
<td align="left">−3.4×1038 +3.4×1038−3.4×1038 +3.4×1038</td>
</tr>
<tr>
<td align="left">FP16</td>
<td align="left">−65504 +65504−65504 +65504</td>
</tr>
<tr>
<td align="left">INT8</td>
<td align="left">−128 +127−128 +127</td>
</tr>
</tbody></table>
<p>INT8只有256个不同的数值，使用INT8来表示 FP32精度的数值，肯定会丢失信息，造成性能下降。<strong>不过TensorRT会提供完全自动化的校准（Calibration ）过程，会以最好的匹配性能将FP32精度的数据降低为INT8精度，最小化性能损失。</strong>关于校准过程，后面会专门做一个探究。</p>
<h3 id="Kernel-Auto-Tuning"><a href="#Kernel-Auto-Tuning" class="headerlink" title="Kernel Auto-Tuning"></a><strong>Kernel Auto-Tuning</strong></h3><p>网络模型在推理计算时，是调用GPU的CUDA核进行计算的。TensorRT可以针对不同的算法，不同的网络模型，不同的GPU平台，进行 CUDA核的调整，以保证当前模型在特定平台上以最优性能计算。</p>
<h3 id="Dynamic-Tensor-Memory"><a href="#Dynamic-Tensor-Memory" class="headerlink" title="Dynamic Tensor Memory"></a><strong>Dynamic Tensor Memory</strong></h3><p>在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。</p>
<h3 id="Multi-Stream-Execution"><a href="#Multi-Stream-Execution" class="headerlink" title="Multi-Stream Execution"></a><strong>Multi-Stream Execution</strong></h3><p>可扩展设计，可并行处理多个输入流</p>
<h2 id="TensorRT使用流程"><a href="#TensorRT使用流程" class="headerlink" title="TensorRT使用流程"></a>TensorRT使用流程</h2><p>TensorRT使用流程分为两个阶段：<strong>预处理阶段和推理阶段</strong></p>
<p><strong>预处理</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221210110614-168493652323429.png"
                      class="" title="image-20230221210110614"
                >

<p><strong>推理</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221210123906-168493652323430.png"
                      class="" title="image-20230221210123906"
                >

<ol>
<li>导出网络定义以及相关权重</li>
<li>解析网络定义以及相关权重</li>
<li>根据显卡算子构造出最优执行计划</li>
<li>将执行计划序列化存储（<strong>预处理结束</strong>）</li>
<li>反序列化执行任务（<strong>推理开始</strong>）</li>
<li>进行推理</li>
</ol>
<p><strong>注意：tensorRT实际上和硬件绑定，在部署过程中，若硬件和软件发生了改变，这一步就要重新进行</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221151952678-168493652323431.png"
                      class="" title="image-20230221151952678"
                >

<h1 id="导出ONNX模型"><a href="#导出ONNX模型" class="headerlink" title="导出ONNX模型"></a>导出ONNX模型</h1><p>这一步主要是为了将深度学习模型的结构和参数导出来。</p>
<p>ONNX不像Pytorch和TensorFlow那样，需要安装框架运行的依赖包，TensorRT可以直接从ONNX文件中读取网络定义和权重信息。除此之外。<strong>ONNX更像是“通用语言”，是为了描述网络结构和相关权重而生。</strong>还有专门的工具可以直查看onnx文件。因此，可以先将深度学习模型导出ONNX文件，通过ONNX文件部署</p>
<p>查看ONNX文件的工具 <code> https://lutzroeder.github.io/netron/</code>  <strong>netron的在线工具</strong></p>
<h2 id="导出onnx文件"><a href="#导出onnx文件" class="headerlink" title="导出onnx文件"></a>导出onnx文件</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造模型实例</span></span><br><span class="line"> model = HRNet() </span><br><span class="line"><span class="comment"># 反序列化权重参数 </span></span><br><span class="line">model.load_state_dict(torch.load(self.weight_path),strict=<span class="literal">False</span>) </span><br><span class="line">model.<span class="built_in">eval</span>() </span><br><span class="line"><span class="comment"># 定义输入名称，list结构，可能有多个输入 </span></span><br><span class="line">input_names = [<span class="string">&#x27;input&#x27;</span>] </span><br><span class="line"><span class="comment"># 定义输出名称，list结构，可能有多个输出 </span></span><br><span class="line">output_names = [<span class="string">&#x27;output&#x27;</span>] </span><br><span class="line"><span class="comment"># 构造输入用以验证onnx模型的正确性 </span></span><br><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">288</span>) </span><br><span class="line"><span class="comment"># 导出 </span></span><br><span class="line">torch.onnx.export(model, <span class="built_in">input</span>, output_path,</span><br><span class="line">                          export_params=<span class="literal">True</span>,                          </span><br><span class="line">                          opset_version=<span class="number">11</span>,                          </span><br><span class="line">                          do_constant_folding=<span class="literal">True</span>,                          </span><br><span class="line">                          input_names=input_names,                          </span><br><span class="line">                          output_names=output_names)</span><br></pre></td></tr></table></figure></div>

<p>参数介绍：</p>
<ul>
<li>model为pytorch模型实例</li>
<li>input为测试输入数据（形状必须要和模型输入一致，但是数值可以是随机的）</li>
<li>output_path为导出路径，xxx.onnx</li>
<li>export_params为是否导出参数权重，必然是True</li>
<li>opset_version&#x3D;11 发行版本，11就可以了</li>
<li><strong>do_constant_folding是否对常量进行折叠，True就可以了</strong></li>
<li>input_names是模型输入的名称，list类型，因为模型可能有多个输入</li>
<li>output_names同上，只不过这是针对输出的</li>
</ul>
<h1 id="TensorRT推理阶段"><a href="#TensorRT推理阶段" class="headerlink" title="TensorRT推理阶段"></a>TensorRT推理阶段</h1><p>根据onnx文件所描述的模型结构和权重和当前的软硬件环境生成对应的执行计划。</p>
<p>这一步时间比较长，所以需要序列化执行文件，<strong>生成xxx.engine</strong>文件持久化保存</p>
<h2 id="trtexec-exe实现预推理（推荐）"><a href="#trtexec-exe实现预推理（推荐）" class="headerlink" title="trtexec.exe实现预推理（推荐）"></a>trtexec.exe实现预推理（推荐）</h2><p>使用trtexec.exe实现预推理需要系统安装好cudatoolkit和cudnn，否则无法正常运行</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221154127768-168493652323432.png"
                      class="" title="image-20230221154127768"
                >

<p>搜索运行cmd，并且cd到此目录下，并且将需要部署的onnx文件复制到此目录下</p>
<p>输入以下指令：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=xxx.onnx --saveEngine=xxx.engine --fp16</span><br></pre></td></tr></table></figure></div>

<p>–fp16开启 float16精度的推理（推荐此模式，一方面能够加速，另一方面精度下降比较小）</p>
<p>–int8 开启 int8精度的推理（不太推荐，虽然更快，但是精度下降太厉害了）</p>
<p>–onnx onnx路径</p>
<p>–saveEngine执行计划（推理引擎）序列化地址</p>
<h2 id="python代码实现预推理"><a href="#python代码实现预推理" class="headerlink" title="python代码实现预推理"></a>python代码实现预推理</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必用依赖 </span></span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt </span><br><span class="line"><span class="comment"># 创建logger：日志记录器 </span></span><br><span class="line">logger = trt.Logger(trt.Logger.WARNING) </span><br><span class="line"><span class="comment"># 创建构建器builder </span></span><br><span class="line">builder = trt.Builder(logger) </span><br><span class="line"><span class="comment"># 预创建网络     空网络</span></span><br><span class="line">network = builder.create_network(<span class="number">1</span> &amp;lt;&amp;lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) </span><br><span class="line"><span class="comment"># 加载onnx解析器 </span></span><br><span class="line">parser = trt.OnnxParser(network, logger) </span><br><span class="line">success = parser.parse_from_file(onnx_path) </span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(parser.num_errors):</span><br><span class="line">  <span class="built_in">print</span>(parser.get_error(idx))</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> success:  <span class="keyword">pass</span>  <span class="comment"># Error handling code here </span></span><br><span class="line"> <span class="comment"># builder配置    builder解析器将onnx文件解析成网络，并配置网络</span></span><br><span class="line">config = builder.create_builder_config()</span><br><span class="line"><span class="comment"># 分配显存作为工作区间，一般建议为显存一半的大小 </span></span><br><span class="line">config.max_workspace_size = <span class="number">1</span> &amp;lt;&amp;lt; <span class="number">30</span>  <span class="comment"># 1 Mi </span></span><br><span class="line">serialized_engine = builder.build_serialized_network(network, config) </span><br><span class="line"><span class="comment"># 序列化生成engine文件 </span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(engine_path, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">   f.write(serialized_engine)</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;generate file success!&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<h1 id="TensorRt部署阶段"><a href="#TensorRt部署阶段" class="headerlink" title="TensorRt部署阶段"></a>TensorRt部署阶段</h1><p>在本阶段，需要使用代码实现加载执行计划文件（xxx.engine）</p>
<p>其中input&#x2F;output为你的输入数据，h_input&#x2F;h_output为<strong>锁页内存</strong>（非锁页内存也是可以的，但是建议用锁页内存防止被系统页面置换到外存中），d_input&#x2F;d_output为显存  </p>
<p>锁页内存：硬件外设直接访问CPU内存，从而避免过多的复制操作。CPU也可以访问上述锁页内存，但是此内存是不能移动或换页到磁盘上的。另外，<strong>在GPU上分配的内存默认都是锁页内存，这只是因为GPU不支持将内存交换到磁盘上。</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221155924683-168493652323433.png"
                      class="" title="image-20230221155924683"
                >

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必用依赖</span></span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit  <span class="comment">#负责数据初始化，内存管理，销毁等</span></span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> cuda  <span class="comment">#GPU CPU之间的数据传输</span></span><br><span class="line"><span class="comment"># 创建logger：日志记录器</span></span><br><span class="line">logger = trt.Logger(trt.Logger.WARNING)</span><br><span class="line"><span class="comment"># 创建runtime并反序列化生成engine</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(“sample.engine”, “rb”) <span class="keyword">as</span> f, trt.Runtime(logger) <span class="keyword">as</span> runtime:</span><br><span class="line">    engine = runtime.deserialize_cuda_engine(f.read())</span><br><span class="line"><span class="comment"># 分配CPU锁页内存和GPU显存   		分配锁页内存和显存只需要一次</span></span><br><span class="line">h_input = cuda.pagelocked_empty(trt.volume(context.get_binding_shape(<span class="number">0</span>)), dtype=np.float32)</span><br><span class="line">h_output = cuda.pagelocked_empty(trt.volume(context.get_binding_shape(<span class="number">1</span>)), dtype=np.float32)</span><br><span class="line">d_input = cuda.mem_alloc(h_input.nbytes)</span><br><span class="line">d_output = cuda.mem_alloc(h_output.nbytes)</span><br><span class="line"><span class="comment"># 创建cuda流</span></span><br><span class="line">stream = cuda.Stream()</span><br><span class="line"><span class="comment"># 创建context并进行推理</span></span><br><span class="line"><span class="keyword">with</span> engine.create_execution_context() <span class="keyword">as</span> context:</span><br><span class="line">    <span class="comment"># 将输入数据从锁页内存拷贝到显存</span></span><br><span class="line">    cuda.memcpy_htod_async(d_input, h_input, stream)</span><br><span class="line">    <span class="comment"># 进行推理</span></span><br><span class="line">    context.execute_async_v2(bindings=[<span class="built_in">int</span>(d_input), <span class="built_in">int</span>(d_output)], stream_handle=stream.handle)</span><br><span class="line">    <span class="comment"># 将输出结果从显存拷贝到锁页内存</span></span><br><span class="line">    cuda.memcpy_dtoh_async(h_output, d_output, stream)</span><br><span class="line">    <span class="comment"># 同步流</span></span><br><span class="line">    stream.synchronize()</span><br><span class="line">    <span class="comment"># 返回主内存的输出结果. 该数据等同于原始模型的输出数据</span></span><br><span class="line">    <span class="keyword">return</span> h_output</span><br></pre></td></tr></table></figure></div>

<p>内存和显存只分配一次，所以将代码进行封装</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造你自己的输入数据 </span></span><br><span class="line"><span class="built_in">input</span> = np.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">288</span>) </span><br><span class="line"><span class="comment"># 拷贝到锁页内存 </span></span><br><span class="line">np.copyto(h_input, <span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造你自己的输入数据 </span></span><br><span class="line"><span class="built_in">input</span> = np.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">288</span>) </span><br><span class="line"><span class="comment"># 拷贝到锁页内存 </span></span><br><span class="line">np.copyto(h_input, <span class="built_in">input</span>)</span><br></pre></td></tr></table></figure></div>

<h1 id="TensorRT实现batch批处理"><a href="#TensorRT实现batch批处理" class="headerlink" title="TensorRT实现batch批处理"></a>TensorRT实现batch批处理</h1><p>将多个输入数据构造为一个batch，一次性输入深度学习模型并进行预测，并获得对应的结果。</p>
<h2 id="导出ONNX"><a href="#导出ONNX" class="headerlink" title="导出ONNX"></a>导出ONNX</h2><p>我们在导出模型的时候，需要将模型输入的batch参数声明为动态参数，例如，hrnet的输入数据维度为(1, 3 , 384, 288)，第一个维度为batch的大小，第二个维度为RGB三个色彩通道，第三个维度为图片的高，第四个维度为图片的宽，因此这里我们在导出onnx模型时，<strong>需要将第一个维度的参数声明为动态参数</strong>，具体代码如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义输入名称，list结构，可能有多个输入</span></span><br><span class="line">input_names = [<span class="string">&#x27;input&#x27;</span>]</span><br><span class="line"><span class="comment"># 定义输出名称，list结构，可能有多个输出</span></span><br><span class="line">output_names = [<span class="string">&#x27;output&#x27;</span>]</span><br><span class="line"><span class="comment"># 声明动态维度，这里我们把input的第0维度赋名为batch_size</span></span><br><span class="line">dynamic_axes = &#123;</span><br><span class="line">            <span class="string">&#x27;input&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch_size&#x27;</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line"> <span class="comment"># 构造输入，用以onnx验证</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">288</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.onnx.export(model, <span class="built_in">input</span>, output_path,</span><br><span class="line">                          export_params=<span class="literal">True</span>,</span><br><span class="line">                          opset_version=<span class="number">10</span>,</span><br><span class="line">                          do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">                          input_names=input_names,</span><br><span class="line">                          output_names=output_names,</span><br><span class="line">                          dynamic_axes=dynamic_axes)</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221162707928-168493652323434.png"
                      class="" title="image-20230221162707928"
                >

<h2 id="模型构造阶段"><a href="#模型构造阶段" class="headerlink" title="模型构造阶段"></a>模型构造阶段</h2><p>当要batch批处理时，需指定shapes参数</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --fp16 --shapes=input:32x3x384x288 --onnx=xxxx.onnx --saveEngine=xxxx.trt </span><br></pre></td></tr></table></figure></div>

<h1 id="执行阶段"><a href="#执行阶段" class="headerlink" title="执行阶段"></a>执行阶段</h1><p>在执行阶段，唯一需要注意的是TensorRT的输入必须是严格固定的batch_size大小，即每次输入到trt模型时，输入必须是batch_size大小严格等于构造阶段的输入，<strong>因此对于小于batch_size大小的数据需要0填充处理</strong> </p>
<h1 id="TensorRT官档介绍"><a href="#TensorRT官档介绍" class="headerlink" title="TensorRT官档介绍"></a>TensorRT官档介绍</h1><h2 id="The-Programming-Model"><a href="#The-Programming-Model" class="headerlink" title="The Programming Model"></a>The Programming Model</h2><p>TensorRT 构建阶段的最高级别接口是<strong>Builder</strong> （ C++ 、 Python ）。构建器负责优化模型并生成Engine 。</p>
<p>为了构建引擎，您需要：</p>
<ul>
<li>创建网络定义</li>
<li>为builder指定配置</li>
<li>调用builder创建引擎</li>
</ul>
<p><code>NetworkDefinition</code>接口用于定义模型。将模型传输到 TensorRT 的最常见途径是以 ONNX 格式从框架中导出模型，并使用 TensorRT 的 ONNX 解析器来填充网络定义。但是，您也可以使用 TensorRT 的Layer和Tensor 接口逐步构建定义。</p>
<p>无论选择哪种方式，还必须定义哪些张量是网络的输入和输出。<strong>未标记为输出的张量被认为是可以由构建器优化掉的瞬态值</strong>。输入和输出张量必须命名，以便在运行时，TensorRT 知道如何将输入和输出缓冲区绑定到模型。</p>
<p><strong><code>BuilderConfig</code>接口用于指定TensorRT如何优化模型。</strong>在可用的配置选项中，您可以控制 TensorRT 降低计算精度的能力，控制内存和运行时执行速度之间的权衡，以及限制对 CUDA ®内核的选择。由于构建器可能需要几分钟或更长时间才能运行，因此您还可以控制构建器搜索内核的方式，以及缓存搜索结果以供后续运行使用。</p>
<p><strong>一旦有了网络定义和构建器配置，就可以调用构建器来创建引擎。</strong>构建器消除了无效计算、折叠常量、重新排序和组合操作以在 GPU 上更高效地运行。它可以选择性地降低浮点计算的精度，方法是简单地在 16 位浮点中运行它们，或者通过量化浮点值以便可以使用 8 位整数执行计算。它还使用不同的数据格式对每一层的多次实现进行计时，然后计算执行模型的最佳时间表，从而最大限度地降低内核执行和格式转换的综合成本。</p>
<p><strong>构建器以称为计划的序列化形式创建引擎，该计划可以立即反序列化，或保存到磁盘以供以后使用。</strong></p>
<p><strong>注意：</strong></p>
<ul>
<li><strong>TensorRT 创建的引擎特定于创建它们的 TensorRT 版本和创建它们的 GPU。</strong></li>
<li>TensorRT 的网络定义不会深度复制参数数组（例如卷积的权重）。因此，在构建阶段完成之前，您不得释放这些阵列的内存。使用 ONNX 解析器导入网络时，解析器拥有权重，因此在构建阶段完成之前不得将其销毁。</li>
<li>构建器时间算法以确定最快的。与其他 GPU 工作并行运行构建器可能会扰乱时序，导致优化不佳。</li>
</ul>
<h3 id="The-Runtime-Phase"><a href="#The-Runtime-Phase" class="headerlink" title="The Runtime Phase"></a>The Runtime Phase</h3><p>TensorRT 执行阶段的最高级别接口是<code>Runtime</code>。 使用运行时时，您通常会执行以下步骤：</p>
<ul>
<li>反序列化创建引擎的计划(plan 文件)</li>
<li><strong>从引擎创建执行上下文(context) 然后，反复：</strong></li>
<li><strong>填充输入缓冲区以进行推理</strong></li>
<li>调用enqueue()或execute()以运行推理</li>
</ul>
<p><code>Engine</code>接口）代表一个优化模型。您可以查询引擎以获取有关网络输入和输出张量的信息——预期的维度、数据类型、数据格式等。</p>
<p><strong><code>ExecutionContext</code>接口是调用推理的主要接口</strong>。执行上下文包含与特定调用关联的所有状态 – 因此您可以拥有与单个引擎关联的多个上下文，并并行运行它们。</p>
<p>调用推理时，您必须在适当的位置设置输入和输出缓冲区。根据数据的性质，这可能在 CPU 或 GPU 内存中。如果根据您的模型不明显，您可以查询引擎以确定在哪个内存空间中提供缓冲区。</p>
<p>设置缓冲区后，可以同步（执行）或异步（入队）调用推理。在后一种情况下，所需的内核在 CUDA 流上排队，并尽快将控制权返回给应用程序。一些网络需要在 CPU 和 GPU 之间进行多次控制传输，因此控制可能不会立即返回。要等待异步执行完成，请使用<code>cudaStreamSynchronize</code>在流上同步。</p>
<h2 id="Plugins"><a href="#Plugins" class="headerlink" title="Plugins"></a>Plugins</h2><p>TensorRT 有一个<code>Plugin</code>接口，允许应用程序提供 TensorRT 本身不支持的操作的实现。在转换网络时，ONNX 解析器可以找到使用 TensorRT 的<code>PluginRegistry</code>创建和注册的插件。<a class="link"   target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#extending" >自定义层扩展 TensorRT <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Types-and-Precision"><a href="#Types-and-Precision" class="headerlink" title="Types and Precision"></a>Types and Precision</h2><p>TensorRT 支持使用 <code>FP32</code>、<code>FP16</code>、<code>INT8</code>、<code>Bool</code> 和 <code>INT32</code> 数据类型的计算。 当 TensorRT 选择 CUDA 内核在网络中实现浮点运算时，它默认为 <code>FP32</code> 实现。有两种方法可以配置不同的精度级别：</p>
<ul>
<li>为了在模型级别控制精度， BuilderFlag选项可以向 TensorRT 指示它在搜索最快时可能会选择较低精度的实现（并且因为较低的精度通常更快，如果允许的话，它通常会）。 因此，您可以轻松地指示 TensorRT 为您的整个模型使用 FP16 计算。对于输入动态范围约为 1 的正则化模型，这通常会产生显着的加速，而准确度的变化可以忽略不计。</li>
<li>对于更细粒度的控制，由于网络的一部分对数值敏感或需要高动态范围，因此层必须以更高的精度运行，可以为该层<strong>指定算术精度。</strong></li>
</ul>
<h2 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a>Quantization</h2><p>TensorRT 支持量化浮点，其中浮点值被线性压缩并四舍五入为 8 位整数。这显着提高了算术吞吐量，同时降低了存储要求和内存带宽。在量化浮点张量时，TensorRT 需要知道它的动态范围——即表示什么范围的值很重要——<strong>量化时会钳制超出该范围的值。</strong></p>
<p><strong>动态范围信息可由构建器根据代表性输入数据计算（这称为校准–<code>calibration</code>）</strong>。或者，您可以在框架中执行量化感知训练，并将模型与必要的动态范围信息一起导入到 TensorRT。</p>
<h2 id="Tensors-and-Data-Formats"><a href="#Tensors-and-Data-Formats" class="headerlink" title="Tensors and Data Formats"></a>Tensors and Data Formats</h2><p>在定义网络时，TensorRT 假设张量由多维 C 样式数组表示。每一层对其输入都有特定的解释：例如，2D 卷积将假定其输入的最后三个维度是 CHW 格式 – 没有选项可以使用，例如 WHC 格式。</p>
<p>请注意，张量最多只能包含 2^31-1 个元素。 在优化网络的同时，TensorRT 在内部执行转换（<strong>包括到 HWC，但也包括更复杂的格式转换</strong>）以使用尽可能快的 CUDA 内核。通常，选择格式是为了优化性能，而应用程序无法控制这些选择。然而，底层数据格式暴露在 I&#x2F;O 边界（网络输入和输出，以及将数据传入和传出插件），以允许应用程序最大限度地减少不必要的格式转换。</p>
<h2 id="Dynamic-Shapes"><a href="#Dynamic-Shapes" class="headerlink" title="Dynamic Shapes"></a>Dynamic Shapes</h2><p>默认情况下，TensorRT 根据定义时的输入形状（批量大小、图像大小等）优化模型。但是，可以将构建器配置为允许在运行时调整输入维度。为了启用此功能，您可以在构建器配置中指定一个或多个<code>OptimizationProfile</code>实例，其中包含每个输入的最小和最大形状，以及该范围内的优化点。</p>
<p>TensorRT 为每个配置文件创建一个优化的引擎，选择适用于 [最小、最大] 范围内的所有形状的 CUDA 内核，并且对于优化点来说是最快的——<strong>通常每个配置文件都有不同的内核</strong>。然后，您可以在运行时在配置文件中进行选择。</p>
<h2 id="DLA"><a href="#DLA" class="headerlink" title="DLA"></a>DLA</h2><p>TensorRT 支持 NVIDIA 的深度学习加速器 (DLA)，这是许多 NVIDIA SoC 上的专用推理处理器，支持 TensorRT 层的子集。 TensorRT 允许您在 DLA 上执行部分网络，而在 GPU 上执行其余部分；对于可以在任一设备上执行的层，您可以在构建器配置中逐层选择目标设备。</p>
<h2 id="Updating-Weights"><a href="#Updating-Weights" class="headerlink" title="Updating Weights"></a>Updating Weights</h2><p><strong>在构建引擎时，您可以指定它可能需要稍后更新其权重</strong>。如果您经常在不更改结构的情况下更新模型的权重，例如在强化学习中或在保留相同结构的同时重新训练模型时，这将很有用。权重更新是通过**<code>Refitter</code>** 接口执行的。</p>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a><strong>工具</strong></h2><h3 id="trtexec"><a href="#trtexec" class="headerlink" title="trtexec"></a>trtexec</h3><p>示例目录中包含一个名为<code>trtexec</code>的命令行包装工具。 <code>trtexec</code>是一种无需开发自己的应用程序即可快速使用 TensorRT 的工具。 trtexec工具有三个主要用途：</p>
<ul>
<li><strong>在随机或用户提供的输入数据上对网络进行基准测试。</strong></li>
<li>从模型生成序列化引擎。</li>
<li>从构建器生成序列化时序缓存。</li>
</ul>
<h3 id="Polygraphy"><a href="#Polygraphy" class="headerlink" title="Polygraphy"></a>Polygraphy</h3><p>Polygraphy 是一个工具包，旨在帮助在 TensorRT 和其他框架中运行和调试深度学习模型。它包括一个Python API和一个使用此 API 构建的命令行界面 (CLI) 。</p>
<p>除此之外，使用 Polygraphy，您可以：</p>
<ul>
<li>在多个后端之间运行推理，例如 TensorRT 和 ONNX-Runtime，并比较结果（例如<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/api/01_comparing_frameworks" >API <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 、 <a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/cli/run/01_comparing_frameworks" >CLI <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）</li>
<li>将模型转换为各种格式，例如具有训练后量化的 TensorRT 引擎（例如<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/api/04_int8_calibration_in_tensorrt" >API <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 、 <a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/cli/convert/01_int8_calibration_in_tensorrt" >CLI <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）</li>
<li>查看有关各种类型模型的信息（例如<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/cli/inspect" >CLI <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）</li>
<li>在命令行上修改 ONNX 模型：<ul>
<li>提取子图（例如<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/cli/surgeon/01_isolating_subgraphs" >CLI <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）</li>
<li>简化和清理（例如<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/cli/surgeon/02_folding_constants" >CLI <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）</li>
</ul>
</li>
<li>隔离 TensorRT 中的错误策略（例如<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/tools/Polygraphy/examples/cli/debug/01_debugging_flaky_trt_tactics" >CLI <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）</li>
</ul>
<h1 id="TensorRT的C-接口解析"><a href="#TensorRT的C-接口解析" class="headerlink" title="TensorRT的C++接口解析"></a>TensorRT的C++接口解析</h1><p>C++ API 可以通过头文件<code>NvInfer.h</code>访问，并且位于<code>nvinfer1</code>命名空间中。例如，一个简单的应用程序可能以下开头：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> “NvInfer.h”</span></span><br><span class="line"></span><br><span class="line">using namespace nvinfer1;</span><br></pre></td></tr></table></figure></div>

<p><strong>CUDA 上下文会在 TensorRT 第一次调用 CUDA 时自动创建，</strong>如果在该点之前不存在。通常最好在第一次调用 TensoRT 之前自己创建和配置 CUDA 上下文。 为了说明对象的生命周期，本章中的代码不使用智能指针；但是，建议将它们与 TensorRT 接口一起使用。</p>
<h2 id="The-Build-Phase"><a href="#The-Build-Phase" class="headerlink" title="The Build Phase"></a>The Build Phase</h2><p>要创建构建器，首先需要实例化ILogger接口。此示例捕获所有警告消息，但忽略信息性消息：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span> :</span> public ILogger           </span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">void</span> <span class="title function_">log</span><span class="params">(Severity severity, <span class="type">const</span> <span class="type">char</span>* msg)</span> noexcept override</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// suppress info-level messages</span></span><br><span class="line">        <span class="keyword">if</span> (severity &lt;= Severity::kWARNING)</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; msg &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; logger;</span><br></pre></td></tr></table></figure></div>

<p>然后，您可以创建构建器的实例：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = createInferBuilder(logger);</span><br></pre></td></tr></table></figure></div>

<h3 id="Creating-a-Network-Definition"><a href="#Creating-a-Network-Definition" class="headerlink" title="Creating a Network Definition"></a>Creating a Network Definition</h3><p>创建构建器后，优化模型的第一步是创建网络定义：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">uint32_t</span> flag = <span class="number">1U</span> &lt;&lt;static_cast&lt;<span class="type">uint32_t</span>&gt;</span><br><span class="line">    (NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); 	<span class="comment">//为了使用 ONNX 解析器导入模型，需要kEXPLICIT_BATCH标志。</span></span><br><span class="line"></span><br><span class="line">INetworkDefinition* network = builder-&gt;createNetworkV2(flag);</span><br></pre></td></tr></table></figure></div>

<h3 id="Importing-a-Model-using-the-ONNX-Parser"><a href="#Importing-a-Model-using-the-ONNX-Parser" class="headerlink" title="Importing a Model using the ONNX Parser"></a>Importing a Model using the ONNX Parser</h3><p>现在，需要从 ONNX 表示中填充网络定义。 ONNX 解析器 API 位于文件NvOnnxParser.h中，解析器位于<code>nvonnxparser</code> C++ 命名空间中。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> “NvOnnxParser.h”</span></span><br><span class="line"></span><br><span class="line">using namespace nvonnxparser;</span><br></pre></td></tr></table></figure></div>

<p>您可以创建一个 ONNX 解析器来填充网络，如下所示：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IParser*  parser = createParser(*network, logger);</span><br></pre></td></tr></table></figure></div>

<p>然后，读取模型文件并处理任何错误。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parser-&gt;parseFromFile(modelFile, </span><br><span class="line">    static_cast&lt;<span class="type">int32_t</span>&gt;(ILogger::Severity::kWARNING));</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int32_t</span> i = <span class="number">0</span>; i &lt; parser.getNbErrors(); ++i)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; parser-&gt;getError(i)-&gt;desc() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>TensorRT 网络定义的一个重要方面是它包含指向模型权重的指针，这些指针由构建器复制到优化的引擎中。<strong>由于网络是通过解析器创建的，解析器拥有权重占用的内存，因此在构建器运行之前不应删除解析器对象。</strong></p>
<h3 id="Building-an-Engine"><a href="#Building-an-Engine" class="headerlink" title="Building an Engine"></a>Building an Engine</h3><p>下一步是创建一个构建配置，指定 TensorRT 应该如何优化模型。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IBuilderConfig* config = builder-&gt;createBuilderConfig();</span><br></pre></td></tr></table></figure></div>

<p>这个接口有很多属性，你可以设置这些属性来控制 TensorRT 如何优化网络。<strong>一个重要的属性是最大工作空间大小</strong>。层实现通常需要一个临时工作空间，并且此参数限制了网络中任何层可以使用的最大大小。<strong>如果提供的工作空间不足，TensorRT 可能无法找到层的实现</strong>。默认情况下，工作区设置为给定设备的总全局内存大小；必要时限制它，例如，在单个设备上构建多个引擎时。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;setMemoryPoolLimit(MemoryPoolType::kWORKSPACE, <span class="number">1U</span> &lt;&lt; <span class="number">20</span>);</span><br></pre></td></tr></table></figure></div>

<p>一旦指定了配置，就可以构建引擎。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IHostMemory*  serializedModel = builder-&gt;buildSerializedNetwork(*network, *config);</span><br></pre></td></tr></table></figure></div>

<p><strong>由于序列化引擎包含权重的必要拷贝</strong>，因此不再需要解析器、网络定义、构建器配置和构建器，可以安全地删除：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">delete parser;</span><br><span class="line">delete network;</span><br><span class="line">delete config;</span><br><span class="line">delete builder;</span><br></pre></td></tr></table></figure></div>

<p>然后可以将引擎保存到磁盘，并且可以删除它被序列化到的缓冲区。</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete serializedModel</span><br></pre></td></tr></table></figure></div>

<p><strong>注意：序列化引擎不能跨平台或 TensorRT 版本移植。引擎特定于它们构建的确切 GPU 模型（除了平台和 TensorRT 版本）。</strong></p>
<h2 id="Deserializing-a-Plan"><a href="#Deserializing-a-Plan" class="headerlink" title="Deserializing a Plan"></a>Deserializing a Plan</h2><p>假设您之前已经序列化了一个优化模型并希望执行推理，您将需要创建一个<strong>运行时接口</strong>的实例。与构建器一样，运行时需要一个记录器实例：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IRuntime* runtime = createInferRuntime(logger);</span><br></pre></td></tr></table></figure></div>

<p>假设您已将模型从缓冲区中读取，然后可以对其进行反序列化以获得引擎：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ICudaEngine* engine = </span><br><span class="line">  runtime-&gt;deserializeCudaEngine(modelData, modelSize);</span><br></pre></td></tr></table></figure></div>

<h2 id="Performing-Inference"><a href="#Performing-Inference" class="headerlink" title="Performing Inference"></a>Performing Inference</h2><p>引擎拥有优化的模型，但要执行推理，<strong>我们需要管理中间激活的额外状态</strong>。这是通过<code>ExecutionContext</code>接口完成的：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IExecutionContext *context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure></div>

<p>一个引擎可以有多个执行上下文，允许一组权重用于多个重叠的推理任务。 （当前的一个例外是<strong>使用动态形状时，每个优化配置文件只能有一个执行上下文。</strong>）</p>
<p><strong>要执行推理，您必须为输入和输出传递 TensorRT 缓冲区</strong>，TensorRT 要求您在指针数组中指定。您可以使用为输入和输出张量提供的名称查询引擎，以在数组中找到正确的位置：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int32_t</span> inputIndex = engine-&gt;getBindingIndex(INPUT_NAME);</span><br><span class="line"><span class="type">int32_t</span> outputIndex = engine-&gt;getBindingIndex(OUTPUT_NAME);</span><br></pre></td></tr></table></figure></div>

<p>使用这些索引，设置一个缓冲区数组，指向 GPU 上的输入和输出缓冲区：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span>* buffers[<span class="number">2</span>];</span><br><span class="line">buffers[inputIndex] = inputBuffer;</span><br><span class="line">buffers[outputIndex] = outputBuffer;</span><br></pre></td></tr></table></figure></div>

<p>然后，您可以调用 TensorRT 的 <strong>enqueue</strong> 方法以使用CUDA 流异步启动推理：</p>
<div class="highlight-container" data-rel="C"><figure class="iseeu highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context-&gt;enqueueV2(buffers, stream, nullptr);</span><br></pre></td></tr></table></figure></div>

<p>通常在内核之前和之后将<code>cudaMemcpyAsync()</code> 排入队列以从 GPU 中移动数据（如果数据尚不存在）。</p>
<p> <strong><code>enqueueV2()</code>的最后一个参数是一个可选的 CUDA 事件，当输入缓冲区被消耗时发出信号，并且可以安全地重用它们的内存。</strong></p>
<p>要确定内核（可能还有<code>memcpy()</code> ）何时完成，请使用标准 CUDA 同步机制，例如事件或等待流。</p>
<p><strong>如果您更喜欢同步推理，请使用<code>executeV2</code>方法而不是<code>enqueueV2 </code>。</strong></p>
<h1 id="TensorRT-的-Python-接口解析"><a href="#TensorRT-的-Python-接口解析" class="headerlink" title="TensorRT 的 Python 接口解析"></a>TensorRT 的 Python 接口解析</h1><p>Python API 可以通过tensorrt模块访问：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br></pre></td></tr></table></figure></div>

<h2 id="The-Build-Phase-1"><a href="#The-Build-Phase-1" class="headerlink" title="The Build Phase"></a>The Build Phase</h2><p>要创建构建器，您需要首先创建一个记录器。 Python 绑定包括一个简单的记录器实现，它将高于特定严重性的所有消息记录到<code>stdout</code> 。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logger = trt.Logger(trt.Logger.WARNING)</span><br></pre></td></tr></table></figure></div>

<p>或者，可以通过从<code>ILogger</code>类派生来定义您自己的记录器实现：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLogger</span>(trt.ILogger):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">       trt.ILogger.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self, severity, msg</span>):</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># Your custom logging implementation here</span></span><br><span class="line"></span><br><span class="line">logger = MyLogger()</span><br></pre></td></tr></table></figure></div>

<p>然后，您可以创建一个构建器：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">builder = trt.Builder(logger)</span><br></pre></td></tr></table></figure></div>

<h3 id="Creating-a-Network-Definition-in-Python"><a href="#Creating-a-Network-Definition-in-Python" class="headerlink" title="Creating a Network Definition in Python"></a>Creating a Network Definition in Python</h3><p>创建构建器后，优化模型的第一步是创建网络定义：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br></pre></td></tr></table></figure></div>

<p><strong>为了使用 ONNX 解析器导入模型，需要<code>EXPLICIT_BATCH</code>标志。</strong></p>
<h3 id="Importing-a-Model-using-the-ONNX-Parser-1"><a href="#Importing-a-Model-using-the-ONNX-Parser-1" class="headerlink" title="Importing a Model using the ONNX Parser"></a>Importing a Model using the ONNX Parser</h3><p>现在，需要从 ONNX 表示中填充网络定义。您可以创建一个 ONNX 解析器来填充网络，如下所示：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = trt.OnnxParser(network, logger)</span><br></pre></td></tr></table></figure></div>

<p>然后，读取模型文件并处理任何错误：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">success = parser.parse_from_file(model_path)</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(parser.num_errors):</span><br><span class="line">    <span class="built_in">print</span>(parser.get_error(idx))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> success:</span><br><span class="line">    <span class="keyword">pass</span> <span class="comment"># Error handling code here</span></span><br></pre></td></tr></table></figure></div>

<h3 id="Building-an-Engine-1"><a href="#Building-an-Engine-1" class="headerlink" title="Building an Engine"></a>Building an Engine</h3><p>下一步是创建一个构建配置，指定 TensorRT 应该如何优化模型：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config = builder.create_builder_config()</span><br></pre></td></tr></table></figure></div>

<p>这个接口有很多属性，你可以设置这些属性来控制 TensorRT 如何优化网络。一个重要的属性是最大工作空间大小。层实现通常需要一个临时工作空间，并且此参数限制了网络中任何层可以使用的最大大小。如果提供的工作空间不足，TensorRT 可能无法找到层的实现：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, <span class="number">1</span> &lt;&lt; <span class="number">20</span>) <span class="comment"># 1 MiB</span></span><br></pre></td></tr></table></figure></div>

<p>指定配置后，可以使用以下命令构建和序列化引擎：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">serialized_engine = builder.build_serialized_network(network, config)</span><br></pre></td></tr></table></figure></div>

<p>将引擎保存到文件以供将来使用可能很有用。你可以这样做：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(“sample.engine”, “wb”) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(serialized_engine)</span><br></pre></td></tr></table></figure></div>

<h2 id="Performing-Inference-1"><a href="#Performing-Inference-1" class="headerlink" title="Performing Inference"></a>Performing Inference</h2><p><strong>引擎拥有优化的模型，但要执行推理需要额外的中间激活状态。这是通过<code>IExecutionContext</code>接口完成的：</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context = engine.create_execution_context()</span><br></pre></td></tr></table></figure></div>

<p>一个引擎可以有多个执行上下文，允许一组权重用于多个重叠的推理任务。 （当前的一个例外是使用动态形状时，每个优化配置文件只能有一个执行上下文。）</p>
<p><strong>要执行推理，您必须为输入和输出传递 TensorRT 缓冲区</strong>，TensorRT 要求您在 GPU 指针列表中指定。您可以使用为输入和输出张量提供的名称查询引擎，以在数组中找到正确的位置：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_idx = engine[input_name]</span><br><span class="line">output_idx = engine[output_name]		<span class="comment">#通过引擎查找输出的索引</span></span><br></pre></td></tr></table></figure></div>

<p>使用这些索引，为每个输入和输出设置 GPU 缓冲区。多个 Python 包允许您在 GPU 上分配内存，包括但不限于 PyTorch、Polygraphy CUDA 包装器和 PyCUDA。</p>
<p>然后，创建一个 GPU 指针列表。例如，对于 PyTorch CUDA 张量，您可以使用<code>data_ptr()</code>方法访问 GPU 指针；对于 Polygraphy <code>DeviceArray</code> ，使用<code>ptr</code>属性：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">buffers = [<span class="literal">None</span>] * <span class="number">2</span> <span class="comment"># Assuming 1 input and 1 output</span></span><br><span class="line">buffers[input_idx] = input_ptr</span><br><span class="line">buffers[output_idx] = output_ptr</span><br></pre></td></tr></table></figure></div>

<p><strong>填充输入缓冲区后，您可以调用 TensorRT 的<code>execute_async</code>方法以使用 CUDA 流异步启动推理。</strong></p>
<p>首先，创建 CUDA 流。如果您已经有 CUDA 流，则可以使用指向现有流的指针。例如，对于 PyTorch CUDA 流，即<code>torch.cuda.Stream() </code>，您可以使用<code>cuda_stream</code>属性访问指针；对于 Polygraphy CUDA 流，使用<code>ptr</code>属性。 接下来，开始推理：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.execute_async_v2(buffers, stream_ptr)</span><br></pre></td></tr></table></figure></div>

<p>通常在内核之前和之后将异步<code>memcpy()</code>排入队列以从 GPU 中移动数据（如果数据尚不存在）。</p>
<p><strong>要确定内核（可能还有memcpy() ）何时完成，请使用标准 CUDA 同步机制</strong>，例如事件或等待流。例如，对于 Polygraphy，使用：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.synchronize()</span><br></pre></td></tr></table></figure></div>

<p>如果您更喜欢同步推理，请使用<code>execute_v2</code>方法而不是<code>execute_async_v2</code> 。</p>
<h1 id="TensorRT如何工作"><a href="#TensorRT如何工作" class="headerlink" title="TensorRT如何工作"></a>TensorRT如何工作</h1><h2 id="Object-Lifetimes"><a href="#Object-Lifetimes" class="headerlink" title="Object Lifetimes"></a>Object Lifetimes</h2><p>TensorRT 的 API 是基于类的，其中一些类充当其他类的工厂。<strong>对于用户拥有的对象，工厂对象的生命周期必须跨越它创建的对象的生命周期。</strong>例如， <code>NetworkDefinition</code>和<code>BuilderConfig</code>类是从构建器类创建的，这些类的对象应该在构建器工厂对象之前销毁。</p>
<p>此规则的一个重要例外是从构建器创建引擎。创建引擎后，您可以销毁构建器、网络、解析器和构建配置并继续使用引擎。（<strong>构建器可以提前删除，因为引擎中包含了网络的定义和权重等</strong>）</p>
<h2 id="Error-Handling-and-Logging"><a href="#Error-Handling-and-Logging" class="headerlink" title="Error Handling and Logging"></a>Error Handling and Logging</h2><p><strong>创建 TensorRT 顶级接口（<code>builder</code>、<code>runtime</code> 或 <code>refitter</code>）时，您必须提供<code>Logger</code> 接口的实现。</strong>记录器用于诊断和信息性消息；它的详细程度是可配置的。由于记录器可用于在 TensorRT 生命周期的任何时间点传回信息，因此它的生命周期必须跨越应用程序中对该接口的任何使用。实现也必须是线程安全的，因为 TensorRT 可以在内部使用工作线程。</p>
<p><strong>对对象的 API 调用将使用与相应顶级接口关联的记录器。</strong>例如，在对<code>ExecutionContext::enqueue()</code>的调用中，执行上下文是从引擎创建的，因此 TensorRT 将使用与引擎运行时关联的记录器。<code>（关联记录器）</code></p>
<p>错误处理的主要方法是<code>ErrorRecorde</code> 接口。<strong>您可以实现此接口，并将其附加到 API 对象以接收与该对象关联的错误。对象的记录器也将传递给它创建的任何其他记录器</strong> – 例如，如果您将错误记录器附加到引擎，并从该引擎创建执行上下文，它将使用相同的记录器。如果您随后将新的错误记录器附加到执行上下文，它将仅接收来自该上下文的错误。如果生成错误但没有找到错误记录器，它将通过关联的记录器发出。<code>（错误记录器）</code></p>
<p><strong>请注意，CUDA 错误通常是异步的 – 因此，当执行多个推理或其他 CUDA 流在单个 CUDA 上下文中异步工作时，可能会在与生成它的执行上下文不同的执行上下文中观察到异步 GPU 错误。</strong></p>
<h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><p>TensorRT 使用大量设备内存，即 GPU 可直接访问的内存，而不是连接到 CPU 的主机内存。由于设备内存通常是一种受限资源，因此了解 TensorRT 如何使用它很重要。</p>
<h3 id="The-Build-Phase-2"><a href="#The-Build-Phase-2" class="headerlink" title="The Build Phase"></a>The Build Phase</h3><p><strong>在构建期间，TensorRT 为时序层实现分配设备内存</strong>。一些实现可能会消耗大量临时内存，尤其是在使用大张量的情况下。您可以通过构建器的<code>maxWorkspace</code>属性控制最大临时内存量。这默认为设备全局内存的完整大小，但可以在必要时进行限制。如果构建器发现由于工作空间不足而无法运行的适用内核，它将发出一条日志消息来指示这一点。</p>
<p><strong>然而，即使工作空间相对较小，也需要为输入、输出和权重创建缓冲区。</strong> TensorRT 对操作系统因此类分配而返回内存不足是稳健的，但在某些平台上，操作系统可能会成功提供内存，随后内存不足killer进程观察到系统内存不足，并终止 TensorRT .如果发生这种情况，请在重试之前尽可能多地释放系统内存。</p>
<p>在构建阶段，通常在主机内存中至少有两个权重拷贝：<strong>来自原始网络的权重拷贝，以及在构建引擎时作为引擎一部分包含的权重拷贝。</strong>此外，当 TensorRT 组合权重（例如卷积与批量归一化）时，将创建<strong>额外的临时权重张量。</strong></p>
<h3 id="The-Runtime-Phase-1"><a href="#The-Runtime-Phase-1" class="headerlink" title="The Runtime Phase"></a>The Runtime Phase</h3><p>在运行时，TensorRT 使用相对较少的主机内存，但可以使用大量的设备内存。</p>
<p><strong>引擎在反序列化时分配设备内存来存储模型权重</strong>。由于序列化引擎几乎都是权重，因此它的大小非常接近权重所需的设备内存量。</p>
<p><code>ExecutionContext</code>使用两种设备内存：</p>
<ul>
<li>一些层实现所需的<strong>持久内存</strong>——例如，一些卷积实现使用边缘掩码，并且这种状态不能像权重那样在上下文之间共享，因为它的大小取决于层输入形状，这可能因上下文而异。该内存在创建执行上下文时分配，并在其生命周期内持续。</li>
<li><strong>暂存内存</strong>，用于在处理网络时保存中间结果。该内存用于中间激活张量。它还用于层实现所需的临时存储，其边界由<code>IBuilderConfig::setMaxWorkspaceSize()</code>控制。</li>
</ul>
<p>您可以选择通过<code>ICudaEngine::createExecutionContextWithoutDeviceMemory()</code><strong>创建一个没有暂存内存的执行上下文，并在网络执行期间自行提供该内存。</strong> <strong>这允许您在未同时运行的多个上下文之间共享它，或者在推理未运行时用于其他用途。</strong> <code>ICudaEngine::getDeviceMemorySize()</code>返回所需的暂存内存量。</p>
<p>构建器在构建网络时发出有关执行上下文使用的持久内存和暂存内存量的信息，严重性为 <code>kINFO</code> 。检查日志，消息类似于以下内容：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[08/12/2021-17:39:11] [I] [TRT] Total Host Persistent Memory: 106528</span><br><span class="line">[08/12/2021-17:39:11] [I] [TRT] Total Device Persistent Memory: 29785600</span><br><span class="line">[08/12/2021-17:39:11] [I] [TRT] Total Scratch Memory: 9970688</span><br></pre></td></tr></table></figure></div>

<p>默认情况下，TensorRT 直接从 CUDA 分配设备内存。但是，<strong>您可以将 TensorRT 的<code>IGpuAllocator</code>接口的实现附加到构建器或运行时，并自行管理设备内存。</strong>如果您的应用程序希望控制所有 GPU 内存并子分配给 TensorRT，而不是让 TensorRT 直接从 CUDA 分配，这将非常有用。</p>
<p>TensorRT 的依赖项（ <a class="link"   target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn" >cuDNN <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>和<a class="link"   target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas" >cuBLAS <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ）会占用大量设备内存。 <strong>TensorRT 允许您通过构建器配置中的<code>TacticSources</code>属性控制这些库是否用于推理。</strong>请注意，某些层实现需要这些库，因此当它们被排除时，网络可能无法编译。</p>
<p>CUDA 基础设施和 TensorRT 的设备代码也会消耗设备内存。内存量因平台、设备和 TensorRT 版本而异。您可以使用<code>cudaGetMemInfo</code>来确定正在使用的设备内存总量。</p>
<h2 id="Threading"><a href="#Threading" class="headerlink" title="Threading"></a>Threading</h2><p>一般来说，TensorRT 对象不是线程安全的。预期的运行时并发模型是不同的线程将在不同的执行上下文上操作。<strong>上下文包含执行期间的网络状态（激活值等），因此在不同线程中同时使用上下文会导致未定义的行为</strong>。 为了支持这个模型，以下操作是线程安全的：</p>
<ul>
<li>运行时或引擎上的非修改操作。</li>
<li>从 TensorRT 运行时反序列化引擎。</li>
<li>从引擎创建执行上下文。</li>
<li>注册和注销插件。</li>
</ul>
<p><strong>在不同线程中使用多个构建器没有线程安全问题；</strong>但是，构建器使用时序来确定所提供参数的最快内核，并且使用具有相同 GPU 的多个构建器将扰乱时序和 TensorRT 构建最佳引擎的能力。使用多线程使用不同的 GPU 构建不存在此类问题。</p>
<h2 id="Determinism"><a href="#Determinism" class="headerlink" title="Determinism"></a>Determinism</h2><p>TensorRT <code>builder</code> 使用时间来找到最快的内核来实现给定的运算符。时序内核会受到噪声的影响——GPU 上运行的其他工作、GPU 时钟速度的波动等<strong>。时序噪声意味着在构建器的连续运行中，可能不会选择相同的实现</strong>。</p>
<p><strong><code>AlgorithmSelector</code>接口允许您强制构建器为给定层选择特定实现</strong>。您可以使用它来确保构建器从运行到运行选择相同的内核。</p>
<p>一旦构建了引擎，它就是确定性的：在相同的运行时环境中提供相同的输入将产生相同的输出。</p>
<h1 id="TensorRT-进阶用法"><a href="#TensorRT-进阶用法" class="headerlink" title="TensorRT 进阶用法"></a>TensorRT 进阶用法</h1><h2 id="The-Timing-Cache"><a href="#The-Timing-Cache" class="headerlink" title="The Timing Cache"></a>The Timing Cache</h2><p><strong>为了减少构建器时间，TensorRT 创建了一个层时序缓存，以在构建器阶段保存层分析信息。</strong>它包含的信息特定于目标构建器设备、CUDA 和 TensorRT 版本，以及可以更改层实现的 <code>BuilderConfig</code> 参数，例如<code>BuilderFlag::kTF32或BuilderFlag::kREFIT</code> 。</p>
<p><strong>如果有其他层具有相同的输入&#x2F;输出张量配置和层参数，则 TensorRT 构建器会跳过分析并重用重复层的缓存结果。如果缓存中的计时查询未命中，则构建器会对该层计时并更新缓存。</strong></p>
<p>时序缓存可以被序列化和反序列化。您可以通过<code>IBuilderConfig::createTimingCache</code>从缓冲区加载序列化缓存：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ITimingCache* cache = </span><br><span class="line"> config-&gt;createTimingCache(cacheFile.data(), cacheFile.size());</span><br></pre></td></tr></table></figure></div>

<p>将缓冲区大小设置为0会创建一个新的空时序缓存。</p>
<p><strong>然后，在构建之前将缓存附加到构建器配置。</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;setTimingCache(*cache, false);</span><br></pre></td></tr></table></figure></div>

<p>在构建期间，由于缓存未命中，时序缓存可以增加更多信息。在构建之后，它可以被序列化以与另一个构建器一起使用。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IHostMemory* serializedCache = cache-&gt;serialize();</span><br></pre></td></tr></table></figure></div>

<p>如果构建器没有附加时间缓存，构建器会创建自己的临时本地缓存并在完成时将其销毁。</p>
<h2 id="Refitting-An-Engine"><a href="#Refitting-An-Engine" class="headerlink" title="Refitting An Engine"></a>Refitting An Engine</h2><p><strong>TensorRT 可以用新的权重改装引擎而无需重建它</strong>，但是，在构建时必须指定这样做的选项：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">config-&gt;setFlag(BuilderFlag::kREFIT) </span><br><span class="line">builder-&gt;buildSerializedNetwork(network, config);</span><br></pre></td></tr></table></figure></div>

<p><strong>稍后，您可以创建一个<code>Refitter</code>对象：</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ICudaEngine* engine = ...;</span><br><span class="line">IRefitter* refitter = createInferRefitter(*engine,gLogger)</span><br></pre></td></tr></table></figure></div>

<p><strong>然后更新权重。</strong>例如，要更新名为“<code>MyLayer</code>”的卷积层的内核权重：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Weights newWeights = ...;</span><br><span class="line">refitter-&gt;setWeights(<span class="string">&quot;MyLayer&quot;</span>,WeightsRole::kKERNEL,</span><br><span class="line">                    newWeights);</span><br></pre></td></tr></table></figure></div>

<p><strong>新的权重应该与用于构建引擎的原始权重具有相同的计数</strong>。如果出现问题，例如错误的层名称或角色或权重计数发生变化， <code>setWeights</code>返回 <code>false</code>。</p>
<p><strong>由于引擎优化的方式，如果您更改一些权重，您可能还必须提供一些其他权重。</strong>该界面可以告诉您需要提供哪些额外的权重。</p>
<p><strong>您可以使用<code>INetworkDefinition::setWeightsName()</code>在构建时命名权重 – ONNX 解析器使用此 API 将权重与 ONNX 模型中使用的名称相关联</strong>。然后，您可以使用<code>setNamedWeights</code>更新权重：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Weights newWeights = ...;</span><br><span class="line">refitter-&gt;setNamedWeights(<span class="string">&quot;MyWeights&quot;</span>, newWeights);</span><br></pre></td></tr></table></figure></div>

<p><code>setNamedWeights</code>和<code>setWeights</code>可以同时使用，即，您可以通过<code>setNamedWeights</code>更新具有名称的权重，并通过<code>setWeights</code>更新那些未命名的权重。</p>
<p><strong>这通常需要两次调用<code>IRefitter::getMissing</code></strong> ，首先获取必须提供的权重对象的数量，然后获取它们的层和角色。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const int32_t n = refitter-&gt;getMissing(<span class="number">0</span>, nullptr, nullptr);</span><br><span class="line">std::vector&lt;const char*&gt; layerNames(n);</span><br><span class="line">std::vector&lt;WeightsRole&gt; weightsRoles(n);</span><br><span class="line">refitter-&gt;getMissing(n, layerNames.data(), </span><br><span class="line">                        weightsRoles.data());</span><br></pre></td></tr></table></figure></div>

<p>或者，要获取所有缺失权重的名称，请运行：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">const int32_t n = refitter-&gt;getMissingWeights(<span class="number">0</span>, nullptr);</span><br><span class="line">std::vector&lt;const char*&gt; weightsNames(n);</span><br><span class="line">refitter-&gt;getMissingWeights(n, weightsNames.data());</span><br></pre></td></tr></table></figure></div>

<p>您可以按任何顺序提供缺失的权重：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (int32_t i = <span class="number">0</span>; i &lt; n; ++i)</span><br><span class="line">    refitter-&gt;setWeights(layerNames[i], weightsRoles[i],</span><br><span class="line">                         Weights&#123;...&#125;);</span><br></pre></td></tr></table></figure></div>

<p><strong>返回的缺失权重集是完整的，从某种意义上说，仅提供缺失的权重不会产生对任何更多权重的需求。</strong></p>
<p>提供所有权重后，您可以更新引擎：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">bool</span> success = refitter-&gt;refitCudaEngine();</span><br><span class="line"><span class="keyword">assert</span>(success);</span><br></pre></td></tr></table></figure></div>

<p>如果 <code>refit</code> 返回 <code>false</code>，请检查日志以获取诊断信息，可能是关于仍然丢失的权重。 然后，您可以删除<code>refitter</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete refitter;</span><br></pre></td></tr></table></figure></div>

<p>更新后的引擎的行为就像它是从使用新权重更新的网络构建的一样。</p>
<p>要查看引擎中的所有可改装权重，请使用<code>refitter-&gt;getAll(...)</code>或<code>refitter-&gt;getAllWeights(...)</code> ；类似于上面使用<code>getMissing</code>和<code>getMissingWeights</code>的方式。</p>
<h2 id="Algorithm-Selection-and-Reproducible-Builds"><a href="#Algorithm-Selection-and-Reproducible-Builds" class="headerlink" title="Algorithm Selection and Reproducible Builds"></a>Algorithm Selection and Reproducible Builds</h2><p><strong>TensorRT 优化器的默认行为是选择全局最小化引擎执行时间的算法。</strong>它通过定时每个实现来做到这一点，有时，当实现具有相似的时间时，<strong>系统噪声</strong>可能会决定在构建器的任何特定运行中将选择哪个。不同的实现通常会使用不同的浮点值累加顺序，两种实现可能使用不同的算法，甚至以不同的精度运行。因此，构建器的不同调用通常不会导致引擎返回位相同的结果。</p>
<p>有时，确定性构建或重新创建早期构建的算法选择很重要。<strong>通过提供<code>IAlgorithmSelector</code>接口的实现并使用<code>setAlgorithmSelector</code>将其附加到构建器配置</strong>，您可以手动指导算法选择。</p>
<p>方法<code>IAlgorithmSelector::selectAlgorithms</code>接收一个<code>AlgorithmContext</code> ，其中包含有关层算法要求的信息，以及一组满足这些要求的算法选择。<strong>它返回 TensorRT 应该为层考虑的算法集。</strong></p>
<p>构建器将从这些算法中选择一种可以最小化网络全局运行时间的算法。如果未返回任何选项并且**<code>BuilderFlag::kREJECT_EMPTY_ALGORITHMS</code>未设置，则 TensorRT 将其解释为意味着任何算法都可以用于该层**。要覆盖此行为并在返回空列表时生成错误，请设置<code>BuilderFlag::kREJECT_EMPTY_ALGORITHMSS</code>标志。</p>
<p>在 TensorRT 完成对给定配置文件的网络优化后，它会调用<code>reportAlgorithms</code> ，它可用于记录为每一层做出的最终选择。</p>
<p><code>selectAlgorithms</code>返回一个选项。要重播早期构建中的选择，请使用<code>reportAlgorithms</code>记录该构建中的选择，并在<code>selectAlgorithms</code>中返回它们。</p>
<p><code>sampleAlgorithmSelector</code>演示了如何使用算法选择器在构建器中实现确定性和可重复性。</p>
<p>注意：</p>
<ul>
<li>算法选择中的“层”概念与<code>INetworkDefinition</code>中的<code>ILayer</code>不同。由于融合优化<strong>，前者中的“层”可以等同于多个网络层的集合。</strong></li>
<li><code>selectAlgorithms</code>中选择最快的算法可能不会为整个网络产生最佳性能，因为它可能会增加重新格式化的开销。</li>
<li>如果 TensorRT 发现该层是空操作，则 <code>IAlgorithm</code>的时间在<code>selectAlgorithms</code>中为0 。</li>
<li><code>reportAlgorithms</code>不提供提供给<code>selectAlgorithms</code>的<code>IAlgorithm</code>的时间和工作空间信息。</li>
</ul>
<h2 id="Creating-A-Network-Definition-From-Scratch"><a href="#Creating-A-Network-Definition-From-Scratch" class="headerlink" title="Creating A Network Definition From Scratch"></a>Creating A Network Definition From Scratch</h2><p>除了使用解析器，您还可以通过网络定义 API 将网络直接定义到 TensorRT。<strong>此场景假设每层权重已在主机内存中准备好在网络创建期间传递给 TensorRT。</strong></p>
<p>以下示例创建了一个简单的网络，其中包含 <code>Input</code>、<code>Convolution</code>、<code>Pooling</code>、 <code>MatrixMultiply</code>、<code>Shuffle</code> 、<code>Activation</code> 和 <code>Softmax</code> 层。</p>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>此部分对应的代码可以在<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples/python/network_api_pytorch_mnist" >network_api_pytorch_mnist <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>中找到。</p>
<p><strong>这个例子使用一个帮助类来保存一些关于模型的元数据：</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ModelData</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    INPUT_NAME = <span class="string">&quot;data&quot;</span></span><br><span class="line">    INPUT_SHAPE = (<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    OUTPUT_NAME = <span class="string">&quot;prob&quot;</span></span><br><span class="line">    OUTPUT_SIZE = <span class="number">10</span></span><br><span class="line">    DTYPE = trt.float32</span><br></pre></td></tr></table></figure></div>

<p>在此示例中，权重是从 Pytorch MNIST 模型导入的。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = mnist_model.get_weights()</span><br></pre></td></tr></table></figure></div>

<p><strong>创建记录器、构建器和网络类。</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TRT_LOGGER = trt.Logger(trt.Logger.ERROR)</span><br><span class="line">builder = trt.Builder(TRT_LOGGER)</span><br><span class="line">EXPLICIT_BATCH = <span class="number">1</span> &lt;&lt; (<span class="built_in">int</span>)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)</span><br><span class="line">network = builder.create_network(common.EXPLICIT_BATCH)</span><br></pre></td></tr></table></figure></div>

<p><code>kEXPLICIT_BATCH</code>标志的更多信息，请参阅<a class="link"   target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#explicit-implicit-batch" >显式与隐式批处理 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>部分。</p>
<p><strong>接下来，为网络创建输入张量，指定张量的名称、数据类型和形状。</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_tensor = network.add_input(name=ModelData.INPUT_NAME, dtype=ModelData.DTYPE, shape=ModelData.INPUT_SHAPE)</span><br></pre></td></tr></table></figure></div>

<p>添加一个卷积层，指定输入、输出图的数量、内核形状、权重、偏差和步幅：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv1_w = weights[<span class="string">&#x27;conv1.weight&#x27;</span>].numpy()</span><br><span class="line">    conv1_b = weights[<span class="string">&#x27;conv1.bias&#x27;</span>].numpy()</span><br><span class="line">    conv1 = network.add_convolution(<span class="built_in">input</span>=input_tensor, num_output_maps=<span class="number">20</span>, kernel_shape=(<span class="number">5</span>, <span class="number">5</span>), kernel=conv1_w, bias=conv1_b)</span><br><span class="line">    conv1.stride = (<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></div>

<p>添加一个池化层，指定输入（前一个卷积层的输出）、池化类型、窗口大小和步幅：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool1 = network.add_pooling(<span class="built_in">input</span>=conv1.get_output(<span class="number">0</span>), <span class="built_in">type</span>=trt.PoolingType.MAX, window_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    pool1.stride = (<span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div>

<p>添加下一对卷积和池化层：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conv2_w = weights[<span class="string">&#x27;conv2.weight&#x27;</span>].numpy()</span><br><span class="line">conv2_b = weights[<span class="string">&#x27;conv2.bias&#x27;</span>].numpy()</span><br><span class="line">conv2 = network.add_convolution(pool1.get_output(<span class="number">0</span>), <span class="number">50</span>, (<span class="number">5</span>, <span class="number">5</span>), conv2_w, conv2_b)</span><br><span class="line">conv2.stride = (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">pool2 = network.add_pooling(conv2.get_output(<span class="number">0</span>), trt.PoolingType.MAX, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">pool2.stride = (<span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div>

<p>添加一个 Shuffle 层来重塑输入，为矩阵乘法做准备：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch = <span class="built_in">input</span>.shape[<span class="number">0</span>]</span><br><span class="line">mm_inputs = np.prod(<span class="built_in">input</span>.shape[<span class="number">1</span>:])			<span class="comment">#计算所有元素的乘积</span></span><br><span class="line">input_reshape = net.add_shuffle(<span class="built_in">input</span>)</span><br><span class="line">input_reshape.reshape_dims = trt.Dims2(batch, mm_inputs)</span><br></pre></td></tr></table></figure></div>

<p>现在，添加一个 <code>MatrixMultiply</code> 层。在这里，模型导出器提供了转置权重，因此为这些权重指定了<code>kTRANSPOSE</code>选项。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter_const = net.add_constant(trt.Dims2(nbOutputs, k), weights[<span class="string">&quot;fc1.weight&quot;</span>].numpy())</span><br><span class="line">mm = net.add_matrix_multiply(input_reshape.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE, filter_const.get_output(<span class="number">0</span>), trt.MatrixOperation.TRANSPOSE);</span><br></pre></td></tr></table></figure></div>

<p>添加将在批次维度广播的偏差添加：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bias_const = net.add_constant(trt.Dims2(<span class="number">1</span>, nbOutputs), weights[<span class="string">&quot;fc1.bias&quot;</span>].numpy())</span><br><span class="line">bias_add = net.add_elementwise(mm.get_output(<span class="number">0</span>), bias_const.get_output(<span class="number">0</span>), trt.ElementWiseOperation.SUM)</span><br></pre></td></tr></table></figure></div>

<p>添加 Relu 激活层：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">relu1 = network.add_activation(<span class="built_in">input</span>=fc1.get_output(<span class="number">0</span>), <span class="built_in">type</span>=trt.ActivationType.RELU)</span><br></pre></td></tr></table></figure></div>

<p>添加最后的全连接层，并将该层的输出标记为整个网络的输出：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fc2_w = weights[<span class="string">&#x27;fc2.weight&#x27;</span>].numpy()</span><br><span class="line">fc2_b = weights[<span class="string">&#x27;fc2.bias&#x27;</span>].numpy()</span><br><span class="line">fc2 = network.add_fully_connected(relu1.get_output(<span class="number">0</span>), ModelData.OUTPUT_SIZE, fc2_w, fc2_b)</span><br><span class="line"></span><br><span class="line">fc2.get_output(<span class="number">0</span>).name = ModelData.OUTPUT_NAME</span><br><span class="line">network.mark_output(tensor=fc2.get_output(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></div>

<p>代表 MNIST 模型的网络现已完全构建。</p>
<h2 id="Reduced-Precision"><a href="#Reduced-Precision" class="headerlink" title="Reduced Precision"></a>Reduced Precision</h2><h3 id="Network-level-Control-of-Precision"><a href="#Network-level-Control-of-Precision" class="headerlink" title="Network-level Control of Precision"></a>Network-level Control of Precision</h3><p>默认情况下，TensorRT 以 32 位精度工作，但也可以使用 16 位浮点和 8 位量化浮点执行操作。<strong>使用较低的精度需要更少的内存并实现更快的计算。</strong></p>
<p>降低精度支持取决于您的硬件。您可以查询构建器以检查平台上支持的精度支持：<strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> builder.platform_has_fp16:</span><br></pre></td></tr></table></figure></div>

<p><strong>在构建器配置中设置标志会通知 TensorRT 它可能会选择较低精度的实现</strong>：<strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.set_flag(trt.BuilderFlag.FP16)</span><br></pre></td></tr></table></figure></div>

<p>共有三个精度标志：<code>FP16</code>、<code>INT8</code> 和 <code>TF32</code>，它们可以独立启用。<strong>请注意，如果 TensorRT 导致整体运行时间较短，或者不存在低精度实现，TensorRT 仍将选择更高精度的内核。</strong></p>
<p>当 TensorRT 为层选择精度时，它会根据需要自动转换权重以运行层。虽然使用 <code>FP16</code> 和 <code>TF32</code> 精度相对简单，但使用 <code>INT8</code> 时会有额外的复杂性。</p>
<h3 id="Layer-level-Control-of-Precision"><a href="#Layer-level-Control-of-Precision" class="headerlink" title="Layer-level Control of Precision"></a>Layer-level Control of Precision</h3><p><strong><code>builder-flags</code> 提供了允许的、粗粒度的控制。</strong>然而，有时网络的一部分需要更高的动态范围或对数值精度敏感。您可以限制每层的输入和输出类型： <strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer.precision = trt.fp16</span><br></pre></td></tr></table></figure></div>

<p>这为输入和输出提供了首选类型（此处为D<code>ataType::kFP16</code> ）。</p>
<p><strong>您可以进一步设置图层输出的首选类型：Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer.set_output_type(out_tensor_index, trt.fp16)</span><br></pre></td></tr></table></figure></div>

<p>计算将使用与输入首选相同的浮点类型。大多数 TensorRT 实现具有相同的输入和输出浮点类型；<strong>但是，<code>Convolution</code>、<code>Deconvolution</code> 和 <code>FullyConnected</code> 可以支持量化的 <code>INT8</code> 输入和未量化的 <code>FP16</code> 或 <code>FP32</code> 输出，因为有时需要使用来自量化输入的更高精度输出来保持准确性。</strong></p>
<p>设置精度约束向 TensorRT 提示它应该选择一个输入和输出与首选类型匹配的层实现，如果前一层的输出和下一层的输入与请求的类型不匹配，则插入<strong>重新格式化操作</strong>。请注意，只有通过构建器配置中的标志启用了这些类型，TensorRT 才能选择具有这些类型的实现。</p>
<p>默认情况下，TensorRT 只有在产生更高性能的网络时才会重新格式化操作。如果另一个实现更快，TensorRT 会使用它并发出警告。您可以通过首选构建器配置中的类型约束来覆盖此行为。</p>
<p><strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)</span><br></pre></td></tr></table></figure></div>

<p>如果约束是首选的，TensorRT 会服从它们，除非没有具有首选精度约束的实现，在这种情况下，它会发出警告并使用最快的可用实现。</p>
<p>要将警告更改为错误，请使用<code>OBEY</code>而不是<code>PREFER</code> ：<strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS);</span><br></pre></td></tr></table></figure></div>

<p><strong>精度约束是可选的</strong> – 您可以查询以确定是否已使用Python 中的<code>layer.precision_is_set</code>设置了约束。如果没有设置精度约束，那么在 Python 中读取精度属性，是没有意义的。输出类型约束同样是可选的。</p>
<p><code>layer-&gt;getOutput(i)-&gt;setType()</code>和<code>layer-&gt;setOutputType()</code>之间存在区别——<strong>前者是一种可选类型，它限制了 TensorRT 将为层选择的实现。后者是强制性的（默认为 FP32）并指定网络输出的类型。</strong>如果它们不同，TensorRT 将插入一个强制转换以确保两个规范都得到尊重。因此，如果您为产生网络输出的层调用<code>setOutputType()</code> ，通常还应该将相应的网络输出配置为具有相同的类型。</p>
<h3 id="Enabling-TF32-Inference-Using-C"><a href="#Enabling-TF32-Inference-Using-C" class="headerlink" title="Enabling TF32 Inference Using C++"></a>Enabling TF32 Inference Using C++</h3><p><strong>TensorRT 默认允许使用 TF32 Tensor Cores。</strong>在计算内积时，例如在卷积或矩阵乘法期间，TF32 执行执行以下操作：</p>
<ul>
<li>将 FP32 被乘数舍入到 FP16 精度，但保持 FP32 动态范围。</li>
<li>计算四舍五入的被乘数的精确乘积。</li>
<li>将乘积累加到 FP32 总和中。</li>
</ul>
<p>TF32 Tensor Cores 可以使用 FP32 加速网络，通常不会损失准确性。对于需要高动态范围的权重或激活的模型，它比 FP16 更强大。</p>
<p>不能保证 TF32 Tensor Cores 会被实际使用，也没有办法强制实现使用它们 – TensorRT 可以随时回退到 FP32，如果平台不支持 TF32，则总是回退。但是，您可以通过清除 TF32 builder 标志来禁用它们。</p>
<p><strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.clear_flag(trt.BuilderFlag.TF32)</span><br></pre></td></tr></table></figure></div>

<p>尽管设置了 <code>BuilderFlag::kTF32</code> ，<strong>但在构建引擎时设置环境变量<code>NVIDIA_TF32_OVERRIDE=0</code>会禁用 <code>TF32</code> 。此环境变量在设置为0时会覆盖 NVIDIA 库的任何默认值或编程配置</strong>，因此它们永远不会使用 <strong>TF32 Tensor Cores</strong> 加速 FP32 计算。这只是一个调试工具，NVIDIA 库之外的任何代码都不应更改基于此环境变量的行为。除0以外的任何其他设置都保留供将来使用。</p>
<p>警告：在引擎运行时将环境变量<code>NVIDIA_TF32_OVERRIDE</code>设置为不同的值可能会导致无法预测的精度&#x2F;性能影响。引擎运转时最好不要设置。除非您的应用程序需要 TF32 提供的更高动态范围，否则 <strong><code>FP16</code> 将是更好的解决方案，因为它几乎总能产生更快的性能。</strong></p>
<h2 id="I-x2F-O-Formats"><a href="#I-x2F-O-Formats" class="headerlink" title="I&#x2F;O Formats"></a>I&#x2F;O Formats</h2><p>TensorRT 使用许多不同的数据格式优化网络。为了允许在 TensorRT 和客户端应用程序之间有效传递数据，<strong>这些底层数据格式在网络 I&#x2F;O 边界处公开，即用于标记为网络输入或输出的张量，以及在将数据传入和传出插件时</strong>。对于其他张量，TensorRT 选择导致最快整体执行的格式，并可能插入重新格式化以提高性能。</p>
<p>您可以通过分析可用的 I&#x2F;O 格式以及对 TensorRT 之前和之后的操作最有效的格式来组装最佳数据管道。</p>
<p>要指定 I&#x2F;O 格式，请以位掩码的形式指定一种或多种格式。 <strong>以下示例将输入张量格式设置为<code>TensorFormat::kHWC8</code> 。请注意，此格式仅适用于<code>DataType::kHALF</code> ，因此必须相应地设置数据类型。</strong></p>
<p><strong>Python</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">formats = <span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(tensorrt.TensorFormat.HWC8)</span><br><span class="line">network.get_input(<span class="number">0</span>).allowed_formats = formats</span><br><span class="line">network.get_input(<span class="number">0</span>).dtype = tensorrt.DataType.HALF</span><br></pre></td></tr></table></figure></div>

<p>通过设置构建器配置标志<code>DIRECT_IO</code> ，可以使 TensorRT 避免在网络边界插入重新格式化。这个标志通常会适得其反，原因有两个：</p>
<ul>
<li>如果允许 TensorRT 插入重新格式化，则生成的引擎可能会更慢。重新格式化可能听起来像是浪费工作，但它可以允许最有效的内核耦合。</li>
<li>如果 TensorRT 在不引入此类重新格式化的情况下无法构建引擎，则构建将失败。<strong>故障可能仅发生在某些目标平台上，</strong>因为这些平台的内核支持哪些格式。</li>
</ul>
<p><strong>该标志的存在是为了希望完全控制重新格式化是否发生在 I&#x2F;O 边界的用户，例如构建仅在 DLA 上运行而不回退到 GPU 进行重新格式化的引擎。</strong></p>
<p>请注意，对于矢量化格式，通道维度必须补零到矢量大小的倍数。例如，如果输入绑定的维度为[16,3,224,224] 、 <code>kHALF</code>数据类型和<code>kHWC8</code>格式，则绑定缓冲区的实际所需大小将为1<code>6* 8 *224*224*sizeof(half)</code>字节，甚至尽管<code>engine-&gt;getBindingDimension()</code> API 将张量维度返回为<code>[16,3,224,224]</code> 。填充部分中的值（即本例中的C&#x3D;3,4,…,7 ）必须用零填充。</p>
<h2 id="Compatibility-of-Serialized-Engines"><a href="#Compatibility-of-Serialized-Engines" class="headerlink" title="Compatibility of Serialized Engines"></a>Compatibility of Serialized Engines</h2><p><strong>仅当与用于序列化引擎的相同操作系统、CPU 架构、GPU 模型和 TensorRT 版本一起使用时，序列化引擎才能保证正常工作</strong>。</p>
<p>TensorRT 检查引擎的以下属性，如果它们与引擎被序列化的环境不匹配，将无法反序列化：</p>
<ul>
<li>TensorRT 的主要、次要、补丁和构建版本</li>
<li>计算能力（主要和次要版本）</li>
</ul>
<p>这确保了在构建阶段选择的内核存在并且可以运行。此外，TensorRT 用于从 cuDNN 和 cuBLAS 中选择和配置内核的 API 不支持跨设备兼容性，因此在构建器配置中禁用这些策略源的使用。 TensorRT 还会检查以下属性，如果它们不匹配，则会发出警告：</p>
<ul>
<li>全局内存总线带宽</li>
<li>二级缓存大小</li>
<li>每个块和每个多处理器的最大共享内存</li>
<li><strong>纹理对齐要求</strong></li>
<li>多处理器数量</li>
<li><strong>GPU 设备是集成的还是分立的</strong></li>
</ul>
<p>如果引擎序列化和运行时系统之间的 GPU 时钟速度不同，则从序列化系统中选择的策略对于运行时系统可能不是最佳的，并且可能会导致一些性能下降。</p>
<p>如果反序列化过程中可用的设备内存小于序列化过程中的数量，反序列化可能会由于内存分配失败而失败。</p>
<p><strong>在大型设备上构建小型模型时，TensorRT 可能会选择效率较低但可在可用资源上更好地扩展的内核。</strong>因此，如果优化单个TensorRT 引擎以在同一架构中的多个设备上使用，最好的方法是在最小的设备上运行构建器。或者，您可以在计算资源有限的大型设备上构建引擎</p>
<h2 id="Explicit-vs-Implicit-Batch"><a href="#Explicit-vs-Implicit-Batch" class="headerlink" title="Explicit vs Implicit Batch"></a>Explicit vs Implicit Batch</h2><p>TensorRT 支持两种指定网络的模式：显式批处理和隐式批处理。</p>
<p>在隐式批处理模式下，每个张量都有一个隐式批处理维度，所有其他维度必须具有恒定长度。此模式由 TensoRT 的早期版本使用，现在已弃用，但继续支持以实现向后兼容性。 在显式批处理模式下，所有维度都是显式的并且可以是动态的，即它们的长度可以在执行时改变。许多新功能（例如动态形状和循环）仅在此模式下可用。 ONNX 解析器也需要它。</p>
<p>例如，考虑一个处理 NCHW 格式的具有 3 个通道的大小为 HxW 的 N 个图像的网络。在运行时，输入张量的维度为 [N,3,H,W]。这两种模式在<code>INetworkDefinition</code>指定张量维度的方式上有所不同：</p>
<ul>
<li>在显式批处理模式下，网络指定 [N,3,H,W]。</li>
<li>在隐式批处理模式下，网络仅指定 [3,H,W]。<strong>批次维度 N 是隐式的。</strong></li>
</ul>
<p>“<strong>跨批次对话</strong>”的操作无法在隐式批次模式下表达，因为无法在网络中指定批次维度。隐式批处理模式下无法表达的操作示例：</p>
<ul>
<li>减少整个批次维度</li>
<li>重塑批次维度</li>
<li>用另一个维度转置批次维度</li>
</ul>
<p>例外是张量可以在整个批次中广播，通过方法<code>ITensor::setBroadcastAcrossBatch</code>用于网络输入，并通过隐式广播用于其他张量。</p>
<p>显式批处理模式消除了限制 – 批处理轴是轴 0。显式批处理的更准确术语是“<code>batch oblivious</code>”，因为在这种模式下，TensorRT 对引导轴没有特殊的语义含义，除非特定操作需要. 实际上，在显式批处理模式下，甚至可能没有批处理维度（例如仅处理单个图像的网络），或者可能存在多个长度不相关的批处理维度（例如比较从两个批处理中提取的所有可能对）。</p>
<p><strong><code>INetworkDefinition</code> 时，必须通过标志指定显式与隐式批处理的选择。</strong>这是显式批处理模式的 Python 代码：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">builder = trt.Builder(...)</span><br><span class="line">builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)</span><br></pre></td></tr></table></figure></div>

<p>对于隐式批处理，省略参数或传递 0。</p>
<h2 id="Sparsity"><a href="#Sparsity" class="headerlink" title="Sparsity"></a>Sparsity</h2><p>NVIDIA 安培架构 GPU 支持结构化稀疏性。<strong>在保证精度推理的同时，降低深度学习模型中的部分权重，减小模型所需要的带宽和内存，在提升效率的同时使开发者能够通过减少计算操作来加速神经网络。</strong>为了利用该特性获得更高的推理性能，卷积核权重和全连接权重必须满足以下要求：</p>
<p>对于每个输出通道和内核权重中的每个空间像素，每 4 个输入通道必须至少有 2 个零。换句话说，假设内核权重的形状为[K, C, R, S]和C % 4 &#x3D;&#x3D; 0 ，那么要求是：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> K:</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> R:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">            <span class="keyword">for</span> c_packed <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, C // <span class="number">4</span>):</span><br><span class="line">                num_zeros(weights[k, c_packed*<span class="number">4</span>:(c_packed+<span class="number">1</span>)*<span class="number">4</span>, r, s]) &gt;= <span class="number">2</span></span><br></pre></td></tr></table></figure></div>

<p>要启用稀疏特性，<strong>请在构建器配置中设置<code>kSPARSE_WEIGHTS</code>标志，并确保启用 <code>kFP16</code> 或 <code>kINT8</code> 模式</strong>。例如</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.set_flag(trt.BuilderFlag.SPARSE_WEIGHTS)</span><br></pre></td></tr></table></figure></div>

<p>在构建 TensorRT 引擎时，在 TensorRT 日志的末尾，TensorRT 会报告哪些层包含满足结构稀疏性要求的权重，以及 TensorRT 在哪些层中选择了利用结构化稀疏性的策略。<strong>在某些情况下，具有结构化稀疏性的策略可能比正常策略慢，TensorRT 在这些情况下会选择正常策略。</strong>以下输出显示了一个显示有关稀疏性信息的 TensorRT 日志示例：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[03/23/2021-00:14:05] [I] [TRT] (Sparsity) Layers eligible for sparse math: conv1, conv2, conv3</span><br><span class="line">[03/23/2021-00:14:05] [I] [TRT] (Sparsity) TRT inference plan picked sparse implementation for layers: conv2, conv3</span><br></pre></td></tr></table></figure></div>

<p>强制内核权重具有结构化的稀疏模式可能会导致准确性损失。要通过进一步微调恢复丢失的准确性</p>
<p><strong>要使用<code>trtexec</code>测量结构化稀疏性的推理性能</strong>，请参阅<a class="link"   target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec" >trtexec <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>部分。</p>
<h2 id="Empty-Tensors"><a href="#Empty-Tensors" class="headerlink" title="Empty Tensors"></a>Empty Tensors</h2><p>TensorRT 支持空张量。如果张量具有一个或多个长度为零的维度，则它是一个空张量。<strong>零长度尺寸通常不会得到特殊处理。如果一条规则适用于长度为 L 的任意正值 L 的维度，它通常也适用于 L&#x3D;0。</strong></p>
<p>例如，当沿最后一个轴连接两个维度为 <code>[x,y,z] </code>和 <code>[x,y,w]</code> 的张量时，结果的维度为 <code>[x,y,z+w]</code>，无论 <code>x,y, z，</code>或者 <code>w</code> 为零。</p>
<p><strong>隐式广播规则保持不变，因为只有单位长度维度对广播是特殊的。</strong>例如，给定两个维度为 <code>[1,y,z]</code> 和 <code>[x,1,z]</code> 的张量，它们由<code>IElementWiseLayer</code>计算的总和具有维度<code> [x,y,z]</code>，无论 <code>x、y 或 z</code> 是否为零.</p>
<p>如果一个引擎绑定是一个空的张量，它仍然需要一个非空的内存地址，并且不同的张量应该有不同的地址。这与C++中每个对象都有唯一地址的规则是一致的，例如<code>new float[0]</code>返回一个非空指针。如果使用可能返回零字节空指针的内存分配器，请改为请求至少一个字节。</p>
<h2 id="Reusing-Input-Buffers"><a href="#Reusing-Input-Buffers" class="headerlink" title="Reusing Input Buffers"></a>Reusing Input Buffers</h2><p>TensorRT 还包括一个可选的 CUDA 事件作为<code>enqueue</code>方法的参数，<strong>一旦输入缓冲区可以重用，就会发出信号</strong>。这允许应用程序在完成当前推理的同时立即开始重新填充输入缓冲区以进行下一次推理。例如：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.execute_async_v2(buffers, stream_ptr, inputReady)</span><br></pre></td></tr></table></figure></div>

<h2 id="Engine-Inspector"><a href="#Engine-Inspector" class="headerlink" title="Engine Inspector"></a>Engine Inspector</h2><p><strong>TensorRT 提供<code>IEngineInspector</code> API 来检查 TensorRT 引擎内部的信息。</strong>从反序列化的引擎中调用<code>createEngineInspector()</code>创建引擎<code>inspector</code>，然后调用<code>getLayerInformation()</code>或<code>getEngineInformation() inspector</code> API分别获取引擎中特定层或整个引擎的信息。可以打印出给定引擎的第一层信息，以及引擎的整体信息，如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inspector = engine.create_engine_inspector();</span><br><span class="line">inspector.execution_context = context; <span class="comment"># OPTIONAL</span></span><br><span class="line"><span class="built_in">print</span>(inspector.get_layer_information(<span class="number">0</span>, LayerInformationFormat.JSON); <span class="comment"># Print the information of the first layer in the engine.</span></span><br><span class="line"><span class="built_in">print</span>(inspector.get_engine_information(LayerInformationFormat.JSON); <span class="comment"># Print the information of the entire engine.</span></span><br></pre></td></tr></table></figure></div>

<p><strong>请注意，引擎&#x2F;层信息中的详细程度取决于构建引擎时的<code>ProfilingVerbosity</code>构建器配置设置</strong>。默认情况下， <code>ProfilingVerbosity</code>设置为<code>kLAYER_NAMES_ONLY</code> ，因此只会打印层名称。如果<code>ProfilingVerbosity</code>设置为<code>kNONE</code> ，则不会打印任何信息；如果设置为<code>kDETAILED</code> ，则会打印详细信息。</p>
<p><code>getLayerInformation()</code> API 根据<code>ProfilingVerbosity</code>设置打印的层信息的一些示例：</p>
<p><strong>kLAYER_NAMES_ONLY</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node_of_gpu_0/res4_0_branch2a_1 + node_of_gpu_0/res4_0_branch2a_bn_1 + node_of_gpu_0/res4_0_branch2a_bn_2</span><br></pre></td></tr></table></figure></div>

<p><strong>kDETAILED</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;Name&quot;: &quot;node_of_gpu_0/res4_0_branch2a_1 + node_of_gpu_0/res4_0_branch2a_bn_1 + node_of_gpu_0/res4_0_branch2a_bn_2&quot;,</span><br><span class="line">  &quot;LayerType&quot;: &quot;CaskConvolution&quot;,</span><br><span class="line">  &quot;Inputs&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;Name&quot;: &quot;gpu_0/res3_3_branch2c_bn_3&quot;,</span><br><span class="line">    &quot;Dimensions&quot;: [16,512,28,28],</span><br><span class="line">    &quot;Format/Datatype&quot;: &quot;Thirty-two wide channel vectorized row major Int8 format.&quot;</span><br><span class="line">  &#125;],</span><br><span class="line">  &quot;Outputs&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;Name&quot;: &quot;gpu_0/res4_0_branch2a_bn_2&quot;,</span><br><span class="line">    &quot;Dimensions&quot;: [16,256,28,28],</span><br><span class="line">    &quot;Format/Datatype&quot;: &quot;Thirty-two wide channel vectorized row major Int8 format.&quot;</span><br><span class="line">  &#125;],</span><br><span class="line">  &quot;ParameterType&quot;: &quot;Convolution&quot;,</span><br><span class="line">  &quot;Kernel&quot;: [1,1],</span><br><span class="line">  &quot;PaddingMode&quot;: &quot;kEXPLICIT_ROUND_DOWN&quot;,</span><br><span class="line">  &quot;PrePadding&quot;: [0,0],</span><br><span class="line">  &quot;PostPadding&quot;: [0,0],</span><br><span class="line">  &quot;Stride&quot;: [1,1],</span><br><span class="line">  &quot;Dilation&quot;: [1,1],</span><br><span class="line">  &quot;OutMaps&quot;: 256,</span><br><span class="line">  &quot;Groups&quot;: 1,</span><br><span class="line">  &quot;Weights&quot;: &#123;&quot;Type&quot;: &quot;Int8&quot;, &quot;Count&quot;: 131072&#125;,</span><br><span class="line">  &quot;Bias&quot;: &#123;&quot;Type&quot;: &quot;Float&quot;, &quot;Count&quot;: 256&#125;,</span><br><span class="line">  &quot;AllowSparse&quot;: 0,</span><br><span class="line">  &quot;Activation&quot;: &quot;RELU&quot;,</span><br><span class="line">  &quot;HasBias&quot;: 1,</span><br><span class="line">  &quot;HasReLU&quot;: 1,</span><br><span class="line">  &quot;TacticName&quot;: &quot;sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_epifadd&quot;,</span><br><span class="line">  &quot;TacticValue&quot;: &quot;0x11bde0e1d9f2f35d&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>另外，<strong>当引擎使用动态形状构建时</strong>，引擎信息中的动态维度将显示为-1 ，并且不会显示张量格式信息，因为这些字段取决于推理阶段的实际形状。<strong>要获取特定推理形状的引擎信息，请创建一个<code>IExecutionContext</code></strong> ，将所有输入尺寸设置为所需的形状，然后调用<code>inspector-&gt;setExecutionContext(context)</code> 。设置上下文后，检查器将打印上下文中设置的特定形状的引擎信息。</p>
<p>trtexec工具提供了<code>--profilingVerbosity</code> 、 <code>--dumpLayerInfo</code>和<code>--exportLayerInfo</code>标志，可用于获取给定引擎的引擎信息。有关详细信息，请参阅<a class="link"   target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec" >trtexec <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>部分。</p>
<p>目前，引擎信息中只包含绑定信息和层信息，包括中间张量的维度、精度、格式、策略指标、层类型和层参数。在未来的 TensorRT 版本中，更多信息可能会作为输出 JSON 对象中的新键添加到引擎检查器输出中。还将提供有关检查器输出中的键和字段的更多规范。</p>
<h2 id="Quantization-1"><a href="#Quantization-1" class="headerlink" title="Quantization"></a>Quantization</h2><p>TensorRT 支持使用 8 位整数来表示量化的浮点值。量化方案是对称均匀量化 – 量化值以有符号 INT8 表示，从量化到非量化值的转换只是一个乘法。在相反的方向上，量化使用倒数尺度，然后是舍入和钳位。<strong>要启用任何量化操作，必须在构建器配置中设置 INT8 标志。</strong></p>
<h3 id="Quantization-Workflows"><a href="#Quantization-Workflows" class="headerlink" title="Quantization Workflows"></a>Quantization Workflows</h3><p>创建量化网络有两种工作流程：</p>
<p>训练后量化(PTQ: Post-training quantization) <strong>在网络经过训练后得出比例因子。</strong> TensorRT 为 PTQ 提供了一个工作流程，称为<strong>校准</strong>(<code>calibration</code>)，当网络在代表性输入数据上执行时，它测量每个激活张量内的激活分布，然后使用该分布来估计张量的尺度值。</p>
<p>量化感知训练(QAT: Quantization-aware training) <strong>在训练期间计算比例因子。</strong>这允许训练过程补偿量化和去量化操作的影响。</p>
<p>TensorRT 的<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/pytorch-quantization" >量化工具包 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>是一个 PyTorch 库，可帮助生成可由 TensorRT 优化的 QAT 模型。您还可以利用工具包的 PTQ 方式在 PyTorch 中执行 PTQ 并导出到 ONNX。</p>
<h3 id="Explicit-vs-Implicit-Quantization"><a href="#Explicit-vs-Implicit-Quantization" class="headerlink" title="Explicit vs Implicit Quantization"></a>Explicit vs Implicit Quantization</h3><p>量化网络可以用两种方式表示：</p>
<p><strong>在隐式量化网络中，每个量化张量都有一个相关的尺度。在读写张量时，尺度用于隐式量化和反量化值。</strong></p>
<p>在处理隐式量化网络时，TensorRT 在应用图形优化时将模型视为浮点模型，并适时的使用 INT8 来优化层执行时间。如果一个层在 INT8 中运行得更快，那么它在 INT8 中执行。否则，使用 FP32 或 FP16。在这种模式下，TensorRT 仅针对性能进行优化，您几乎<strong>无法控制 INT8 的使用位置</strong>——即使您在 API 级别明确设置层的精度，TensorRT 也可能在图优化期间将该层与另一个层融合，并丢失它必须在 INT8 中执行的信息。 <strong>TensorRT 的 PTQ（训练后量化） 功能可生成隐式量化网络。</strong></p>
<p>在显式量化的网络中，<strong>在量化和未量化值之间转换的缩放操作由图中的<code>IQuantizeLayer</code> 和<code>IDequantizeLayer</code> 节点显式表示 – 这些节点此后将被称为 Q&#x2F;DQ （量化&#x2F;去量化）节点</strong>。<strong>与隐式量化相比，显式形式精确地指定了与 INT8 之间的转换在何处执行</strong>，并且优化器将仅执行由模型语义规定的精度转换，即使：</p>
<ul>
<li>添加额外的转换可以提高层精度（例如，选择 FP16 内核实现而不是 INT8 实现）</li>
<li>添加额外的转换会导致引擎执行得更快（例如，选择 INT8 内核实现来执行指定为具有浮点精度的层，反之亦然）</li>
</ul>
<p>ONNX 使用显式量化表示 – 当 PyTorch 或 TensorFlow 中的模型导出到 ONNX 时，框架图中的每个伪量化操作都导出为 Q，然后是 DQ。由于 TensorRT 保留了这些层的语义，因此您可以期望任务准确度非常接近框架中看到的准确度。虽然优化保留了量化和去量化的位置，但它们可能会改变模型中浮点运算的顺序，因此结果不会按位相同。 <strong>请注意，与 TensorRT 的 PTQ 相比，在框架中执行 QAT 或 PTQ 然后导出到 ONNX 将产生一个明确量化的模型</strong>。</p>
<h3 id="Per-Tensor-and-Per-Channel-Quantization"><a href="#Per-Tensor-and-Per-Channel-Quantization" class="headerlink" title="Per-Tensor and Per-Channel Quantization"></a>Per-Tensor and Per-Channel Quantization</h3><p>有两种常见的量化尺度粒度：</p>
<ul>
<li><strong>每张量量化：其中使用单个比例值（标量）来缩放整个张量。</strong></li>
<li><strong>每通道量化：沿给定轴广播尺度张量 – 对于卷积神经网络，这通常是通道轴。</strong></li>
</ul>
<p>通过显式量化，<strong>权重可以使用每张量量化进行量化，也可以使用每通道量化进行量化</strong>。在任何一种情况下，比例精度都是 FP32。<strong>激活只能使用每张量量化来量化</strong>。通过显式量化，权重可以使用每张量量化进行量化，也可以使用每通道量化进行量化。在任何一种情况下，比例精度都是 FP32。激活只能使用每张量量化来量化。</p>
<p><strong>当使用每通道量化时，量化轴必须是输出通道轴</strong>。例如，当使用<code>KCRS</code>表示法描述 2D 卷积的权重时， K是输出通道轴，权重量化可以描述为：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">For each k <span class="keyword">in</span> K:</span><br><span class="line">    For each c <span class="keyword">in</span> C:</span><br><span class="line">        For each r <span class="keyword">in</span> R:</span><br><span class="line">            For each s <span class="keyword">in</span> S:</span><br><span class="line">                output[k,c,r,s] := clamp(<span class="built_in">round</span>(<span class="built_in">input</span>[k,c,r,s] / scale[k]))		<span class="comment">#量化轴是k表示以k的尺度为标准</span></span><br></pre></td></tr></table></figure></div>

<p><strong>比例尺是一个系数向量，必须与量化轴具有相同的大小</strong>。量化尺度必须由所有正浮点系数组成。四舍五入方法是四舍五入到最近的关系到偶数，并且钳位在[-128, 127]范围内。</p>
<p>除了定义为的逐点操作外，反量化的执行方式类似：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output[k,c,r,s] := <span class="built_in">input</span>[k,c,r,s] * scale[k]</span><br></pre></td></tr></table></figure></div>

<p><strong>TensorRT 仅支持<code>激活张量</code>的每张量量化，但支持卷积、反卷积、全连接层和 MatMul 的每通道权重量化，其中第二个输入是常数且两个输入矩阵都是二维的。</strong></p>
<h2 id="Setting-Dynamic-Range"><a href="#Setting-Dynamic-Range" class="headerlink" title="Setting Dynamic Range"></a>Setting Dynamic Range</h2><p>TensorRT 提供 API 来直接<strong>设置动态范围（必须由量化张量表示的范围），以支持在 TensorRT 之外计算这些值的隐式量化。</strong> API 允许使用最小值和最大值设置张量的动态范围。由于 TensorRT 目前仅支持对称范围，因此使用<code>max(abs(min_float), abs(max_float))</code>计算比例。<strong>请注意，当<code>abs(min_float) != abs(max_float)</code>时，TensorRT 使用比配置更大的动态范围，这可能会增加舍入误差。</strong></p>
<p><strong>将在 INT8 中执行的操作的所有浮点输入和输出都需要动态范围。</strong></p>
<p>您可以按如下方式设置张量的动态范围：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor.dynamic_range = (min_float, max_float)</span><br></pre></td></tr></table></figure></div>

<h2 id="Post-Training-Quantization-using-Calibration"><a href="#Post-Training-Quantization-using-Calibration" class="headerlink" title="Post-Training Quantization using Calibration"></a>Post-Training Quantization using Calibration</h2><p><strong>在训练后量化中，TensorRT 计算网络中每个张量的比例值。这个过程称为校准</strong>，需要您提供有代表性的输入数据，TensorRT 在其上运行网络以收集每个激活张量的统计信息。</p>
<p>所需的输入数据量取决于应用程序，但实验表明大约 500 张图像足以校准 ImageNet 分类网络。</p>
<p>给定激活张量的统计数据，决定最佳尺度值并不是一门精确的科学——它需要平衡量化表示中的两个误差源：<strong>离散化误差</strong>（随着每个量化值表示的范围变大而增加）和<strong>截断误差</strong>（其中值被限制在可表示范围的极限，<strong>处于极限无法更改</strong>）。<strong>因此，TensorRT 提供了多个不同的校准器，它们以不同的方式计算比例。</strong>较旧的校准器还为 GPU 执行层融合，以在执行校准之前优化掉不需要的张量。这在使用 DLA 时可能会出现问题，其中融合模式可能不同，并且可以使用<code>kCALIBRATE_BEFORE_FUSION</code>量化标志覆盖。</p>
<p><strong>IInt8EntropyCalibrator2</strong></p>
<p><strong>熵校准选择张量的比例因子来优化量化张量的信息论内容，通常会抑制分布中的异常值</strong>。这是当前<strong>推荐</strong>的熵校准器，是 DLA 所必需的。默认情况下，校准发生在图层融合之前。推荐用于基于 CNN 的网络。</p>
<p><strong>IInt8MinMaxCalibrator</strong></p>
<p><strong>该校准器使用激活分布的整个范围来确定比例因子</strong>。它似乎更适合 NLP 任务。默认情况下，校准发生在图层融合之前。推荐用于 NVIDIA BERT（谷歌官方实现的优化版本）等网络。</p>
<p><strong>IInt8EntropyCalibrator</strong></p>
<p>这是原始的熵校准器。它的使用没有<code> LegacyCalibrator</code> 复杂，通常会产生更好的结果。默认情况下，校准发生在图层融合之后。</p>
<p><strong>IInt8LegacyCalibrator</strong></p>
<p>该校准器与 TensorRT 2.0 EA 兼容。此校准器需要用户参数化，<strong>并且在其他校准器产生不良结果时作为备用选项提供</strong>。默认情况下，校准发生在图层融合之后。您可以自定义此校准器以实现最大百分比，例如，观察到 99.99% 的最大百分比对于 NVIDIA BERT 具有最佳精度。</p>
<p><strong>构建 INT8 引擎时，构建器执行以下步骤：</strong></p>
<ol>
<li>构建一个 32 位引擎，在校准集上运行它，并为激活值分布的每个张量记录一个直方图。</li>
<li>从直方图构建一个校准表，为每个张量提供一个比例值。</li>
<li>根据校准表和网络定义构建 INT8 引擎。</li>
</ol>
<p><strong>校准可能很慢，因此步骤 2 的输出（校准表）可以被缓存和重用</strong>。这在多次构建相同的网络时非常有用，例如，在多个平台上 – 特别是，它可以简化工作流程，在具有离散 GPU 的机器上构建校准表，然后在嵌入式平台上重用它。可在此处找到样本校准表。 <strong>在运行校准之前，TensorRT 会查询校准器实现以查看它是否有权访问缓存表。如果是这样，它直接进行到上面的步骤 3</strong>。缓存数据作为指针和长度传递。</p>
<p><strong>只要校准发生在层融合之前，校准缓存数据就可以在平台之间以及为不同设备构建引擎时移植。</strong>这意味着在默认情况下使用<code>IInt8EntropyCalibrator2或IInt8MinMaxCalibrator</code>校准器或设置<code>QuantizationFlag::kCALIBRATE_BEFORE_FUSION</code>时，校准缓存是可移植的。不能保证跨平台或设备的融合是相同的，因此在层融合之后进行校准可能不会产生便携式校准缓存。 除了量化激活，TensorRT 还必须量化权重。它使用对称量化和使用权重张量中找到的最大绝对值计算的量化比例。对于卷积、反卷积和全连接权重，尺度是每个通道的。</p>
<p>注意：当构建器配置为使用 INT8 I&#x2F;O 时，TensorRT 仍希望校准数据位于 <code>FP32</code> 中。您可以通过将 INT8 I&#x2F;O 校准数据转换为 <code>FP32</code> 精度来创建 <code>FP32</code> 校准数据。您还必须确保 <code>FP32 </code>投射校准数据在<code>[-128.0F, 127.0F]</code>范围内，<strong>因此可以转换为 <code>INT8</code> 数据而不会造成任何精度损失。</strong></p>
<p>INT8 校准可与动态范围 API 一起使用。<strong>手动设置动态范围会覆盖 INT8 校准生成的动态范围。</strong></p>
<p>注意：校准是确定性的——也就是说，如果您在同一设备上以相同的顺序为 TensorRT 提供相同的校准输入，则生成的比例在不同的运行中将是相同的。当提供相同的校准输入时，当使用具有相同批量大小的相同设备生成时，校准缓存中的数据将按位相同。当使用不同的设备、不同的批量大小或使用不同的校准输入生成校准缓存时，不能保证校准缓存中的确切数据按位相同。</p>
<h3 id="Calibration-Using-Python"><a href="#Calibration-Using-Python" class="headerlink" title="Calibration Using Python"></a>Calibration Using Python</h3><p>以下步骤说明了如何使用 Python API 创建 INT8 校准器对象。 程序</p>
<ol>
<li>导入 TensorRT：</li>
</ol>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import tensorrt as trt</span><br></pre></td></tr></table></figure></div>

<ol start="2">
<li>与测试&#x2F;验证数据集类似，使用一组输入文件作为校准数据集。确保校准文件代表整个推理数据文件。为了让 TensorRT 使用校准文件，您必须创建一个批处理流对象。批处理流对象用于配置校准器。</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NUM_IMAGES_PER_BATCH = <span class="number">5</span></span><br><span class="line">batchstream = ImageBatchStream(NUM_IMAGES_PER_BATCH, calibration_files)</span><br></pre></td></tr></table></figure></div>

<ol start="3">
<li>使用输入节点名称和批处理流创建一个Int8_calibrator对象：</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Int8_calibrator = EntropyCalibrator([<span class="string">&quot;input_node_name&quot;</span>], batchstream)</span><br></pre></td></tr></table></figure></div>

<ol start="4">
<li>设置 INT8 模式和 INT8 校准器：</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config.set_flag(trt.BuilderFlag.INT8)</span><br><span class="line">config.int8_calibrator = Int8_calibrator</span><br></pre></td></tr></table></figure></div>

<h2 id="Explicit-Quantization"><a href="#Explicit-Quantization" class="headerlink" title="Explicit Quantization"></a>Explicit Quantization</h2><p><strong>当 TensorRT 检测到网络中存在 Q&#x2F;DQ 层时，它会使用显式精度处理逻辑构建一个引擎。</strong></p>
<p><strong>AQ&#x2F;DQ 网络必须在启用 INT8 精度构建器标志的情况下构建：</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;setFlag(BuilderFlag::kINT8);</span><br></pre></td></tr></table></figure></div>

<p>在显式量化中，表示与 INT8 之间的网络变化是显式的，因此，INT8 不能用作类型约束。</p>
<h3 id="Quantized-Weights"><a href="#Quantized-Weights" class="headerlink" title="Quantized Weights"></a>Quantized Weights</h3><p><strong>Q&#x2F;DQ 模型的权重必须使用 <code>FP32</code> 数据类型指定。</strong> TensorRT 使用对权重进行操作的<code>IQuantizeLayer</code>的比例对权重进行量化。<strong>量化的权重存储在引擎文件中</strong>。也可以使用预量化权重，但必须使用 <code>FP32</code> 数据类型指定。 Q 节点的 <code>scale</code> 必须设置为<code>1.0F</code> ，但 <code>DQ</code> 节点必须是真实的 <code>scale</code> 值</p>
<h3 id="ONNX-Support"><a href="#ONNX-Support" class="headerlink" title="ONNX Support"></a>ONNX Support</h3><p>当使用 <code>Quantization Aware Training (QAT)</code> 在 PyTorch 或 TensorFlow 中训练的模型导出到 ONNX 时，框架图中的每个伪量化操作都会导出为一对<code>QuantizeLinear</code>和<code>DequantizeLinear</code> ONNX 运算符。</p>
<p>当 TensorRT 导入 ONNX 模型时，ONNX <code>QuantizeLinear</code>算子作为<code>IQuantizeLayer</code>实例导入，ONNX <code>DequantizeLinear</code>算子作为<code>IDequantizeLayer</code>实例导入。</p>
<p>警告： <strong>ONNX GEMM 算子是一个可以按通道量化的示例。</strong> PyTorch <code>torch.nn.Linear</code>层导出为 ONNX GEMM 算子，具有<code>(K, C)</code>权重布局并启用了<code>transB</code> GEMM 属性（这会在执行 GEMM 操作之前转置权重）。另一方面，TensorFlow在 ONNX 导出之前预转置权重<code>(C, K)</code> ：</p>
<p>PyTorch: $y &#x3D; xW^T$</p>
<p>TensorFlow: $y &#x3D; xW$</p>
<p><strong>PyTorch 权重由 TensorRT 转置</strong>。权重在转置之前由 TensorRT 进行量化，源自从 PyTorch 导出的 ONNX QAT 模型的 GEMM 层使用维度0进行每通道量化（轴K &#x3D; 0 ）；而源自 TensorFlow 的模型使用维度1 （轴K &#x3D; 1 ）。</p>
<p><strong>TensorRT 不支持使用 INT8 张量或量化运算符的预量化 ONNX 模型。</strong>具体来说，以下 ONNX 量化运算符不受支持，如果在 TensorRT 导入 ONNX 模型时遇到它们，则会生成导入错误：</p>
<ul>
<li>QLinearConv&#x2F;QLinearMatmul</li>
<li>ConvInteger&#x2F;MatmulInteger</li>
</ul>
<h3 id="TensorRT-Processing-Of-Q-x2F-DQ-Networks"><a href="#TensorRT-Processing-Of-Q-x2F-DQ-Networks" class="headerlink" title="TensorRT Processing Of Q&#x2F;DQ Networks"></a>TensorRT Processing Of Q&#x2F;DQ Networks</h3><p>当 TensorRT 在 Q&#x2F;DQ 模式下优化网络时，优化过程仅限于不改变网络算术正确性的优化。由于浮点运算的顺序会产生不同的结果（例如，重写 $a * s + b * s$ 为 $(a + b) * s$是一个有效的优化）。允许这些差异通常是后端优化的基础，这也适用于将具有 <code>Q/DQ</code> 层的图转换为使用 INT8 计算。</p>
<p><strong>Q&#x2F;DQ 层控制网络的计算和数据精度</strong>。 <code>IQuantizeLayer</code>实例通过量化将 <code>FP32</code> 张量转换为 <code>INT8</code> 张量， <code>IDequantizeLayer</code>实例通过反量化将INT8张量转换为 FP32 张量。 TensorRT 期望量化层的每个输入上都有一个 <code>Q/DQ</code> 层对。量化层是深度学习层，可以通过与<code>IQuantizeLayer</code>和<code>IDequantizeLayer</code>实例融合来转换为量化层。当 TensorRT 执行这些融合时，它会将可量化层替换为实际使用 INT8 计算操作对 INT8 数据进行操作的量化层。</p>
<p>下图. 可量化的<code>AveragePool</code>层（蓝色）与 <code>DQ</code> 层和 <code>Q</code> 层融合。所有三层都被量化的<code>AveragePool</code>层（绿色）替换。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224092537580-168493652323435.png"
                      class="" title="image-20230224092537580"
                >

<p>在网络优化期间，TensorRT 在称为 <code>Q/DQ</code> 传播的过程中移动 <code>Q/DQ</code> 层。<strong>传播的目标是最大化以低精度处理的图的比例</strong>。<strong>因此，TensorRT 向后传播 Q 节点（以便尽可能早地进行量化）和向前传播 DQ 节点（以便尽可能晚地进行去量化）</strong>。 Q-layers 可以与 <code>commute-with-Quantization</code> 层交换位置，DQ-layers 可以与 <code>commute-with-Dequantization</code> 层交换位置。</p>
<p>下图说明了 DQ 前向传播和 Q 后向传播。这些是对模型的合法重写，因为 <code>Max Pooling</code> 具有 INT8 实现，并且因为 Max Pooling 与 DQ 和 Q 通讯。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224093014841-168493652323436.png"
                      class="" title="image-20230224093014841"
                >

<p><strong>量化层和交换层的处理方式是有区别的，可量化层融合，不可量化层交换</strong>两种类型的层都可以在 INT8 中计算，但可量化层也与 DQ 输入层和 Q 输出层融合。例如， <code>AveragePooling</code>层（可量化）不与 Q 或 DQ 交换，因此使用 <code>Q/DQ</code> 融合对其进行量化，如第一张图所示。这与如何量化 <code>Max Pooling</code>（交换）形成对比。</p>
<h3 id="Q-x2F-DQ-Layer-Placement-Recommendations"><a href="#Q-x2F-DQ-Layer-Placement-Recommendations" class="headerlink" title="Q&#x2F;DQ Layer-Placement Recommendations"></a>Q&#x2F;DQ Layer-Placement Recommendations</h3><p><strong>Q&#x2F;DQ 层在网络中的放置会影响性能和准确性。</strong>由于量化引入的误差，激进量化会导致模型精度下降。但量化也可以减少延迟。此处列出了在网络中放置 Q&#x2F;DQ 层的一些建议。</p>
<p>量化加权运算（卷积、转置卷积和 GEMM）的所有输入。<strong>权重和激活的量化降低了带宽需求</strong>，还使 INT8 计算能够加速带宽受限和计算受限的层。</p>
<p>下图 TensorRT 如何融合卷积层的两个示例。在左边，只有输入被量化。在右边，输入和输出都被量化了。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224094858275-168493652323437.png"
                      class="" title="image-20230224094858275"
                >

<p><strong>默认情况下，不量化加权运算的输出。保留更高精度的去量化输出有时很有用</strong>。例如，如果线性运算后面跟着一个激活函数（SiLU，下图中），它需要更高的精度输入才能产生可接受的精度。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224095026719-168493652323438.png"
                      class="" title="image-20230224095026719"
                >]

<p><strong>不要在训练框架中模拟批量归一化和 ReLU 融合</strong>，因为 TensorRT 优化保证保留这些操作的算术语义。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224095059801-168493652323439.png"
                      class="" title="image-20230224095059801"
                >

<p>TensorRT 可以在加权层之后融合element-wise addition，这对于像 ResNet 和 EfficientNet 这样具有跳跃连接的模型很有用。<strong>element-wise addition层的第一个输入的精度决定了融合输出的精度。</strong></p>
<p>比如下图中，$x_f^1$的精度是浮点数，所以融合卷积的输出仅限于浮点数，后面的Q层不能和卷积融合。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224095136553-168493652323541.png"
                      class="" title="image-20230224095136553"
                >

<p>相比之下，当$x_f^1$量化为 INT8 时，如下图所示，融合卷积的输出也是 INT8，尾部的 Q 层与卷积融合。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224095222434-168493652323540.png"
                      class="" title="image-20230224095222434"
                >

<p>为了获得额外的性能，请尝试使用 Q&#x2F;DQ 量化不交换的层。目前，具有 INT8 输入的非加权层也需要 INT8 输出，因此对输入和输出都进行量化。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese/blob/main/7.TensorRT%E4%B8%AD%E7%9A%84INT8/q-dq-placement6.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese/raw/main/7.TensorRT%E4%B8%AD%E7%9A%84INT8/q-dq-placement6.png"
                      alt="img"
                ></a></p>
<p>如果 TensorRT 无法将操作与周围的 Q&#x2F;DQ 层融合，则性能可能会降低，因此在添加 Q&#x2F;DQ 节点时要保守，并牢记准确性和 TensorRT 性能进行试验。</p>
<p>下图是额外 Q&#x2F;DQ 操作可能导致的次优融合示例（突出显示的浅绿色背景矩形）。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese/blob/main/7.TensorRT%E4%B8%AD%E7%9A%84INT8/sub-optimal.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese/raw/main/7.TensorRT%E4%B8%AD%E7%9A%84INT8/sub-optimal.png"
                      alt="img"
                ></a></p>
<p><strong>对激活使用逐张量量化；和每个通道的权重量化。这种配置已经被经验证明可以带来最佳的量化精度。</strong></p>
<p>您可以通过启用 <code>FP16</code> 进一步优化引擎延迟。 TensorRT 尽可能尝试使用 <code>FP16</code> 而不是 <code>FP32</code>（目前并非所有层类型都支持）</p>
<h3 id="Q-x2F-DQ-Limitations"><a href="#Q-x2F-DQ-Limitations" class="headerlink" title="Q&#x2F;DQ Limitations"></a>Q&#x2F;DQ Limitations</h3><p>TensorRT 执行的一些 Q&#x2F;DQ 图重写优化比较两个或多个 Q&#x2F;DQ 层之间的量化尺度值，并且仅在比较的量化尺度相等时才执行图重写。改装可改装的 TensorRT 引擎时，可以为 Q&#x2F;DQ 节点的尺度分配新值。在 Q&#x2F;DQ 引擎的改装操作期间，TensorRT 检查是否为参与尺度相关优化的 Q&#x2F;DQ 层分配了破坏重写优化的新值，如果为真则抛出异常。</p>
<p>下比较 Q1 和 Q2 的尺度是否相等的示例，如果相等，则允许它们向后传播。如果使用 Q1 和 Q2 的新值对引擎进行改装，使得<code>Q1 != Q2</code> ，则异常中止改装过程。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224100753426-168493652323542.png"
                      class="" title="image-20230224100753426"
                >

<h3 id="QAT-Networks-Using-PyTorch"><a href="#QAT-Networks-Using-PyTorch" class="headerlink" title="QAT Networks Using PyTorch"></a>QAT Networks Using PyTorch</h3><p>PyTorch 1.8.0 和前版支持 ONNX <a class="link"   target="_blank" rel="noopener" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#QuantizeLinear" >QuantizeLinear <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> &#x2F; <a class="link"   target="_blank" rel="noopener" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#dequantizelinear" >DequantizeLinear <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ，支持每通道缩放。您可以使用<a class="link"   target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/tools/pytorch-quantization" >pytorch-quantization <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>进行 INT8 校准，运行量化感知微调，生成 ONNX，最后使用 TensorRT 在此 ONNX 模型上运行推理。</p>
<h1 id="TensorRT-中的动态形状"><a href="#TensorRT-中的动态形状" class="headerlink" title="TensorRT 中的动态形状"></a>TensorRT 中的动态形状</h1><p><strong>动态形状(Dynamic Shapes)</strong> 是延迟指定部分或全部张量维度直到运行时的能力。动态形状可以通过 C++ 和 Python 接口使用。 以下部分提供了更详细的信息；但是，这里概述了构建具有动态形状的引擎的步骤：</p>
<p>1**.网络定义不得具有隐式批次维度。**</p>
<p>通过调用创建tensorrt.INetworkDefinition</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create_network(<span class="number">1</span> &lt;&lt; </span><br><span class="line">        <span class="built_in">int</span>(tensorrt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))		<span class="comment">#这些调用要求网络没有隐式批处理维度。</span></span><br></pre></td></tr></table></figure></div>

<ol start="2">
<li><code>-1</code>作为维度的占位符来指定输入张量的每个运行时维度。</li>
<li>指定一个或多个优化配置文件，为具有运行时维度的输入指定允许的维度范围，以及自动调整器将优化的维度。</li>
<li>要使用引擎：<ul>
<li>从引擎创建执行上下文，与没有动态形状的情况相同。</li>
<li>指定步骤 3 中涵盖输入维度的优化配置文件之一。</li>
<li>指定执行上下文的输入维度。设置输入维度后，您可以获得TensorRT针对给定输入维度计算的输出维度。</li>
<li>Enqueue work。</li>
</ul>
</li>
</ol>
<h2 id="Specifying-Runtime-Dimensions"><a href="#Specifying-Runtime-Dimensions" class="headerlink" title="Specifying Runtime Dimensions"></a>Specifying Runtime Dimensions</h2><p>构建网络时，使用<code>-1</code>表示输入张量的运行时维度。例如，要创建一个名为<code>foo</code>的 3D 输入张量，其中最后两个维度在运行时指定，第一个维度在构建时固定，请发出以下命令。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network_definition.add_input(&quot;foo&quot;, trt.float32, (3, -1, -1))</span><br></pre></td></tr></table></figure></div>

<p>在运行时，您需要在选择优化配置文件后设置输入维度（请参阅<a class="link"   target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles" >优化配置文件 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）。设输入<code>foo</code>的<code>bindingIndex</code>为<code>0</code> ，输入的维度为<code>[3,150,250]</code> 。在为前面的示例设置优化配置文件后，将调用：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.set_binding_shape(0, (3, 150, 250))</span><br></pre></td></tr></table></figure></div>

<p>在运行时，向引擎询问绑定维度会返回用于构建网络的相同维度，这意味着每个运行时维度都会得到<code>-1</code> （<strong>只指定了个别维度，其他维度为-1</strong>）。例如：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">engine.get_binding_shape(<span class="number">0</span>)     <span class="comment"># returns (3, -1, -1)</span></span><br></pre></td></tr></table></figure></div>

<p>要获取特定于每个执行上下文的实际维度，请查询执行上下文：<strong>（<em>上下文</em>,<em>也就是执行任务所需要的相关信息</em>。这个任务可以是一段代码,一个线程,一个进程,一个函数。）</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.get_binding_shape(<span class="number">0</span>)  <span class="comment"># returns (3, 150, 250).</span></span><br></pre></td></tr></table></figure></div>

<p>注意：<strong>输入的<code>setBindingDimensions</code>的返回值仅表明与为该输入设置的优化配置文件相关的一致性</strong>。指定所有输入绑定维度后，您可以通过查询网络输出绑定的维度来检查整个网络在动态输入形状方面是否一致。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::Dims out_dim = context-&gt;getBindingDimensions(out_index);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (out_dim.nbDims == -<span class="number">1</span>) &#123;</span><br><span class="line">gLogError &lt;&lt; <span class="string">&quot;Invalid network output, this might be caused by inconsistent input shapes.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">// abort inference</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="Optimization-Profiles"><a href="#Optimization-Profiles" class="headerlink" title="Optimization Profiles"></a>Optimization Profiles</h2><p>优化配置文件描述了每个网络输入的维度范围以及自动调谐器将用于优化的维度。<strong>使用运行时维度时，您必须在构建时创建至少一个优化配置文件</strong>。两个配置文件可以指定不相交或重叠的范围。</p>
<p>例如，一个配置文件可能指定最小尺寸<code>[3,100,200]</code> ，最大尺寸<code>[3,200,300]</code>和优化尺寸<code>[3,150,250]</code>而另一个配置文件可能指定最小，最大和优化尺寸<code>[3,200,100] ， [3,300,400] ，和[3,250,250]</code> 。</p>
<p>要创建优化配置文件，首先构造一个<code>IOptimizationProfile</code> 。然后设置最小、优化和最大维度，并将其添加到网络配置中。优化配置文件定义的形状必须为网络定义有效的输入形状。以下是前面提到的第一个配置文件对输入<code>foo</code>的调用：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">profile = builder.create_optimization_profile();</span><br><span class="line">profile.set_shape(<span class="string">&quot;foo&quot;</span>, (<span class="number">3</span>, <span class="number">100</span>, <span class="number">200</span>), (<span class="number">3</span>, <span class="number">150</span>, <span class="number">250</span>), (<span class="number">3</span>, <span class="number">200</span>, <span class="number">300</span>)) </span><br><span class="line">config.add_optimization_profile(profile)</span><br></pre></td></tr></table></figure></div>

<p><strong>在运行时，您需要在设置输入维度之前设置优化配置文件。</strong>配置文件按照添加的顺序编号，从0开始。请注意，<strong>每个执行上下文必须使用单独的优化配置文件，不可以同时使用</strong>。 要选择示例中的第一个优化配置文件，请使用：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.set_optimization_profile_async(<span class="number">0</span>, stream)   <span class="comment">#其中stream是在此上下文中用于后续enqueue()或enqueueV2()调用的 CUDA 流。</span></span><br></pre></td></tr></table></figure></div>

<p>如果关联的 CUDA 引擎具有动态输入，则必须使用唯一的配置文件索引至少设置一次优化配置文件，该唯一配置文件索引未被其他未销毁的执行上下文使用。对于为引擎创建的第一个执行上下文，隐式选择配置文件 0。</p>
<p><strong>可以调用<code>setOptimizationProfileAsync()</code>在配置文件之间切换。</strong>它必须在当前上下文中的任何<code>enqueue()</code>或<code>enqueueV2()</code>操作完成后调用。当多个执行上下文同时运行时，允许切换到以前使用但已被具有不同动态输入维度的另一个执行上下文释放的配置文件。</p>
<p><strong>在由多个配置文件构建的引擎中，每个配置文件都有单独的绑定索引。</strong>第K个配置文件的输入&#x2F;输出张量的名称附加了<code>[profile K]</code> ，其中K以十进制表示。例如，如果<code>INetworkDefinition</code>的名称为“ <code>foo </code>”，并且<code>bindingIndex</code>指的是优化配置文件中索引为<code>3</code>的张量，则<code>engine.getBindingName ( bindingIndex )</code> 返回“ <code>foo [profile 3]</code> ”。</p>
<p>同样，如果使用<code>ICudaEngine::getBindingIndex(name)</code>获取第一个配置文件 ( <code>K=0 </code>) 之外的配置文件 K 的索引，请将“<code>[profile K]</code>”附加到<code>INetworkDefinition</code>中使用的名称。例如，如果张量在 <code>INetworkDefinition</code> 中被称为“ <code>foo</code> ” ，则<code>engine.getBindingIndex ( “ foo [profile 3] ” )</code><strong>在优化配置文件<code>3</code>中返回张量“ <code>foo</code>”的绑定索引。</strong>始终省略K&#x3D;0的后缀。</p>
<h3 id="Bindings-For-Multiple-Optimization-Profiles"><a href="#Bindings-For-Multiple-Optimization-Profiles" class="headerlink" title="Bindings For Multiple Optimization Profiles"></a>Bindings For Multiple Optimization Profiles</h3><p>考虑一个具有四个输入、一个输出、在<code>IBuilderConfig</code>中具有三个优化配置文件的网络。该引擎有 <code>15</code> 个绑定，每个优化配置文件有 <code>5 </code>个，在概念上组织为一个表：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230224104755566-168493652323543.png"
                      class="" title="image-20230224104755566"
                >

<p>每行都是一个配置文件。表中的数字表示绑定索引。第一个配置文件的绑定索引为 0..4，第二个配置文件为 5..9，第三个配置文件为 10..14。</p>
<p><strong>对于绑定属于第一个配置文件但指定了另一个配置文件的情况，接口具有“自动更正”功能。</strong>在这种情况下，TensorRT 会警告错误，然后从同一列中选择正确的绑定索引。</p>
<h2 id="Layer-Extensions-For-Dynamic-Shapes"><a href="#Layer-Extensions-For-Dynamic-Shapes" class="headerlink" title="Layer Extensions For Dynamic Shapes"></a>Layer Extensions For Dynamic Shapes</h2><p><strong>一些层具有允许指定动态形状信息的可选输入，并且有一个新层IShapeLayer用于在运行时访问张量的形状。</strong>此外，一些层允许计算新的形状。下一节将讨论语义细节和限制。以下是与动态形状结合使用时可能有用的内容的摘要。</p>
<p><code>IShapeLayer</code>输出一个包含输入张量尺寸的一维张量。例如，如果输入张量的维度为<code>[2,3,5,7]</code> ，则输出张量是包含<code>&#123;2,3,5,7&#125;</code>的四元素一维张量。如果输入张量是标量，则它的维度为[] ，输出张量是包含{}的零元素一维张量。</p>
<p><code>IResizeLayer</code>接受包含所需输出尺寸的可选第二个输入。</p>
<p><code>IShuffleLayer</code>接受包含重塑尺寸的可选第二个输入。例如，以下网络将张量Y重塑为与X具有相同的维度：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reshape = network_definition.add_shuffle(y)</span><br><span class="line">reshape.set_input(1, network_definition.add_shape(X).get_output(0))</span><br></pre></td></tr></table></figure></div>

<p><code>ISliceLayer</code>接受可选的第二、第三和第四个输入，其中包含开始、大小和步幅。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IConcatenationLayer, IElementWiseLayer, IGatherLayer, IIdentityLayer, and IReduceLayer</span><br></pre></td></tr></table></figure></div>

<p>可用于对形状进行计算并创建新的形状张量。</p>
<h2 id="Restrictions-For-Dynamic-Shapes"><a href="#Restrictions-For-Dynamic-Shapes" class="headerlink" title="Restrictions For Dynamic Shapes"></a>Restrictions For Dynamic Shapes</h2><p>由于层的权重具有固定大小，因此会出现以下层限制：</p>
<ul>
<li><code>IConvolutionLayer</code>和<code>IDeconvolutionLayer</code>要求通道维度是构建时常数。</li>
<li><code>IFullyConnectedLayer</code>要求最后三个维度是构建时常量。</li>
<li><code>Int8</code>要求通道维度是构建时常数。</li>
<li>接受额外形状输入的层（ <code>IResizeLayer</code> 、 <code>IShuffleLayer</code> 、 <code>ISliceLayer</code> ）要求额外的形状输入与最小和最大优化配置文件的尺寸以及运行时数据输入的尺寸兼容；否则，它可能导致构建时或运行时错误。</li>
</ul>
<p>必须是构建时常量的值不必是 API 级别的常量。 TensorRT 的形状分析器通过进行形状计算的层进行逐个元素的常数传播。常量传播发现一个值是构建时常量就足够了。</p>
<p>tensorRT是NVIDIA发布的DNN推理引擎，是针对NVIDIA系列硬件进行优化加速，实现最大程度的利用GPU资源，提升推理性能</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230220094750628-16849365232331.png"
                      class="" title="image-20230220094750628"
                >

<h1 id="ONNX"><a href="#ONNX" class="headerlink" title="ONNX"></a>ONNX</h1><p>ONNX（Open Neural Network Exchange），ONNX是微软和Facebook提出用来表示深度学习模型的开放格式。所谓开放就是ONNX定义了一组和环境，平台均无关的标准格式，来增强各种AI模型的可交互性。</p>
<p>换句话说，无论你使用何种训练框架训练模型（比如TensorFlow&#x2F;Pytorch&#x2F;OneFlow&#x2F;Paddle），<strong>在训练完毕后你都可以将这些框架的模型统一转换为ONNX这种统一的格式进行存储</strong>。注意ONNX文件不仅仅存储了神经网络模型的权重，<strong>同时也存储了模型的结构信息以及网络中每一层的输入输出和一些其它的辅助信息</strong>。</p>
<p><strong>yolov3-tiny的onnx模型</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230220101057791-16849365232332.png"
                      class="" title="image-20230220101057791"
                >

<p>在获得ONNX模型之后，模型部署人员自然就可以将这个模型部署到兼容ONNX的运行环境中去。这里一般还会设计到额外的模型转换工作，典型的比如在Android端利用NCNN部署ONNX格式模型，那么就需要将ONNX利用NCNN的转换工具转换到NCNN所支持的<code>bin</code>和<code>param</code>格式。</p>
<h3 id="ProtoBuf简介"><a href="#ProtoBuf简介" class="headerlink" title="ProtoBuf简介"></a>ProtoBuf简介</h3><p>ONNX使用的是<strong>Protobuf</strong>这个序列化数据结构去存储神经网络的权重信息。Protobuf是一种轻便高效的结构化数据存储格式，可以用于结构化数据<strong>串行化</strong>，或者说<strong>序列化</strong>。<strong>它很适合做数据存储或数据交换格式。</strong>可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。</p>
<p>Protobuf协议是一个以<code>*.proto</code>后缀文件为基础的，这个文件描述了用户自定义的数据结构。ONNX是基于Protobuf来做数据存储和传输，那么自然<code>onnx.proto</code>就是ONNX格式文件了</p>
<h3 id="ONNX格式分析"><a href="#ONNX格式分析" class="headerlink" title="ONNX格式分析"></a>ONNX格式分析</h3><p>ONNX中最核心的部分就是<code>onnx.proto</code>（<code>https://github.com/onnx/onnx/blob/master/onnx/onnx.proto</code>）这个文件，它定义了ONNX这个数据协议的规则和一些其它信息。在这个文件里面以<code>message</code>关键字开头的对象是我们需要关心的。</p>
<p>当我们加载了一个ONNX之后，我们获得的就是一个<code>ModelProto</code>，它包含了一些版本信息，生产者信息和一个<code>GraphProto</code>。在<code>GraphProto</code>里面又包含了四个<code>repeated</code>数组，它们分别是<code>node</code>(<code>NodeProto</code>类型)，<code>input</code>(<code>ValueInfoProto</code>类型)，<code>output</code>(<code>ValueInfoProto</code>类型)和<code>initializer</code>(<code>TensorProto</code>类型)，其中<code>node</code>中存放了模型中所有的计算节点，<code>input</code>存放了模型的输入节点，<code>output</code>存放了模型中所有的输出节点，**<code>initializer</code>存放了模型的所有权重参数。**</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230220103617774-16849365232333.png"
                      class="" title="image-20230220103617774"
                >

<p>我们知道要完整的表达一个神经网络，不仅仅要知道网络的各个节点信息，还要知道它们的拓扑关系。这个拓扑关系在ONNX中是如何表示的呢？ONNX的每个计算节点都会有<code>input</code>和<code>output</code>两个数组，这两个数组是string类型，通过<code>input</code>和<code>output</code>的指向关系，我们就可以利用上述信息快速构建出一个深度学习模型的拓扑图。这里要注意一下，**<code>GraphProto</code>中的<code>input</code>数组不仅包含我们一般理解中的图片输入的那个节点，还包含了模型中所有的权重。（所有的节点的权重是保存在initializer中的）**例如，<code>Conv</code>层里面的<code>W</code>权重实体是保存在<code>initializer</code>中的，那么相应的会有一个同名的输入在<code>input</code>中，其背后的逻辑应该是把权重也看成模型的输入，并通过<code>initializer</code>中的权重实体来对这个输入做初始化，即一个赋值的过程。</p>
<p>最后，每个计算节点中还包含了一个<code>AttributeProto</code>数组，用来描述该节点的属性，比如<code>Conv</code>节点或者说卷积层的属性包含<code>group</code>，<code>pad</code>，<code>strides</code>等等</p>
<h3 id="onnx-helper"><a href="#onnx-helper" class="headerlink" title="onnx.helper"></a>onnx.helper</h3><p>现在我们知道ONNX是把一个网络的每一层或者说一个算子当成节点<code>node</code>，使用这些<code>Node</code>去构建一个<code>Graph</code>，即一个网络。最后将<code>Graph</code>和其它的生产者信息，版本信息等合并在一起生成一个<code>Model</code>，也即是最终的ONNX模型文件。 在构建ONNX模型的时候，<code>https://github.com/onnx/onnx/blob/master/onnx/helper.py</code>这个文件非常重要，我们可以利用它提供的<code>make_node</code>，<code>make_graph</code>，<code>make_tensor</code>等等接口完成一个ONNX模型的构建，一个示例如下：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">from</span> onnx <span class="keyword">import</span> helper</span><br><span class="line"><span class="keyword">from</span> onnx <span class="keyword">import</span> AttributeProto, TensorProto, GraphProto</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The protobuf definition can be found here:</span></span><br><span class="line"><span class="comment"># https://github.com/onnx/onnx/blob/master/onnx/onnx.proto</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create one input (ValueInfoProto)</span></span><br><span class="line">X = helper.make_tensor_value_info(<span class="string">&#x27;X&#x27;</span>, TensorProto.FLOAT, [<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">pads = helper.make_tensor_value_info(<span class="string">&#x27;pads&#x27;</span>, TensorProto.FLOAT, [<span class="number">1</span>, <span class="number">4</span>])           <span class="comment">#padding</span></span><br><span class="line"></span><br><span class="line">value = helper.make_tensor_value_info(<span class="string">&#x27;value&#x27;</span>, AttributeProto.FLOAT, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create one output (ValueInfoProto)</span></span><br><span class="line">Y = helper.make_tensor_value_info(<span class="string">&#x27;Y&#x27;</span>, TensorProto.FLOAT, [<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a node (NodeProto) - This is based on Pad-11</span></span><br><span class="line">node_def = helper.make_node(</span><br><span class="line">    <span class="string">&#x27;Pad&#x27;</span>, <span class="comment"># node name</span></span><br><span class="line">    [<span class="string">&#x27;X&#x27;</span>, <span class="string">&#x27;pads&#x27;</span>, <span class="string">&#x27;value&#x27;</span>], <span class="comment"># inputs</span></span><br><span class="line">    [<span class="string">&#x27;Y&#x27;</span>], <span class="comment"># outputs</span></span><br><span class="line">    mode=<span class="string">&#x27;constant&#x27;</span>, <span class="comment"># attributes</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the graph (GraphProto)</span></span><br><span class="line">graph_def = helper.make_graph(</span><br><span class="line">    [node_def],</span><br><span class="line">    <span class="string">&#x27;test-model&#x27;</span>,</span><br><span class="line">    [X, pads, value],</span><br><span class="line">    [Y],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the model (ModelProto)</span></span><br><span class="line">model_def = helper.make_model(graph_def, producer_name=<span class="string">&#x27;onnx-example&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The model is:\n&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(model_def))</span><br><span class="line">onnx.checker.check_model(model_def)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The model is checked!&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p><strong>输入数据是一个一维Tensor，初始维度为<code>[2]</code>，这也是为什么经过维度为<code>[1,4]</code>的Pad操作之后获得的输出Tensor维度为<code>[3,4]</code>。</strong>上例中的Pad操作是没有带任何权重信息的，所以当你打印ONNX模型时，<code>ModelProto</code>的<code>GraphProto</code>是没有<code>initializer</code>这个属性的。</p>
<h1 id="调用TensorRT的方案"><a href="#调用TensorRT的方案" class="headerlink" title="调用TensorRT的方案"></a>调用TensorRT的方案</h1><p><strong>基于C++</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221082244538-16849365232334.png"
                      class="" title="image-20230221082244538"
                >

<p><strong>基于python</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221082316063-16849365232335.png"
                      class="" title="image-20230221082316063"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221082544827-16849365232336.png"
                      class="" title="image-20230221082544827"
                >

<p><strong>repo1</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221083048073-16849365232337.png"
                      class="" title="image-20230221083048073"
                >

<p>使用作者定义的gen_wts.py储存权重，使用C++硬代码调用TensorRT基于C++的api构建模型结构，加载gen_wts.py产生的权重组成完整模型</p>
<p>优点：</p>
<ul>
<li>可以控制每个layer的细节和权重，直接面对TensorRT的api</li>
<li>这种方案不存在算子问题，如果存在不支持的算子，可以自行增加插件，灵活性最高</li>
</ul>
<p>缺点：</p>
<ul>
<li>新模型需要一个layer一个layer写，不具有通用性</li>
<li>作者提供的推理代码是demo级，使用阶段需要修改太多</li>
<li>部署时无法查看网络结构进行分析和排查</li>
</ul>
<p><strong>repo2</strong></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221085107849-16769406687161-16849365232338.png"
                      class="" title="image-20230221085107849"
                >

<p>基于Pytorch的算子，为每个算子写Converter，为每个操作的forward反射到自定义函数；通过反射torch的forward操作捕获模块的权重，调用python api实现模型创建</p>
<p>优点：</p>
<ul>
<li>直接集成python、pytorch、可以实现pytorch模型到tensorRT模型的简单转换</li>
</ul>
<p>缺点：</p>
<ul>
<li>无C++方案</li>
<li>新的算子需要自己实现Converter，需要维护新的算子库</li>
<li>跨设备需要重新安装pytorch，灵活度不利于部署</li>
<li>部署时无法查看网络结构进行分析和排查</li>
</ul>
<p><strong>repo3（推荐）</strong></p>
<p>基于ONNX路线，提供C++、Python接口，深度定制ONNXParrser，低耦合封装，实现常用模型YoloX、YoloV5、RetinaFace、Arcface、SCRFD、DeepSORT算子由官方维护，模型直接导出</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221090526167-16849365232339.png"
                      class="" title="image-20230221090526167"
                >

<p>优点：</p>
<ul>
<li>集成工业级推理方案、支持tensorRT从模型导出到应用到项目中的全部工作</li>
<li>案例有YoloV5等，每个应用均为高性能工业级使用</li>
<li>具有简单的模型导出方法和ONNX问题的解决方案，封装tensorRT细节，支持插件</li>
<li>支持python接口导出模型和推理接口</li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221090939235-168493652323310.png"
                      class="" title="image-20230221090939235"
                >

<h1 id="导出ONNX注意的问题"><a href="#导出ONNX注意的问题" class="headerlink" title="导出ONNX注意的问题"></a>导出ONNX注意的问题</h1><ol>
<li>对于任何用到shape、size返回值参数时，为了避免shape、size的跟踪，产生额外的数值，加上int转换。如<code> tensor.view(tensor,size(0),-1)</code></li>
<li>对于nn.Upsample或nn.functional.interpolate函数，使用<strong>scale_factor</strong>指定倍率，而不是使用size参数指定大小</li>
<li>对于reshape、view操作时，-1的指定要放到batch维度，其他维度可以直接计算。batch维度禁止指定为大于-1的明确数字</li>
<li>torch.onnx.export指定dynamic_axes参数，并且只指定batch维度，不指定其他维度。</li>
</ol>
<p>这些做法的必要性体现在，简化过程的复杂度，去掉gather、shape类的节点</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221092555385-168493652323411.png"
                      class="" title="image-20230221092555385"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221092623949-168493652323412.png"
                      class="" title="image-20230221092623949"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221092806781-168493652323413.png"
                      class="" title="image-20230221092806781"
                >

<h1 id="动态batch和动态宽高"><a href="#动态batch和动态宽高" class="headerlink" title="动态batch和动态宽高"></a>动态batch和动态宽高</h1><p><strong>动态batch</strong></p>
<p>源自tensorRT编译时对batch的处理，静态batch表示无论有多少张图，都按照固定大小batch推理，耗时是固定的</p>
<ol>
<li>导出模型时，注意view操作不能固定batch维度数值，通常写-1</li>
<li>导出模型时，通常可以指定dynamic_axes（可以不指定）</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221095444973-168493652323414.png"
                      class="" title="image-20230221095444973"
                >

<p><strong>动态宽高</strong></p>
<p>ONNX导出时指定的宽高是固定的，TRT编译时也得到固定大小引擎，动态宽高可以得到大小不同的引擎（一个引擎最好对应一钟分辨率的图片）。而使用TRT的动态宽高会带来太多不必要的复杂度，使用中间方案，编译时修改ONNX输入实现相对动态</p>
<ol>
<li>不建议改变batch_size</li>
<li>需要修改ONNX的输入shape<ul>
<li>使用TRT::compile函数的inputDimsSetup参数重定义输入的shape</li>
<li>使用TRT::set_layer_hook_reshape钩子动态修改reshape的参数实现适配</li>
</ul>
</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221095945685-168493652323415.png"
                      class="" title="image-20230221095945685"
                >

<p>修改ONNX的输入后，其中的reshape会受到新的宽度而改变，通过hook钩子修函reshape的参数，实现相对动态</p>
<h1 id="实现一个自定义插件"><a href="#实现一个自定义插件" class="headerlink" title="实现一个自定义插件"></a>实现一个自定义插件</h1><p>导出环节：</p>
<ol>
<li>对于需要插件的layer，创建一个类A继承自torch.autograd.Function</li>
<li>类A增加symbolic静态方法，其中返回g.op()，名称为Plugin_name_s插件名称，info可以带上string类型信息</li>
<li>为类A增加forward静态方法，此时的forward的任何操作不会跟踪并记录到ONNX中</li>
<li>实现一个OP类，继承自nn.Module，在OP.forward中调用A.apply</li>
<li>将OP集成到模型中</li>
</ol>
<p>编译&#x2F;推理环节：</p>
<ol>
<li>在Plugins文件夹中新建cu和hpp文件</li>
<li>写自定义config，用于配置支持的数据类型等</li>
<li>实现类继承自TRTPlugin，主要实现config用于返回自定义config类、getOutputDimensions返回layer处理后的tensor大小</li>
<li>enqueue实现推理工作</li>
</ol>
<h1 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h1><h2 id="Tensor封装"><a href="#Tensor封装" class="headerlink" title="Tensor封装"></a><strong>Tensor封装</strong></h2><p><strong>实现张量的内存管理、维度管理、偏移量计算、cpu&#x2F;gpu相互自动拷贝。避免手动管理内存、计算偏移量</strong></p>
<ol>
<li>CPU&#x2F;GPU内存自动分配释放，内存复用。Tensor使用MixMemory管理内存</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221105750138-168493652323416.png"
                      class="" title="image-20230221105750138"
                >

<ol start="2">
<li>CPU&#x2F;GPU之间自动内存复制</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221110212885-168493652323417.png"
                      class="" title="image-20230221110212885"
                >

<ol start="3">
<li>计算维度的偏移量</li>
</ol>
 <img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221141202162-168493652323418.png"
                      class="" title="image-20230221141202162"
                >

<h2 id="Builder封装"><a href="#Builder封装" class="headerlink" title="Builder封装"></a>Builder封装</h2><p><strong>实现onnx到引擎转换的封装，int8封装，少数几行代码实现需求</strong></p>
<ol>
<li><p>模型编译接口</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221141607655-168493652323419.png"
                      class="" title="image-20230221141607655"
                >
</li>
<li><p>Int8 Calibrator数据处理：Int8会压缩权重进行储存，通过Calibrator数据处理就会得到相应的权重</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221141916977-168493652323420.png"
                      class="" title="image-20230221141916977"
                >
</li>
<li><p>插件处理，自定义插件支持</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221142034328-168493652323421.png"
                      class="" title="image-20230221142034328"
                >
</li>
<li><p>特殊处理，reshape钩子，定制onnx的输入节点shape</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2023/02/20/TensorRT/image-20230221142046759-168493652323422.png"
                      class="" title="image-20230221142046759"
                ></li>
</ol>
<h2 id="Infer封装"><a href="#Infer封装" class="headerlink" title="Infer封装"></a>Infer封装</h2><p><strong>实现tensorRT引擎的推理管理，自动关联引擎的输入输出或者映射，管理上下文，插件</strong></p>
<ol>
<li>抽象input和output关系，避免手动去操作binding</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image-20230221142725737-168493652323423.png"
                      alt="image-20230221142725737"
                ></p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> TensorRT</li>
        <li><strong>作者:</strong> Airex Yu</li>
        <li><strong>创建于:</strong> 2023-02-20 13:39:03</li>
        
            <li>
                <strong>更新于:</strong> 2023-05-30 22:00:08
            </li>
        
        <li>
            <strong>链接:</strong> http://example.com/2023/02/20/TensorRT/
        </li>
        <li>
            <strong>版权声明:</strong> 本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/ONNX/">#ONNX</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/TensorRT/">#TensorRT</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2023/02/20/GPU%E7%BC%96%E7%A8%8B/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">GPU编程</span>
                                    <span class="post-nav-item">上一篇</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/02/06/opencv/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">opencv</span>
                                    <span class="post-nav-item">下一篇</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">TensorRT</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT"><span class="nav-text">TensorRT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">TensorRT是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86"><span class="nav-text">训练和推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86"><span class="nav-text">TensorRT加速原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%82%E9%97%B4%E8%9E%8D%E5%90%88%E6%88%96%E5%BC%A0%E9%87%8F%E8%9E%8D%E5%90%88%EF%BC%88Layer-amp-Tensor-Fusion%EF%BC%89"><span class="nav-text">层间融合或张量融合（Layer&amp;Tensor Fusion）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B2%BE%E5%BA%A6%E6%A0%A1%E5%87%86"><span class="nav-text">数据精度校准</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernel-Auto-Tuning"><span class="nav-text">Kernel Auto-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-Tensor-Memory"><span class="nav-text">Dynamic Tensor Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Stream-Execution"><span class="nav-text">Multi-Stream Execution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="nav-text">TensorRT使用流程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%BC%E5%87%BAONNX%E6%A8%A1%E5%9E%8B"><span class="nav-text">导出ONNX模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%87%BAonnx%E6%96%87%E4%BB%B6"><span class="nav-text">导出onnx文件</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5"><span class="nav-text">TensorRT推理阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#trtexec-exe%E5%AE%9E%E7%8E%B0%E9%A2%84%E6%8E%A8%E7%90%86%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="nav-text">trtexec.exe实现预推理（推荐）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E9%A2%84%E6%8E%A8%E7%90%86"><span class="nav-text">python代码实现预推理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRt%E9%83%A8%E7%BD%B2%E9%98%B6%E6%AE%B5"><span class="nav-text">TensorRt部署阶段</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT%E5%AE%9E%E7%8E%B0batch%E6%89%B9%E5%A4%84%E7%90%86"><span class="nav-text">TensorRT实现batch批处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%87%BAONNX"><span class="nav-text">导出ONNX</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0%E9%98%B6%E6%AE%B5"><span class="nav-text">模型构造阶段</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C%E9%98%B6%E6%AE%B5"><span class="nav-text">执行阶段</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT%E5%AE%98%E6%A1%A3%E4%BB%8B%E7%BB%8D"><span class="nav-text">TensorRT官档介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Programming-Model"><span class="nav-text">The Programming Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Runtime-Phase"><span class="nav-text">The Runtime Phase</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Plugins"><span class="nav-text">Plugins</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Types-and-Precision"><span class="nav-text">Types and Precision</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quantization"><span class="nav-text">Quantization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensors-and-Data-Formats"><span class="nav-text">Tensors and Data Formats</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dynamic-Shapes"><span class="nav-text">Dynamic Shapes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DLA"><span class="nav-text">DLA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Updating-Weights"><span class="nav-text">Updating Weights</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7"><span class="nav-text">工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#trtexec"><span class="nav-text">trtexec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Polygraphy"><span class="nav-text">Polygraphy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT%E7%9A%84C-%E6%8E%A5%E5%8F%A3%E8%A7%A3%E6%9E%90"><span class="nav-text">TensorRT的C++接口解析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Build-Phase"><span class="nav-text">The Build Phase</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-a-Network-Definition"><span class="nav-text">Creating a Network Definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Importing-a-Model-using-the-ONNX-Parser"><span class="nav-text">Importing a Model using the ONNX Parser</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-an-Engine"><span class="nav-text">Building an Engine</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deserializing-a-Plan"><span class="nav-text">Deserializing a Plan</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performing-Inference"><span class="nav-text">Performing Inference</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT-%E7%9A%84-Python-%E6%8E%A5%E5%8F%A3%E8%A7%A3%E6%9E%90"><span class="nav-text">TensorRT 的 Python 接口解析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Build-Phase-1"><span class="nav-text">The Build Phase</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-a-Network-Definition-in-Python"><span class="nav-text">Creating a Network Definition in Python</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Importing-a-Model-using-the-ONNX-Parser-1"><span class="nav-text">Importing a Model using the ONNX Parser</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-an-Engine-1"><span class="nav-text">Building an Engine</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performing-Inference-1"><span class="nav-text">Performing Inference</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C"><span class="nav-text">TensorRT如何工作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Object-Lifetimes"><span class="nav-text">Object Lifetimes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Error-Handling-and-Logging"><span class="nav-text">Error Handling and Logging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Memory"><span class="nav-text">Memory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Build-Phase-2"><span class="nav-text">The Build Phase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Runtime-Phase-1"><span class="nav-text">The Runtime Phase</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Threading"><span class="nav-text">Threading</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Determinism"><span class="nav-text">Determinism</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT-%E8%BF%9B%E9%98%B6%E7%94%A8%E6%B3%95"><span class="nav-text">TensorRT 进阶用法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Timing-Cache"><span class="nav-text">The Timing Cache</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refitting-An-Engine"><span class="nav-text">Refitting An Engine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm-Selection-and-Reproducible-Builds"><span class="nav-text">Algorithm Selection and Reproducible Builds</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Creating-A-Network-Definition-From-Scratch"><span class="nav-text">Creating A Network Definition From Scratch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Python"><span class="nav-text">Python</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reduced-Precision"><span class="nav-text">Reduced Precision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Network-level-Control-of-Precision"><span class="nav-text">Network-level Control of Precision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-level-Control-of-Precision"><span class="nav-text">Layer-level Control of Precision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Enabling-TF32-Inference-Using-C"><span class="nav-text">Enabling TF32 Inference Using C++</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-x2F-O-Formats"><span class="nav-text">I&#x2F;O Formats</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Compatibility-of-Serialized-Engines"><span class="nav-text">Compatibility of Serialized Engines</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Explicit-vs-Implicit-Batch"><span class="nav-text">Explicit vs Implicit Batch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sparsity"><span class="nav-text">Sparsity</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Empty-Tensors"><span class="nav-text">Empty Tensors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reusing-Input-Buffers"><span class="nav-text">Reusing Input Buffers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Engine-Inspector"><span class="nav-text">Engine Inspector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quantization-1"><span class="nav-text">Quantization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantization-Workflows"><span class="nav-text">Quantization Workflows</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Explicit-vs-Implicit-Quantization"><span class="nav-text">Explicit vs Implicit Quantization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Per-Tensor-and-Per-Channel-Quantization"><span class="nav-text">Per-Tensor and Per-Channel Quantization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Setting-Dynamic-Range"><span class="nav-text">Setting Dynamic Range</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Post-Training-Quantization-using-Calibration"><span class="nav-text">Post-Training Quantization using Calibration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Calibration-Using-Python"><span class="nav-text">Calibration Using Python</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Explicit-Quantization"><span class="nav-text">Explicit Quantization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantized-Weights"><span class="nav-text">Quantized Weights</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ONNX-Support"><span class="nav-text">ONNX Support</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorRT-Processing-Of-Q-x2F-DQ-Networks"><span class="nav-text">TensorRT Processing Of Q&#x2F;DQ Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-x2F-DQ-Layer-Placement-Recommendations"><span class="nav-text">Q&#x2F;DQ Layer-Placement Recommendations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-x2F-DQ-Limitations"><span class="nav-text">Q&#x2F;DQ Limitations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QAT-Networks-Using-PyTorch"><span class="nav-text">QAT Networks Using PyTorch</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT-%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E5%BD%A2%E7%8A%B6"><span class="nav-text">TensorRT 中的动态形状</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Specifying-Runtime-Dimensions"><span class="nav-text">Specifying Runtime Dimensions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimization-Profiles"><span class="nav-text">Optimization Profiles</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bindings-For-Multiple-Optimization-Profiles"><span class="nav-text">Bindings For Multiple Optimization Profiles</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Layer-Extensions-For-Dynamic-Shapes"><span class="nav-text">Layer Extensions For Dynamic Shapes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Restrictions-For-Dynamic-Shapes"><span class="nav-text">Restrictions For Dynamic Shapes</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ONNX"><span class="nav-text">ONNX</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ProtoBuf%E7%AE%80%E4%BB%8B"><span class="nav-text">ProtoBuf简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ONNX%E6%A0%BC%E5%BC%8F%E5%88%86%E6%9E%90"><span class="nav-text">ONNX格式分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#onnx-helper"><span class="nav-text">onnx.helper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B0%83%E7%94%A8TensorRT%E7%9A%84%E6%96%B9%E6%A1%88"><span class="nav-text">调用TensorRT的方案</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%BC%E5%87%BAONNX%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">导出ONNX注意的问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E6%80%81batch%E5%92%8C%E5%8A%A8%E6%80%81%E5%AE%BD%E9%AB%98"><span class="nav-text">动态batch和动态宽高</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8F%92%E4%BB%B6"><span class="nav-text">实现一个自定义插件</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B0%81%E8%A3%85"><span class="nav-text">封装</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor%E5%B0%81%E8%A3%85"><span class="nav-text">Tensor封装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Builder%E5%B0%81%E8%A3%85"><span class="nav-text">Builder封装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Infer%E5%B0%81%E8%A3%85"><span class="nav-text">Infer封装</span></a></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Airex Yu</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a> 驱动</span>
                <br>
            <span class="theme-version-container">主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.5</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/01/05 11:45:14
            </div>
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>




    
<script src="/js/libs/mermaid.min.js"></script>

    
<script src="/js/plugins/mermaid.js"></script>





<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
